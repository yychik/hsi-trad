{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    \n",
    "    #===========================================================================================\n",
    "    #Function to build tensor inputs to tensorflow\n",
    "    #---------------------------------------------\n",
    "    #batch_size: Number of samples per batch\n",
    "    #num_steps: Number of sequences per sample\n",
    "    #===========================================================================================\n",
    "    \n",
    "    #Declare the placeholders\n",
    "    inputs = tf.placeholder(tf.float32, shape=(batch_size, num_steps, 74), name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, shape=(batch_size, num_steps, 3), name='labels')\n",
    "    \n",
    "    #Dropout layer probabilities\n",
    "    keep_prob = tf.placeholder(tf.float32 , name='keep_prob')\n",
    "        \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_network(gru_size, num_layers, batch_size, keep_prob):\n",
    "    \n",
    "    #===========================================================================================\n",
    "    #Function to build network. The network will be the following\n",
    "    #LSTM > Dropout > LSTM > Dropout > Dense > Softmax\n",
    "    #---------------------------------------------\n",
    "    #lstm_size: Number of samples per batch\n",
    "    #num_layers: Number of layers for LSTM\n",
    "    #batch_size: Number of sequences per batch\n",
    "    #keep_prob: Dropout probability for the 2 dropout layers\n",
    "    #dense_hidden_units: Number of hidden units within the dense layer\n",
    "    #===========================================================================================\n",
    "    \n",
    "    def build_cell(size, keep_prob):\n",
    "        \n",
    "        #Helper function to create an LSTM cell, just to make things a bit neat\n",
    "        gru = tf.contrib.rnn.GRUCell(size)\n",
    "        \n",
    "        #Add drop out\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(gru, output_keep_prob=keep_prob)\n",
    "        \n",
    "        return drop\n",
    "    \n",
    "    #Stack up LSTM layers, and then add a dense layer\n",
    "    stacked_gru = tf.contrib.rnn.MultiRNNCell([build_cell(gru_size, keep_prob) for _ in range(0, num_layers)])\n",
    "        \n",
    "    #Initial State\n",
    "    initial_state = stacked_gru.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return stacked_gru, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(stacked_gru, dense_hidden_units, in_size, n_class=3):\n",
    "    \n",
    "    #===========================================================================================\n",
    "    # Function to build output function\n",
    "    #---------------------------------------------\n",
    "    # stacked_lstm: input tensor from stacked LSTM\n",
    "    # dense_hidden_units: number of hidden units from dense layer\n",
    "    # n_class: number of class probabilities\n",
    "    #===========================================================================================\n",
    "    \n",
    "    #We have the dense layer, build logits\n",
    "    seq_output = tf.concat(stacked_gru, axis=1)\n",
    "    #print('seq_output:', seq_output.get_shape())\n",
    "    \n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    #print('x:', x.get_shape())\n",
    "    \n",
    "    dense = tf.layers.dense(inputs=x, units=dense_hidden_units, activation=tf.nn.relu)\n",
    "    \n",
    "    softmax_w = tf.Variable(tf.truncated_normal((dense_hidden_units, n_class), stddev=0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(n_class))\n",
    "    \n",
    "    #Calculate logit\n",
    "    logits = tf.matmul(dense, softmax_w) + softmax_b\n",
    "    #print('logits:', logits.get_shape())\n",
    "    #Output\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    #print('out:', out.get_shape())\n",
    "    \n",
    "    return out, logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, n_class=3):\n",
    "    \n",
    "    #===========================================================================================\n",
    "    # Function to build output function\n",
    "    #---------------------------------------------\n",
    "    # dense: input tensor from dense layer\n",
    "    # n_class: number of class probabilities\n",
    "    #===========================================================================================\n",
    "    \n",
    "    #print('build_loss logits:', logits.get_shape())\n",
    "    #print('build_loss targets:', targets.get_shape())\n",
    "    \n",
    "    #Reshape targets\n",
    "    y_reshaped =  tf.reshape(targets, logits.get_shape())\n",
    "    #print('y_reshaped:', y_reshaped.get_shape())\n",
    "    \n",
    "    #Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    \n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define LSTM Parameters and structure\n",
    "class DirectionLSTM:\n",
    "    \n",
    "    #============================================================================================\n",
    "    #Class Constructor\n",
    "    #-----------------\n",
    "    #Construct GRU object to carry around\n",
    "    #\n",
    "    #\n",
    "    #============================================================================================\n",
    "    def __init__(self, n_class=3, batch_size=128, num_steps=10, gru_size=50, dense_size=50, num_layers=2,\n",
    "                 learning_rate=0.001, grad_clip=2, online=False):\n",
    "        \n",
    "        #If Online learning, we will be having batch_size of 1 and 1 time step\n",
    "        if online == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "        \n",
    "        #Reset graph\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        #Build inputs\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "        \n",
    "        #Build LSTM Cell\n",
    "        cell, self.initial_state = build_network(gru_size, num_layers, batch_size, self.keep_prob)\n",
    "        \n",
    "        #Link Up the RNN cells\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, self.inputs, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        #Get softmax predictions\n",
    "        self.prediction, self.logits = build_output(outputs, dense_size, gru_size, n_class)\n",
    "        \n",
    "        #Loss and optimizer\n",
    "        self.loss = build_loss(self.logits, self.targets, n_class)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)\n",
    "        \n",
    "        #print(self.targets.get_shape())\n",
    "        #print(self.logits.get_shape())\n",
    "        \n",
    "        #Evaluation metrics\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(tf.argmax(self.prediction, 1), tf.argmax(tf.reshape(self.targets, self.prediction.get_shape()), 1))\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_predictions, 'float'), name='accuracy')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
