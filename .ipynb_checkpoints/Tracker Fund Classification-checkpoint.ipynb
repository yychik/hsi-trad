{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction As Classification\n",
    "Continuing the 2800-HK price prediction from classification perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Modules and load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "np.random.seed(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import sklearn\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import Keras module\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 238791969078181173\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11308947866\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 16693026708071157388\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#Check GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pretty plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>P_L1</th>\n",
       "      <th>P_L2</th>\n",
       "      <th>P_L3</th>\n",
       "      <th>P_L4</th>\n",
       "      <th>P_L5</th>\n",
       "      <th>P_L6</th>\n",
       "      <th>P_L7</th>\n",
       "      <th>P_L8</th>\n",
       "      <th>P_L9</th>\n",
       "      <th>...</th>\n",
       "      <th>P_L11</th>\n",
       "      <th>P_L12</th>\n",
       "      <th>P_L13</th>\n",
       "      <th>P_L14</th>\n",
       "      <th>P_L15</th>\n",
       "      <th>P_L16</th>\n",
       "      <th>P_L17</th>\n",
       "      <th>P_L18</th>\n",
       "      <th>P_L19</th>\n",
       "      <th>P_L20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-08-31</th>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>23.35</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.05</td>\n",
       "      <td>21.85</td>\n",
       "      <td>...</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.85</td>\n",
       "      <td>22.15</td>\n",
       "      <td>22.25</td>\n",
       "      <td>22.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-03</th>\n",
       "      <td>24.30</td>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>23.35</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.05</td>\n",
       "      <td>...</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.85</td>\n",
       "      <td>22.15</td>\n",
       "      <td>22.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-04</th>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>23.35</td>\n",
       "      <td>22.70</td>\n",
       "      <td>...</td>\n",
       "      <td>21.85</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.85</td>\n",
       "      <td>22.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-05</th>\n",
       "      <td>24.35</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>23.35</td>\n",
       "      <td>...</td>\n",
       "      <td>22.05</td>\n",
       "      <td>21.85</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-06</th>\n",
       "      <td>24.50</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>...</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.05</td>\n",
       "      <td>21.85</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                P   P_L1   P_L2   P_L3   P_L4   P_L5   P_L6   P_L7   P_L8  \\\n",
       "Date                                                                        \n",
       "2007-08-31  24.35  23.80  23.35  23.70  23.90  23.30  23.35  22.70  22.05   \n",
       "2007-09-03  24.30  24.35  23.80  23.35  23.70  23.90  23.30  23.35  22.70   \n",
       "2007-09-04  24.30  24.30  24.35  23.80  23.35  23.70  23.90  23.30  23.35   \n",
       "2007-09-05  24.35  24.30  24.30  24.35  23.80  23.35  23.70  23.90  23.30   \n",
       "2007-09-06  24.50  24.35  24.30  24.30  24.35  23.80  23.35  23.70  23.90   \n",
       "\n",
       "             P_L9  ...    P_L11  P_L12  P_L13  P_L14  P_L15  P_L16  P_L17  \\\n",
       "Date               ...                                                      \n",
       "2007-08-31  21.85  ...    20.95  21.70  22.35  22.30  22.05  22.70  22.85   \n",
       "2007-09-03  22.05  ...    20.80  20.95  21.70  22.35  22.30  22.05  22.70   \n",
       "2007-09-04  22.70  ...    21.85  20.80  20.95  21.70  22.35  22.30  22.05   \n",
       "2007-09-05  23.35  ...    22.05  21.85  20.80  20.95  21.70  22.35  22.30   \n",
       "2007-09-06  23.30  ...    22.70  22.05  21.85  20.80  20.95  21.70  22.35   \n",
       "\n",
       "            P_L18  P_L19  P_L20  \n",
       "Date                             \n",
       "2007-08-31  22.15  22.25  22.85  \n",
       "2007-09-03  22.85  22.15  22.25  \n",
       "2007-09-04  22.70  22.85  22.15  \n",
       "2007-09-05  22.05  22.70  22.85  \n",
       "2007-09-06  22.30  22.05  22.70  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import price data\n",
    "#Load the historical prices of 2800-HK, with lags 1 to lag 20\n",
    "price_hist_data = pd.read_csv('price_only.csv', skiprows=1, parse_dates=['Date']).set_index(['Date'])\n",
    "price_hist_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Ask</th>\n",
       "      <th>Bid</th>\n",
       "      <th>20D Vol</th>\n",
       "      <th>MA5</th>\n",
       "      <th>MA15</th>\n",
       "      <th>MA12</th>\n",
       "      <th>MA20</th>\n",
       "      <th>...</th>\n",
       "      <th>DY_LTM</th>\n",
       "      <th>DY_NTM</th>\n",
       "      <th>ADV_VOL</th>\n",
       "      <th>PAYOUT</th>\n",
       "      <th>ANALYST_SENTIMENT</th>\n",
       "      <th>EPS_GRW_FY1</th>\n",
       "      <th>EPS_GRW_FY2</th>\n",
       "      <th>PE_NTM</th>\n",
       "      <th>PE_LTM</th>\n",
       "      <th>C2D_LTM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-08-31</th>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>2.367823</td>\n",
       "      <td>23.82</td>\n",
       "      <td>22.841667</td>\n",
       "      <td>22.696667</td>\n",
       "      <td>22.6225</td>\n",
       "      <td>...</td>\n",
       "      <td>2.782797</td>\n",
       "      <td>2.880128</td>\n",
       "      <td>99.419898</td>\n",
       "      <td>48.051962</td>\n",
       "      <td>1.909071</td>\n",
       "      <td>30.724490</td>\n",
       "      <td>2.717280</td>\n",
       "      <td>16.695058</td>\n",
       "      <td>16.975805</td>\n",
       "      <td>58.050474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-03</th>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>2.266743</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.120832</td>\n",
       "      <td>22.830000</td>\n",
       "      <td>22.7250</td>\n",
       "      <td>...</td>\n",
       "      <td>2.784563</td>\n",
       "      <td>2.892308</td>\n",
       "      <td>11.570023</td>\n",
       "      <td>48.305678</td>\n",
       "      <td>1.835359</td>\n",
       "      <td>31.005439</td>\n",
       "      <td>1.876752</td>\n",
       "      <td>16.709085</td>\n",
       "      <td>16.925571</td>\n",
       "      <td>58.154835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-04</th>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>2.259649</td>\n",
       "      <td>24.02</td>\n",
       "      <td>23.412500</td>\n",
       "      <td>22.960000</td>\n",
       "      <td>22.8325</td>\n",
       "      <td>...</td>\n",
       "      <td>2.784339</td>\n",
       "      <td>2.894377</td>\n",
       "      <td>69.555725</td>\n",
       "      <td>48.307102</td>\n",
       "      <td>1.725886</td>\n",
       "      <td>31.040968</td>\n",
       "      <td>1.891823</td>\n",
       "      <td>16.697662</td>\n",
       "      <td>16.919413</td>\n",
       "      <td>58.177640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-05</th>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>2.172140</td>\n",
       "      <td>24.22</td>\n",
       "      <td>23.620832</td>\n",
       "      <td>23.136667</td>\n",
       "      <td>22.9075</td>\n",
       "      <td>...</td>\n",
       "      <td>2.767348</td>\n",
       "      <td>2.867314</td>\n",
       "      <td>24.265623</td>\n",
       "      <td>48.267351</td>\n",
       "      <td>1.741908</td>\n",
       "      <td>30.716929</td>\n",
       "      <td>1.878484</td>\n",
       "      <td>16.841225</td>\n",
       "      <td>17.017692</td>\n",
       "      <td>58.195446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-06</th>\n",
       "      <td>24.50</td>\n",
       "      <td>24.50</td>\n",
       "      <td>24.50</td>\n",
       "      <td>24.50</td>\n",
       "      <td>24.50</td>\n",
       "      <td>2.160638</td>\n",
       "      <td>24.36</td>\n",
       "      <td>23.825000</td>\n",
       "      <td>23.373333</td>\n",
       "      <td>22.9975</td>\n",
       "      <td>...</td>\n",
       "      <td>2.775339</td>\n",
       "      <td>2.878880</td>\n",
       "      <td>88.845002</td>\n",
       "      <td>48.288397</td>\n",
       "      <td>1.696151</td>\n",
       "      <td>31.336520</td>\n",
       "      <td>1.861943</td>\n",
       "      <td>16.780660</td>\n",
       "      <td>16.975485</td>\n",
       "      <td>58.224743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Close   High    Low    Ask    Bid   20D Vol    MA5       MA15  \\\n",
       "Date                                                                        \n",
       "2007-08-31  24.35  24.35  24.35  24.35  24.35  2.367823  23.82  22.841667   \n",
       "2007-09-03  24.30  24.30  24.30  24.30  24.30  2.266743  23.90  23.120832   \n",
       "2007-09-04  24.30  24.30  24.30  24.30  24.30  2.259649  24.02  23.412500   \n",
       "2007-09-05  24.35  24.35  24.35  24.35  24.35  2.172140  24.22  23.620832   \n",
       "2007-09-06  24.50  24.50  24.50  24.50  24.50  2.160638  24.36  23.825000   \n",
       "\n",
       "                 MA12     MA20    ...        DY_LTM    DY_NTM    ADV_VOL  \\\n",
       "Date                              ...                                      \n",
       "2007-08-31  22.696667  22.6225    ...      2.782797  2.880128  99.419898   \n",
       "2007-09-03  22.830000  22.7250    ...      2.784563  2.892308  11.570023   \n",
       "2007-09-04  22.960000  22.8325    ...      2.784339  2.894377  69.555725   \n",
       "2007-09-05  23.136667  22.9075    ...      2.767348  2.867314  24.265623   \n",
       "2007-09-06  23.373333  22.9975    ...      2.775339  2.878880  88.845002   \n",
       "\n",
       "               PAYOUT  ANALYST_SENTIMENT  EPS_GRW_FY1  EPS_GRW_FY2     PE_NTM  \\\n",
       "Date                                                                            \n",
       "2007-08-31  48.051962           1.909071    30.724490     2.717280  16.695058   \n",
       "2007-09-03  48.305678           1.835359    31.005439     1.876752  16.709085   \n",
       "2007-09-04  48.307102           1.725886    31.040968     1.891823  16.697662   \n",
       "2007-09-05  48.267351           1.741908    30.716929     1.878484  16.841225   \n",
       "2007-09-06  48.288397           1.696151    31.336520     1.861943  16.780660   \n",
       "\n",
       "               PE_LTM    C2D_LTM  \n",
       "Date                              \n",
       "2007-08-31  16.975805  58.050474  \n",
       "2007-09-03  16.925571  58.154835  \n",
       "2007-09-04  16.919413  58.177640  \n",
       "2007-09-05  17.017692  58.195446  \n",
       "2007-09-06  16.975485  58.224743  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Fundamentals Data\n",
    "fund_data = pd.read_csv('new_index_data.csv', skiprows=1, parse_dates=['Date']).set_index(['Date'])\n",
    "fund_data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hang Seng Index</th>\n",
       "      <th>SSE Composite Index</th>\n",
       "      <th>ASX All Ordinaries</th>\n",
       "      <th>India S&amp;P BSE SENSEX</th>\n",
       "      <th>TOPIX</th>\n",
       "      <th>KOSPI Composite Index</th>\n",
       "      <th>Taiwan TAIEX</th>\n",
       "      <th>FTSE Bursa Malaysia KLCI</th>\n",
       "      <th>FTSE Straits Times Index</th>\n",
       "      <th>Philippines PSE PSEi</th>\n",
       "      <th>...</th>\n",
       "      <th>Turkey BIST 100</th>\n",
       "      <th>S&amp;P 500</th>\n",
       "      <th>DJ Industrial Average</th>\n",
       "      <th>Colombia IGBC</th>\n",
       "      <th>Canada S&amp;P/TSX Composite</th>\n",
       "      <th>Brazil Bovespa Index</th>\n",
       "      <th>Mexico IPC</th>\n",
       "      <th>Israel TA-125</th>\n",
       "      <th>Saudi Arabia All Share (TASI)</th>\n",
       "      <th>FTSE JSE All Share</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-08-31</th>\n",
       "      <td>23984.14</td>\n",
       "      <td>5218.825</td>\n",
       "      <td>6248.3</td>\n",
       "      <td>15318.60</td>\n",
       "      <td>1608.25</td>\n",
       "      <td>1873.24</td>\n",
       "      <td>8982.16</td>\n",
       "      <td>1273.93</td>\n",
       "      <td>3328.43</td>\n",
       "      <td>3365.29</td>\n",
       "      <td>...</td>\n",
       "      <td>50198.60</td>\n",
       "      <td>1473.99</td>\n",
       "      <td>13357.74</td>\n",
       "      <td>10728.74</td>\n",
       "      <td>13660.48</td>\n",
       "      <td>54637.24</td>\n",
       "      <td>30347.86</td>\n",
       "      <td>1034.67</td>\n",
       "      <td>8226.97</td>\n",
       "      <td>28660.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-03</th>\n",
       "      <td>23904.09</td>\n",
       "      <td>5321.055</td>\n",
       "      <td>6272.5</td>\n",
       "      <td>15422.05</td>\n",
       "      <td>1605.44</td>\n",
       "      <td>1881.81</td>\n",
       "      <td>8979.96</td>\n",
       "      <td>1284.14</td>\n",
       "      <td>3321.36</td>\n",
       "      <td>3369.14</td>\n",
       "      <td>...</td>\n",
       "      <td>49936.94</td>\n",
       "      <td>1473.99</td>\n",
       "      <td>13357.74</td>\n",
       "      <td>10750.79</td>\n",
       "      <td>13660.48</td>\n",
       "      <td>54832.51</td>\n",
       "      <td>30797.60</td>\n",
       "      <td>1047.33</td>\n",
       "      <td>8017.54</td>\n",
       "      <td>28887.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-04</th>\n",
       "      <td>23886.07</td>\n",
       "      <td>5294.045</td>\n",
       "      <td>6297.1</td>\n",
       "      <td>15465.40</td>\n",
       "      <td>1596.74</td>\n",
       "      <td>1874.74</td>\n",
       "      <td>8922.98</td>\n",
       "      <td>1283.75</td>\n",
       "      <td>3308.81</td>\n",
       "      <td>3312.30</td>\n",
       "      <td>...</td>\n",
       "      <td>50032.59</td>\n",
       "      <td>1489.42</td>\n",
       "      <td>13448.86</td>\n",
       "      <td>10880.85</td>\n",
       "      <td>13755.23</td>\n",
       "      <td>55250.47</td>\n",
       "      <td>30932.71</td>\n",
       "      <td>1054.69</td>\n",
       "      <td>7878.70</td>\n",
       "      <td>29051.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-05</th>\n",
       "      <td>24069.17</td>\n",
       "      <td>5310.716</td>\n",
       "      <td>6274.3</td>\n",
       "      <td>15446.15</td>\n",
       "      <td>1569.47</td>\n",
       "      <td>1865.59</td>\n",
       "      <td>8913.85</td>\n",
       "      <td>1297.93</td>\n",
       "      <td>3375.02</td>\n",
       "      <td>3342.35</td>\n",
       "      <td>...</td>\n",
       "      <td>49421.38</td>\n",
       "      <td>1472.29</td>\n",
       "      <td>13305.47</td>\n",
       "      <td>10819.91</td>\n",
       "      <td>13683.28</td>\n",
       "      <td>54407.83</td>\n",
       "      <td>30809.55</td>\n",
       "      <td>1048.70</td>\n",
       "      <td>7853.66</td>\n",
       "      <td>28696.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-06</th>\n",
       "      <td>24050.40</td>\n",
       "      <td>5393.660</td>\n",
       "      <td>6265.3</td>\n",
       "      <td>15616.31</td>\n",
       "      <td>1568.52</td>\n",
       "      <td>1888.81</td>\n",
       "      <td>9017.08</td>\n",
       "      <td>1298.85</td>\n",
       "      <td>3399.49</td>\n",
       "      <td>3326.53</td>\n",
       "      <td>...</td>\n",
       "      <td>49601.39</td>\n",
       "      <td>1478.55</td>\n",
       "      <td>13363.35</td>\n",
       "      <td>10844.40</td>\n",
       "      <td>13795.69</td>\n",
       "      <td>54569.00</td>\n",
       "      <td>30816.95</td>\n",
       "      <td>1033.23</td>\n",
       "      <td>7853.66</td>\n",
       "      <td>28850.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Hang Seng Index  SSE Composite Index  ASX All Ordinaries  \\\n",
       "Date                                                                   \n",
       "2007-08-31         23984.14             5218.825              6248.3   \n",
       "2007-09-03         23904.09             5321.055              6272.5   \n",
       "2007-09-04         23886.07             5294.045              6297.1   \n",
       "2007-09-05         24069.17             5310.716              6274.3   \n",
       "2007-09-06         24050.40             5393.660              6265.3   \n",
       "\n",
       "            India S&P BSE SENSEX    TOPIX  KOSPI Composite Index  \\\n",
       "Date                                                               \n",
       "2007-08-31              15318.60  1608.25                1873.24   \n",
       "2007-09-03              15422.05  1605.44                1881.81   \n",
       "2007-09-04              15465.40  1596.74                1874.74   \n",
       "2007-09-05              15446.15  1569.47                1865.59   \n",
       "2007-09-06              15616.31  1568.52                1888.81   \n",
       "\n",
       "            Taiwan TAIEX  FTSE Bursa Malaysia KLCI  FTSE Straits Times Index  \\\n",
       "Date                                                                           \n",
       "2007-08-31       8982.16                   1273.93                   3328.43   \n",
       "2007-09-03       8979.96                   1284.14                   3321.36   \n",
       "2007-09-04       8922.98                   1283.75                   3308.81   \n",
       "2007-09-05       8913.85                   1297.93                   3375.02   \n",
       "2007-09-06       9017.08                   1298.85                   3399.49   \n",
       "\n",
       "            Philippines PSE PSEi         ...          Turkey BIST 100  \\\n",
       "Date                                     ...                            \n",
       "2007-08-31               3365.29         ...                 50198.60   \n",
       "2007-09-03               3369.14         ...                 49936.94   \n",
       "2007-09-04               3312.30         ...                 50032.59   \n",
       "2007-09-05               3342.35         ...                 49421.38   \n",
       "2007-09-06               3326.53         ...                 49601.39   \n",
       "\n",
       "            S&P 500  DJ Industrial Average  Colombia IGBC  \\\n",
       "Date                                                        \n",
       "2007-08-31  1473.99               13357.74       10728.74   \n",
       "2007-09-03  1473.99               13357.74       10750.79   \n",
       "2007-09-04  1489.42               13448.86       10880.85   \n",
       "2007-09-05  1472.29               13305.47       10819.91   \n",
       "2007-09-06  1478.55               13363.35       10844.40   \n",
       "\n",
       "            Canada S&P/TSX Composite  Brazil Bovespa Index  Mexico IPC  \\\n",
       "Date                                                                     \n",
       "2007-08-31                  13660.48              54637.24    30347.86   \n",
       "2007-09-03                  13660.48              54832.51    30797.60   \n",
       "2007-09-04                  13755.23              55250.47    30932.71   \n",
       "2007-09-05                  13683.28              54407.83    30809.55   \n",
       "2007-09-06                  13795.69              54569.00    30816.95   \n",
       "\n",
       "            Israel TA-125  Saudi Arabia All Share (TASI)  FTSE JSE All Share  \n",
       "Date                                                                          \n",
       "2007-08-31        1034.67                        8226.97            28660.35  \n",
       "2007-09-03        1047.33                        8017.54            28887.48  \n",
       "2007-09-04        1054.69                        7878.70            29051.96  \n",
       "2007-09-05        1048.70                        7853.66            28696.67  \n",
       "2007-09-06        1033.23                        7853.66            28850.19  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Global index data\n",
    "idx_data = pd.read_csv('indices.csv', skiprows=1, parse_dates=['Date']).set_index(['Date'])\n",
    "idx_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pre-process Raw Data\n",
    "1. Generate labels\n",
    "2. Apply lags to global index data\n",
    "3. Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate Labels from Price History Data\n",
    "#Generate UP/DOWN labels from log change\n",
    "cutoff_perc = 0.0005 #0.05% return as cuttoff to define UP label\n",
    "lag = 1 #forward returns\n",
    "\n",
    "labels = np.zeros([price_hist_data.shape[0]])\n",
    "\n",
    "#Caluclate log-returns\n",
    "ret = np.log(price_hist_data['P'].shift(-lag)/price_hist_data['P'])\n",
    "labels = [1 if r > cutoff_perc else 0 for r in ret]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Applying lags to index data\n",
    "#Seperate the indices into 2 classes - lag or no_lag\n",
    "no_lag = [0, 1, 2, 4, 5, 6, 9, 10]\n",
    "lag = [i for i in range(0,idx_data.shape[1]) if i not in no_lag]\n",
    "\n",
    "#Processing the dataset by applying appropriate lags\n",
    "lagged_data = idx_data.iloc[:,lag].shift(1)\n",
    "idx_data = pd.concat([idx_data.iloc[:,no_lag], lagged_data], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove first row\n",
    "idx_data = idx_data.iloc[1:, :]\n",
    "price_hist_data = price_hist_data.iloc[1:, :]\n",
    "fund_data = fund_data.iloc[1:, :]\n",
    "labels=labels[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of idx_data:  (2483, 42)\n",
      "Shape of price_hist_data:  (2483, 21)\n",
      "Shape of fund_data:  (2483, 30)\n",
      "Shape of labels:  2483\n"
     ]
    }
   ],
   "source": [
    "#Check Dimensions to make sure everythings right before continuing..\n",
    "print(\"Shape of idx_data: \", idx_data.shape)\n",
    "print(\"Shape of price_hist_data: \", price_hist_data.shape)\n",
    "print(\"Shape of fund_data: \", fund_data.shape)\n",
    "print(\"Shape of labels: \", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training data\n",
    "X = np.array(pd.concat([price_hist_data, idx_data, fund_data], axis=1))\n",
    "y = to_categorical(labels, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking whether there are NAs\n",
    "[np.sum(np.isnan(X), axis=0) > 0] == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Split training, validation and test set\n",
    "\n",
    "In this stage we have a look-ahead bias free set of data (X) and the labels y. Next, we will need to:\n",
    "- Normalize the input data. To avoid look ahead bias, we will z-score the features, using ONLY the training set.\n",
    "- Next, generate input data into LSTM network. We will need an overlapping sequence at 1-day window as input samples. Specifically suppose the *timestep* is 240, we will have a list of array consists of *number of rows of X* - 240 entries, each element has dimentions (240, num_of_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to split raw data into training, validation and test set, returns a numpy array.\n",
    "def split_data(input_data=[], train_size=0.8, val_size=0.2, test_size=0):\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    #PARAM: input_data: numpy nd array\n",
    "    #PARAM: training_size: size of training set in decimal\n",
    "    #PARAM: val_size: size of validation set in decimal\n",
    "    #PARAM: test_size: size of test set in decimal\n",
    "    #OUTPUT: tuple (train_set, validation_set, test_set)\n",
    "    #------------------------------------------------\n",
    "    \n",
    "    #First check whether traing_size + val_size + test_size = 1 and each of the entries are positive\n",
    "    assert(train_size + val_size + test_size==1), \"Sum of training, validation and test size needs to be 1!\"\n",
    "    assert(train_size * val_size * test_size > 0), \"Sizes have to be positive!\"\n",
    "    \n",
    "    #Check input_data type is numpy array, after casting\n",
    "    if type(input_data) != 'numpy.ndarray':\n",
    "        input_data = np.array(input_data) \n",
    "    \n",
    "    assert(isinstance(input_data, np.ndarray)), \"Input has to be a numpy array!\"\n",
    "    \n",
    "    \n",
    "    #Calculate cut-off points\n",
    "    train_cut_index = int(train_size * input_data.shape[0])\n",
    "    val_cut_index = train_cut_index + int(val_size * input_data.shape[0])\n",
    "    \n",
    "    #Split the data\n",
    "    if len(input_data.shape) == 1:\n",
    "        train, val, test = input_data[:train_cut_index], input_data[train_cut_index:val_cut_index], input_data[val_cut_index:]\n",
    "    else:\n",
    "        train, val, test = input_data[:train_cut_index,:], input_data[train_cut_index:val_cut_index, :], input_data[val_cut_index:, :]\n",
    "    \n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1489, 93)\n",
      "(496, 93)\n",
      "(498, 93)\n",
      "(1489, 2)\n",
      "(496, 2)\n",
      "(498, 2)\n"
     ]
    }
   ],
   "source": [
    "#------------\n",
    "#TEST OUTPUT\n",
    "#------------\n",
    "#Split data\n",
    "X_train, X_val, X_test = split_data(X, train_size=0.6, val_size=0.2, test_size=0.2)\n",
    "y_train, y_val, y_test = split_data(y, train_size=0.6, val_size=0.2, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to re-structure the data to get batches. Re-shape the data to have overlapping training set for time-series learning.\n",
    "def get_inputs(input_data, labels, batch_size, timesteps):\n",
    "    \n",
    "    #First get the total number of samples generated\n",
    "    n_seq = input_data.shape[0] - timesteps + 1\n",
    "    \n",
    "    #features, classes\n",
    "    n_dim = input_data.shape[1]\n",
    "    n_class = labels.shape[1]\n",
    "    \n",
    "    #Calculate the number of batches possible\n",
    "    n_samples = n_seq * timesteps\n",
    "    n_batches = n_samples // (batch_size * timesteps)\n",
    "\n",
    "    #Generate labels to have (batch_size, 1)\n",
    "    targets=labels[:n_batches * batch_size,:]\n",
    "    \n",
    "    \n",
    "    #Generate training data with dim (batch_size, timesteps, n_features)\n",
    "    output = []\n",
    "    \n",
    "        \n",
    "    for jj in range(0, n_batches):\n",
    "    #Generate the sequences\n",
    "        \n",
    "        #if jj == n_batches:\n",
    "            \n",
    "            #output.append([input_data[jj*batch_size:, :]])\n",
    "            #targets.append([labels[jj*batch_size:, :]])            \n",
    "            #yield np.vstack(output), np.vstack(targets  )\n",
    "            \n",
    "        #else:            \n",
    "        for ii in range(jj * batch_size, (jj + 1) * batch_size):                    \n",
    "            #Getting the overlapping samples\n",
    "            output.append([scale(input_data[ii:ii+timesteps, :])])\n",
    "            #targets.append([labels[ii:ii+timesteps, :]])\n",
    "\n",
    "    return np.vstack(output), targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 4, 5)\n",
      "(6, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryan_yychik/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#------------\n",
    "#TEST OUTPUT\n",
    "#------------\n",
    "#get inputs\n",
    "t = np.reshape(np.arange(1,51), (10,5))\n",
    "s = np.reshape(np.arange(1,21), (10,2))\n",
    "\n",
    "#for (x, y) in get_inputs(t,s, 3, 4):\n",
    "#    print('training size: ', x.shape)\n",
    "#    print('training: \\n', x)\n",
    "#    print('label size: ', y.shape)\n",
    "#    print('labels: \\n', y)\n",
    "\n",
    "test_train, test_label = get_inputs(t,s,3,4)\n",
    "    \n",
    "\n",
    "#print(test_train[0:1])\n",
    "#print(test_label[0:1])\n",
    "#print(t)\n",
    "print(test_train.shape)\n",
    "print(test_label.shape)\n",
    "#print(test_train)\n",
    "#test_train.reshape((-1, 4, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define Parameters\n",
    "learning_rate = 0.0005\n",
    "epochs= 100\n",
    "loss = 'binary_crossentropy'\n",
    "batch_size = 256\n",
    "timesteps = 22\n",
    "n_dim = X.shape[1]\n",
    "n_classes = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Optimizer - using Adam Optimizer\n",
    "optimizer = optimizers.Adam(lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate Inputs\n",
    "X_train_1, y_train_1 = get_inputs(X_train, y_train, batch_size, timesteps)\n",
    "X_val_1, y_val_1 = get_inputs(X_val, y_val, batch_size, timesteps)\n",
    "X_test_1, y_test_1 = get_inputs(X_test, y_test, batch_size, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define LSTM Network object\n",
    "def build_network(n_hidden_layer, dropout, input_shape, batch_size, return_sequences=True, stateful=True):\n",
    "    \n",
    "    #Define network architect\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_hidden_layer[0], \n",
    "                   input_shape=input_shape,\n",
    "                   kernel_initializer='truncated_normal',\n",
    "                   recurrent_initializer='truncated_normal',\n",
    "                   bias_initializer='RandomUniform',\n",
    "                   batch_size=batch_size, \n",
    "                   return_sequences=return_sequences, \n",
    "                   stateful=stateful))\n",
    "    \n",
    "    model.add(Dropout(dropout[0]))\n",
    "    \n",
    "    model.add(LSTM(n_hidden_layer[1], \n",
    "                   return_sequences=False,\n",
    "                   kernel_initializer='truncated_normal',\n",
    "                   recurrent_initializer='truncated_normal',\n",
    "                   bias_initializer='RandomUniform',\n",
    "                   stateful=stateful))\n",
    "    \n",
    "    model.add(Dropout(dropout[1]))\n",
    "    \n",
    "    model.add(Dense(n_hidden_layer[2], activation='relu', kernel_initializer='truncated_normal'))\n",
    "    \n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lstm1 = build_network([5,5,10], [0.1,0.1], input_shape=(timesteps, n_dim), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compile\n",
    "#lstm1.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fitted = lstm1.fit(X_train_1, y_train_1, batch_size=batch_size, epochs=100, verbose=1, validation_data=(X_val_1, y_val_1), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pred = lstm1.predict(X_test_1, batch_size=256)\n",
    "#pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot_metric(fitted, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 5: Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Plot Training vs Validation Curve\n",
    "def plot_metric(fitted, metric):\n",
    "    plt.figure()\n",
    "    plt.plot(fitted.history[metric])\n",
    "    plt.plot(fitted.history['val_' + metric])\n",
    "    plt.title('Training ' + metric + ' & Validation ' + metric)\n",
    "    plt.ylabel(metric)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training ' + metric, 'Validation ' + metric])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 6: Parameter Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search the following parameters:\n",
    "- Learning Rate & decay rate\n",
    "- Number of Hidden Layers\n",
    "- Dropout Rate\n",
    "- Timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "best_drop_out             -> (0.5538553460451876, 0.50439770945911711)\n",
      "best_lr                   -> 0.060754747925204793\n",
      "best_n_hidden             -> (92, 256, 50)\n",
      "best_timestep             -> 177\n",
      "do_loss                   -> [0.68905091285705566, 0.69272321462631226, 0.69057\n",
      "hd_loss                   -> [0.69231581687927246, 0.69247829914093018, 0.69271\n",
      "lr_loss                   -> [0.68858450651168823, 0.69039624929428101, 0.69040\n",
      "t_loss                    -> [0.69411438703536987, 0.71084636449813843, 0.69192\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define number of random trials\n",
    "n_trials = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1, Number of layers: (92, 209, 106),  Best Validation Loss: 0.6947\n",
      "Steps: 2, Number of layers: (76, 247, 255),  Best Validation Loss: 0.6935\n",
      "Steps: 3, Number of layers: (197, 157, 184),  Best Validation Loss: 0.6923\n",
      "Steps: 4, Number of layers: (256, 243, 154),  Best Validation Loss: 0.6882\n",
      "Steps: 5, Number of layers: (262, 8, 167),  Best Validation Loss: 0.6935\n",
      "Steps: 6, Number of layers: (133, 64, 231),  Best Validation Loss: 0.6936\n",
      "Steps: 7, Number of layers: (50, 246, 30),  Best Validation Loss: 0.6922\n",
      "Steps: 8, Number of layers: (45, 179, 55),  Best Validation Loss: 0.6919\n",
      "Steps: 9, Number of layers: (224, 168, 164),  Best Validation Loss: 0.6920\n",
      "Steps: 10, Number of layers: (297, 154, 35),  Best Validation Loss: 0.6920\n",
      "Steps: 11, Number of layers: (193, 73, 109),  Best Validation Loss: 0.6932\n",
      "Steps: 12, Number of layers: (233, 199, 129),  Best Validation Loss: 0.6922\n",
      "Steps: 13, Number of layers: (153, 89, 9),  Best Validation Loss: 0.6937\n",
      "Steps: 14, Number of layers: (291, 279, 261),  Best Validation Loss: 0.6876\n",
      "Steps: 15, Number of layers: (238, 216, 164),  Best Validation Loss: 0.6913\n",
      "Steps: 16, Number of layers: (249, 116, 49),  Best Validation Loss: 0.6912\n",
      "Steps: 17, Number of layers: (210, 136, 189),  Best Validation Loss: 0.6913\n",
      "Steps: 18, Number of layers: (215, 273, 235),  Best Validation Loss: 0.6912\n",
      "Steps: 19, Number of layers: (161, 140, 109),  Best Validation Loss: 0.6913\n",
      "Steps: 20, Number of layers: (218, 196, 62),  Best Validation Loss: 0.6936\n",
      "Steps: 21, Number of layers: (226, 93, 153),  Best Validation Loss: 0.6949\n",
      "Steps: 22, Number of layers: (143, 175, 118),  Best Validation Loss: 0.6910\n",
      "Steps: 23, Number of layers: (19, 156, 238),  Best Validation Loss: 0.6934\n",
      "Steps: 24, Number of layers: (74, 99, 69),  Best Validation Loss: 0.6928\n",
      "Steps: 25, Number of layers: (209, 251, 146),  Best Validation Loss: 0.6937\n",
      "Steps: 26, Number of layers: (16, 117, 279),  Best Validation Loss: 0.6928\n",
      "Steps: 27, Number of layers: (282, 163, 95),  Best Validation Loss: 0.6904\n",
      "Steps: 28, Number of layers: (287, 105, 49),  Best Validation Loss: 0.6916\n",
      "Steps: 29, Number of layers: (105, 260, 52),  Best Validation Loss: 0.6919\n",
      "Steps: 30, Number of layers: (12, 44, 138),  Best Validation Loss: 0.6928\n",
      "Steps: 31, Number of layers: (149, 142, 287),  Best Validation Loss: 0.6941\n",
      "Steps: 32, Number of layers: (249, 154, 237),  Best Validation Loss: 0.6939\n",
      "Steps: 33, Number of layers: (176, 280, 161),  Best Validation Loss: 0.6918\n",
      "Steps: 34, Number of layers: (192, 82, 197),  Best Validation Loss: 0.6923\n",
      "Steps: 35, Number of layers: (5, 159, 173),  Best Validation Loss: 0.6929\n",
      "Steps: 36, Number of layers: (292, 134, 293),  Best Validation Loss: 0.6908\n",
      "Steps: 37, Number of layers: (163, 228, 297),  Best Validation Loss: 0.6913\n",
      "Steps: 38, Number of layers: (114, 144, 33),  Best Validation Loss: 0.6929\n",
      "Steps: 39, Number of layers: (262, 257, 207),  Best Validation Loss: 0.6912\n",
      "Steps: 40, Number of layers: (16, 195, 41),  Best Validation Loss: 0.6934\n",
      "Steps: 41, Number of layers: (179, 221, 290),  Best Validation Loss: 0.6926\n",
      "Steps: 42, Number of layers: (52, 91, 36),  Best Validation Loss: 0.6928\n",
      "Steps: 43, Number of layers: (150, 241, 267),  Best Validation Loss: 0.6905\n",
      "Steps: 44, Number of layers: (187, 284, 5),  Best Validation Loss: 0.6973\n",
      "Steps: 45, Number of layers: (34, 93, 187),  Best Validation Loss: 0.6931\n",
      "Steps: 46, Number of layers: (272, 207, 120),  Best Validation Loss: 0.6938\n",
      "Steps: 47, Number of layers: (17, 175, 83),  Best Validation Loss: 0.6926\n",
      "Steps: 48, Number of layers: (10, 88, 181),  Best Validation Loss: 0.6927\n",
      "Steps: 49, Number of layers: (128, 85, 60),  Best Validation Loss: 0.6936\n",
      "Steps: 50, Number of layers: (93, 297, 273),  Best Validation Loss: 0.6912\n",
      "Steps: 51, Number of layers: (7, 93, 124),  Best Validation Loss: 0.6937\n",
      "Steps: 52, Number of layers: (192, 239, 68),  Best Validation Loss: 0.6920\n",
      "Steps: 53, Number of layers: (46, 129, 239),  Best Validation Loss: 0.6938\n",
      "Steps: 54, Number of layers: (139, 178, 198),  Best Validation Loss: 0.6899\n",
      "Steps: 55, Number of layers: (143, 76, 82),  Best Validation Loss: 0.6914\n",
      "Steps: 56, Number of layers: (239, 199, 295),  Best Validation Loss: 0.6927\n",
      "Steps: 57, Number of layers: (168, 262, 14),  Best Validation Loss: 0.6941\n",
      "Steps: 58, Number of layers: (52, 288, 95),  Best Validation Loss: 0.6934\n",
      "Steps: 59, Number of layers: (61, 7, 106),  Best Validation Loss: 0.6933\n",
      "Steps: 60, Number of layers: (25, 261, 149),  Best Validation Loss: 0.6920\n",
      "Steps: 61, Number of layers: (198, 271, 37),  Best Validation Loss: 0.6908\n",
      "Steps: 62, Number of layers: (109, 55, 297),  Best Validation Loss: 0.6925\n",
      "Steps: 63, Number of layers: (205, 284, 231),  Best Validation Loss: 0.6907\n",
      "Steps: 64, Number of layers: (181, 145, 22),  Best Validation Loss: 0.6932\n",
      "Steps: 65, Number of layers: (206, 224, 56),  Best Validation Loss: 0.6910\n",
      "Steps: 66, Number of layers: (18, 205, 155),  Best Validation Loss: 0.6930\n",
      "Steps: 67, Number of layers: (196, 238, 166),  Best Validation Loss: 0.6894\n",
      "Steps: 68, Number of layers: (152, 146, 249),  Best Validation Loss: 0.6918\n",
      "Steps: 69, Number of layers: (244, 61, 204),  Best Validation Loss: 0.6931\n",
      "Steps: 70, Number of layers: (63, 78, 221),  Best Validation Loss: 0.6926\n",
      "Steps: 71, Number of layers: (14, 174, 14),  Best Validation Loss: 0.6935\n",
      "Steps: 72, Number of layers: (267, 202, 251),  Best Validation Loss: 0.6938\n",
      "Steps: 73, Number of layers: (90, 25, 153),  Best Validation Loss: 0.6930\n",
      "Steps: 74, Number of layers: (189, 179, 50),  Best Validation Loss: 0.6931\n",
      "Steps: 75, Number of layers: (32, 120, 207),  Best Validation Loss: 0.6939\n",
      "Steps: 76, Number of layers: (154, 171, 34),  Best Validation Loss: 0.6926\n",
      "Steps: 77, Number of layers: (59, 213, 103),  Best Validation Loss: 0.6928\n",
      "Steps: 78, Number of layers: (110, 120, 267),  Best Validation Loss: 0.6921\n",
      "Steps: 79, Number of layers: (219, 157, 284),  Best Validation Loss: 0.6913\n",
      "Steps: 80, Number of layers: (81, 61, 6),  Best Validation Loss: 0.6928\n",
      "Steps: 81, Number of layers: (96, 54, 117),  Best Validation Loss: 0.6937\n",
      "Steps: 82, Number of layers: (160, 115, 123),  Best Validation Loss: 0.6899\n",
      "Steps: 83, Number of layers: (219, 288, 170),  Best Validation Loss: 0.6929\n",
      "Steps: 84, Number of layers: (224, 83, 170),  Best Validation Loss: 0.6924\n",
      "Steps: 85, Number of layers: (77, 140, 61),  Best Validation Loss: 0.6924\n",
      "Steps: 86, Number of layers: (82, 216, 147),  Best Validation Loss: 0.6939\n",
      "Steps: 87, Number of layers: (204, 87, 240),  Best Validation Loss: 0.6915\n",
      "Steps: 88, Number of layers: (151, 271, 226),  Best Validation Loss: 0.6934\n",
      "Steps: 89, Number of layers: (261, 255, 41),  Best Validation Loss: 0.6948\n",
      "Steps: 90, Number of layers: (233, 196, 97),  Best Validation Loss: 0.6942\n",
      "Steps: 91, Number of layers: (89, 68, 36),  Best Validation Loss: 0.6934\n",
      "Steps: 92, Number of layers: (214, 15, 297),  Best Validation Loss: 0.6935\n",
      "Steps: 93, Number of layers: (219, 174, 293),  Best Validation Loss: 0.6943\n",
      "Steps: 94, Number of layers: (35, 105, 106),  Best Validation Loss: 0.6919\n",
      "Steps: 95, Number of layers: (25, 20, 84),  Best Validation Loss: 0.6931\n",
      "Steps: 96, Number of layers: (246, 145, 84),  Best Validation Loss: 0.6896\n",
      "Steps: 97, Number of layers: (236, 237, 96),  Best Validation Loss: 0.6904\n",
      "Steps: 98, Number of layers: (295, 93, 160),  Best Validation Loss: 0.6919\n",
      "Steps: 99, Number of layers: (291, 295, 112),  Best Validation Loss: 0.6878\n",
      "Steps: 100, Number of layers: (196, 169, 103),  Best Validation Loss: 0.6923\n",
      "Steps: 101, Number of layers: (229, 245, 35),  Best Validation Loss: 0.6870\n",
      "Steps: 102, Number of layers: (16, 136, 280),  Best Validation Loss: 0.6934\n",
      "Steps: 103, Number of layers: (239, 96, 57),  Best Validation Loss: 0.6928\n",
      "Steps: 104, Number of layers: (13, 105, 198),  Best Validation Loss: 0.6919\n",
      "Steps: 105, Number of layers: (165, 148, 65),  Best Validation Loss: 0.6897\n",
      "Steps: 106, Number of layers: (24, 239, 146),  Best Validation Loss: 0.6912\n",
      "Steps: 107, Number of layers: (296, 207, 219),  Best Validation Loss: 0.6885\n",
      "Steps: 108, Number of layers: (232, 152, 278),  Best Validation Loss: 0.6940\n",
      "Steps: 109, Number of layers: (176, 281, 190),  Best Validation Loss: 0.6902\n",
      "Steps: 110, Number of layers: (104, 129, 99),  Best Validation Loss: 0.6919\n",
      "Steps: 111, Number of layers: (226, 139, 183),  Best Validation Loss: 0.6921\n",
      "Steps: 112, Number of layers: (265, 277, 196),  Best Validation Loss: 0.6922\n",
      "Steps: 113, Number of layers: (124, 255, 130),  Best Validation Loss: 0.6894\n",
      "Steps: 114, Number of layers: (7, 97, 193),  Best Validation Loss: 0.6939\n",
      "Steps: 115, Number of layers: (128, 107, 164),  Best Validation Loss: 0.6940\n",
      "Steps: 116, Number of layers: (23, 127, 47),  Best Validation Loss: 0.6927\n",
      "Steps: 117, Number of layers: (86, 254, 27),  Best Validation Loss: 0.6917\n",
      "Steps: 118, Number of layers: (52, 219, 143),  Best Validation Loss: 0.6933\n",
      "Steps: 119, Number of layers: (264, 92, 211),  Best Validation Loss: 0.6909\n",
      "Steps: 120, Number of layers: (222, 268, 17),  Best Validation Loss: 0.6936\n",
      "Steps: 121, Number of layers: (261, 114, 134),  Best Validation Loss: 0.6948\n",
      "Steps: 122, Number of layers: (177, 12, 103),  Best Validation Loss: 0.6930\n",
      "Steps: 123, Number of layers: (49, 166, 185),  Best Validation Loss: 0.6929\n",
      "Steps: 124, Number of layers: (58, 260, 203),  Best Validation Loss: 0.6924\n",
      "Steps: 125, Number of layers: (129, 37, 79),  Best Validation Loss: 0.6940\n",
      "Steps: 126, Number of layers: (151, 254, 86),  Best Validation Loss: 0.6907\n",
      "Steps: 127, Number of layers: (245, 71, 269),  Best Validation Loss: 0.6935\n",
      "Steps: 128, Number of layers: (180, 225, 147),  Best Validation Loss: 0.6946\n",
      "Steps: 129, Number of layers: (283, 51, 180),  Best Validation Loss: 0.6925\n",
      "Steps: 130, Number of layers: (109, 80, 127),  Best Validation Loss: 0.6924\n",
      "Steps: 131, Number of layers: (134, 136, 241),  Best Validation Loss: 0.6944\n",
      "Steps: 132, Number of layers: (82, 239, 84),  Best Validation Loss: 0.6935\n",
      "Steps: 133, Number of layers: (165, 250, 120),  Best Validation Loss: 0.6930\n",
      "Steps: 134, Number of layers: (217, 53, 283),  Best Validation Loss: 0.6928\n",
      "Steps: 135, Number of layers: (295, 205, 57),  Best Validation Loss: 0.6916\n",
      "Steps: 136, Number of layers: (111, 14, 249),  Best Validation Loss: 0.6928\n",
      "Steps: 137, Number of layers: (77, 79, 279),  Best Validation Loss: 0.6923\n",
      "Steps: 138, Number of layers: (243, 277, 43),  Best Validation Loss: 0.6935\n",
      "Steps: 139, Number of layers: (102, 237, 21),  Best Validation Loss: 0.6929\n",
      "Steps: 140, Number of layers: (214, 197, 200),  Best Validation Loss: 0.6925\n",
      "Steps: 141, Number of layers: (102, 18, 286),  Best Validation Loss: 0.6923\n",
      "Steps: 142, Number of layers: (200, 203, 125),  Best Validation Loss: 0.6915\n",
      "Steps: 143, Number of layers: (146, 188, 282),  Best Validation Loss: 0.6919\n",
      "Steps: 144, Number of layers: (289, 140, 11),  Best Validation Loss: 0.6934\n",
      "Steps: 145, Number of layers: (210, 149, 5),  Best Validation Loss: 0.6908\n",
      "Steps: 146, Number of layers: (276, 90, 268),  Best Validation Loss: 0.6919\n",
      "Steps: 147, Number of layers: (17, 129, 177),  Best Validation Loss: 0.6925\n",
      "Steps: 148, Number of layers: (175, 247, 171),  Best Validation Loss: 0.6928\n",
      "Steps: 149, Number of layers: (229, 196, 149),  Best Validation Loss: 0.6904\n",
      "Steps: 150, Number of layers: (239, 161, 54),  Best Validation Loss: 0.6904\n",
      "Steps: 151, Number of layers: (22, 236, 251),  Best Validation Loss: 0.6951\n",
      "Steps: 152, Number of layers: (36, 245, 84),  Best Validation Loss: 0.6927\n",
      "Steps: 153, Number of layers: (43, 116, 81),  Best Validation Loss: 0.6932\n",
      "Steps: 154, Number of layers: (67, 249, 40),  Best Validation Loss: 0.6925\n",
      "Steps: 155, Number of layers: (110, 64, 115),  Best Validation Loss: 0.6927\n",
      "Steps: 156, Number of layers: (143, 285, 91),  Best Validation Loss: 0.6920\n",
      "Steps: 157, Number of layers: (266, 185, 298),  Best Validation Loss: 0.6932\n",
      "Steps: 158, Number of layers: (192, 290, 33),  Best Validation Loss: 0.6893\n",
      "Steps: 159, Number of layers: (246, 32, 79),  Best Validation Loss: 0.6922\n",
      "Steps: 160, Number of layers: (93, 150, 216),  Best Validation Loss: 0.6940\n",
      "Steps: 161, Number of layers: (164, 96, 35),  Best Validation Loss: 0.6921\n",
      "Steps: 162, Number of layers: (284, 215, 42),  Best Validation Loss: 0.6904\n",
      "Steps: 163, Number of layers: (40, 279, 252),  Best Validation Loss: 0.6930\n",
      "Steps: 164, Number of layers: (229, 292, 269),  Best Validation Loss: 0.6862\n",
      "Steps: 165, Number of layers: (199, 188, 195),  Best Validation Loss: 0.6912\n",
      "Steps: 166, Number of layers: (128, 148, 277),  Best Validation Loss: 0.6939\n",
      "Steps: 167, Number of layers: (81, 220, 151),  Best Validation Loss: 0.6903\n",
      "Steps: 168, Number of layers: (53, 36, 27),  Best Validation Loss: 0.6936\n",
      "Steps: 169, Number of layers: (256, 25, 86),  Best Validation Loss: 0.6923\n",
      "Steps: 170, Number of layers: (135, 55, 178),  Best Validation Loss: 0.6933\n",
      "Steps: 171, Number of layers: (109, 247, 133),  Best Validation Loss: 0.6912\n",
      "Steps: 172, Number of layers: (49, 291, 186),  Best Validation Loss: 0.6924\n",
      "Steps: 173, Number of layers: (255, 73, 128),  Best Validation Loss: 0.6944\n",
      "Steps: 174, Number of layers: (292, 139, 124),  Best Validation Loss: 0.6903\n",
      "Steps: 175, Number of layers: (279, 229, 83),  Best Validation Loss: 0.6916\n",
      "Steps: 176, Number of layers: (145, 147, 161),  Best Validation Loss: 0.6918\n",
      "Steps: 177, Number of layers: (298, 159, 167),  Best Validation Loss: 0.6913\n",
      "Steps: 178, Number of layers: (72, 281, 292),  Best Validation Loss: 0.6937\n",
      "Steps: 179, Number of layers: (204, 111, 15),  Best Validation Loss: 0.6911\n",
      "Steps: 180, Number of layers: (112, 123, 267),  Best Validation Loss: 0.6933\n",
      "Steps: 181, Number of layers: (193, 249, 65),  Best Validation Loss: 0.6962\n",
      "Steps: 182, Number of layers: (73, 72, 119),  Best Validation Loss: 0.6933\n",
      "Steps: 183, Number of layers: (222, 207, 81),  Best Validation Loss: 0.6921\n",
      "Steps: 184, Number of layers: (15, 211, 29),  Best Validation Loss: 0.6941\n",
      "Steps: 185, Number of layers: (5, 95, 66),  Best Validation Loss: 0.6930\n",
      "Steps: 186, Number of layers: (217, 220, 271),  Best Validation Loss: 0.6899\n",
      "Steps: 187, Number of layers: (148, 37, 239),  Best Validation Loss: 0.6935\n",
      "Steps: 188, Number of layers: (124, 259, 11),  Best Validation Loss: 0.6938\n",
      "Steps: 189, Number of layers: (149, 210, 54),  Best Validation Loss: 0.6916\n",
      "Steps: 190, Number of layers: (131, 43, 124),  Best Validation Loss: 0.6931\n",
      "Steps: 191, Number of layers: (160, 189, 144),  Best Validation Loss: 0.6932\n",
      "Steps: 192, Number of layers: (95, 286, 156),  Best Validation Loss: 0.6922\n",
      "Steps: 193, Number of layers: (174, 222, 24),  Best Validation Loss: 0.6958\n",
      "Steps: 194, Number of layers: (130, 220, 74),  Best Validation Loss: 0.6911\n",
      "Steps: 195, Number of layers: (291, 180, 150),  Best Validation Loss: 0.6912\n",
      "Steps: 196, Number of layers: (180, 168, 56),  Best Validation Loss: 0.6914\n",
      "Steps: 197, Number of layers: (44, 225, 16),  Best Validation Loss: 0.6913\n",
      "Steps: 198, Number of layers: (166, 255, 130),  Best Validation Loss: 0.6886\n",
      "Steps: 199, Number of layers: (226, 218, 105),  Best Validation Loss: 0.6932\n",
      "Steps: 200, Number of layers: (73, 27, 142),  Best Validation Loss: 0.6930\n"
     ]
    }
   ],
   "source": [
    "#Initiate empty list to store best validation loss for a particular number of hidden layers\n",
    "hd = []\n",
    "hd_loss = []\n",
    "steps = 1\n",
    "\n",
    "#Loop through the learning rate and train network\n",
    "for h in zip(np.random.randint(5,300, n_trials), np.random.randint(5,300, n_trials), np.random.randint(5,300, n_trials)): \n",
    "    \n",
    "    hd_network = build_network(h, [0,0], input_shape=(timesteps, n_dim), batch_size=batch_size)\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optimizers.Adam(lr=1e-5)\n",
    "    \n",
    "    #Compile\n",
    "    hd_network.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    #Train\n",
    "    fitted_hidden = hd_network.fit(X_train_1, y_train_1, batch_size=batch_size, epochs=3, verbose=0, validation_data=(X_val_1, y_val_1), shuffle=True)\n",
    "    \n",
    "    #Min loss\n",
    "    val_loss = np.min(fitted_hidden.history['val_loss'])\n",
    "    \n",
    "    #Display\n",
    "    print('Steps: %d,' % steps, 'Number of layers: (%d, %d, %d), ' % (h[0], h[1], h[2]), 'Best Validation Loss: %.4f' %  val_loss)\n",
    "    \n",
    "    #Save\n",
    "    hd.append(h)\n",
    "    hd_loss.append(val_loss)\n",
    "    \n",
    "    #Update\n",
    "    steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Number of Hidden Layer:  (229, 292, 269)  Validation Loss: 0.6862 \n",
      "Stored 'best_n_hidden' (tuple)\n",
      "Stored 'hd_loss' (list)\n"
     ]
    }
   ],
   "source": [
    "#Best number of hidden layer\n",
    "best_hidden_loss = np.min(hd_loss)\n",
    "best_n_hidden = hd[np.argmin(hd_loss)]\n",
    "print('Best Number of Hidden Layer: ', best_n_hidden, \" Validation Loss: %.4f \" % best_hidden_loss)\n",
    "%store best_n_hidden\n",
    "%store hd_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initiate empty list to store best validation loss for a particular learning rate\n",
    "learnrate = []\n",
    "lr_loss = []\n",
    "steps = 1\n",
    "\n",
    "lr = np.random.uniform(1e-1, 1e-5, n_trials)\n",
    "lr_network = build_network(best_n_hidden, [0,0], input_shape=(timesteps, n_dim), batch_size=batch_size)\n",
    "\n",
    "#Loop through the learning rate and train network\n",
    "for learning_rate in lr:\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optimizers.Adam(lr=learning_rate)\n",
    "    \n",
    "    #Compile\n",
    "    lr_network.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    #Train\n",
    "    fitted_lr = lr_network.fit(X_train_1, y_train_1, batch_size=batch_size, epochs=3, verbose=0, validation_data=(X_val_1, y_val_1), shuffle=True)\n",
    "    \n",
    "    #Min Validation loss\n",
    "    val_loss = np.min(fitted_lr.history['val_loss'])\n",
    "    \n",
    "    #Display\n",
    "    print('Steps: %d, learning rate: %.6f, Best Validation Loss: %.4f' %(steps, learning_rate, val_loss))\n",
    "          \n",
    "    #Save\n",
    "    learnrate.append(learning_rate)\n",
    "    lr_loss.append(val_loss)\n",
    "    \n",
    "    #Update\n",
    "    steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Best Learning Rate\n",
    "best_lr_loss = np.min(lr_loss)\n",
    "best_lr = learnrate[np.argmin(lr_loss)]\n",
    "print('Best Learning Rate: %.4f' % best_lr, \" Validation Loss: %.4f \" % best_lr_loss)\n",
    "%store best_lr\n",
    "%store lr_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_out = zip(np.random.uniform(0.05, 0.8, n_trials), np.random.uniform(0.05, 0.8, n_trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initiate empty list to store best validation loss for a particular learning rate\n",
    "do = []\n",
    "do_loss = []\n",
    "steps = 1\n",
    "\n",
    "#Loop through the learning rate and train network\n",
    "for d in zip(np.random.uniform(0.05, 0.8, n_trials), np.random.uniform(0.05, 0.8, n_trials)):\n",
    "\n",
    "    do_network = build_network(best_n_hidden, [0,0], input_shape=(timesteps, n_dim), batch_size=batch_size)\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optimizers.Adam(lr=best_lr)\n",
    "    \n",
    "    #Compile\n",
    "    do_network.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    #Train\n",
    "    fitted_dropout = do_network.fit(X_train_1, y_train_1, batch_size=batch_size, epochs=3, verbose=0, validation_data=(X_val_1, y_val_1), shuffle=True)\n",
    "    \n",
    "    #Min loss\n",
    "    val_loss = np.min(fitted_dropout.history['val_loss'])\n",
    "    \n",
    "    #Display\n",
    "    print('Steps: %d,' % steps, 'Dropout: (%.4f, %.4f), ' % (d[0], d[1]), 'Best Validation Loss: %.4f' %  val_loss)\n",
    "    \n",
    "    #Save\n",
    "    do.append(d)\n",
    "    do_loss.append(val_loss)\n",
    "    \n",
    "    #Update\n",
    "    steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_do_loss = np.min(do_loss)\n",
    "best_drop_out = do[np.argmin(do_loss)]\n",
    "print('Best Drop out probability: (%.2f, %.2f) ' % (best_drop_out[0], best_drop_out[1]), 'Validation Loss: %.4f' % best_do_loss)\n",
    "%store best_drop_out\n",
    "%store do_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timesteps = np.random.choice(np.arange(5,250), n_trials, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initiate empty list to store best validation loss for a particular timestep\n",
    "t_loss = []\n",
    "steps = 1\n",
    "ts = []\n",
    "\n",
    "#Loop through the learning rate and train network\n",
    "for t in timesteps:\n",
    "    \n",
    "    #Generate inputs on timesteps\n",
    "    X_train_2, y_train_2 = get_inputs(X_train, y_train, batch_size, t)\n",
    "    X_val_2, y_val_2 = get_inputs(X_val, y_val, batch_size, t)\n",
    "    \n",
    "    #Build network\n",
    "    t_network = build_network(best_n_hidden, best_drop_out, input_shape=(t, n_dim), batch_size=batch_size)\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optimizers.Adam(lr=best_lr)\n",
    "    \n",
    "    #Compile\n",
    "    t_network.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    #Train\n",
    "    fitted_timesteps = t_network.fit(X_train_2, y_train_2, batch_size=batch_size, epochs=1, verbose=0, validation_data=(X_val_2, y_val_2), shuffle=True)\n",
    "    \n",
    "    #Min Validation loss\n",
    "    val_loss = np.min(fitted_timesteps.history['val_loss'])\n",
    "    \n",
    "    #Display\n",
    "    print('Steps: %d,' % steps, 'Timesteps: %d' % t, 'Best Validation Loss: %.4f' % val_loss)\n",
    "    \n",
    "    #Save\n",
    "    ts.append(t)\n",
    "    t_loss.append(val_loss)\n",
    "    \n",
    "    #Update\n",
    "    steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_t_loss = np.min(t_loss)\n",
    "best_timestep = ts[np.argmin(t_loss)]\n",
    "print('Best Timestep: %d , ' % best_timestep, 'Validation Loss: %.4f' % best_t_loss)\n",
    "\n",
    "%store best_timestep\n",
    "%store t_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Summary')\n",
    "print('-------------')\n",
    "print('Learning Rate: %.5f' % best_lr)\n",
    "print('Number of layers: ', best_n_hidden)\n",
    "print('Dropout Probability: (%.2f, %.2f)' % best_drop_out)\n",
    "print('Timesteps: ', best_timestep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 7: Refit using best parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In case the best parameters are not loaded..\n",
    "best_drop_out = (0.25, 0.25)\n",
    "best_lr = 5e-5\n",
    "best_n_hidden = (45, 25, 50)\n",
    "best_timestep = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_best, y_train_best = get_inputs(X_train, y_train, batch_size, best_timestep)\n",
    "X_val_best, y_val_best = get_inputs(X_val, y_val, batch_size, best_timestep)\n",
    "X_test_best, y_test_best = get_inputs(X_test, y_test, batch_size, best_timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "#Precision\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "#Recall\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "#F1\n",
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score\n",
    "\n",
    "#Matthews Correlation\n",
    "def mcor(y_true, y_pred):\n",
    "    #matthews_correlation\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    " \n",
    " \n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    " \n",
    " \n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    " \n",
    " \n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    " \n",
    " \n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) \n",
    " \n",
    "    return numerator / (denominator + K.epsilon())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Refit using tuned parameters\n",
    "best_network = build_network(best_n_hidden, best_drop_out, input_shape=(best_timestep, n_dim), batch_size=batch_size)\n",
    "optimizer = optimizers.Adam(lr=best_lr)\n",
    "best_network.compile(loss=loss, optimizer=optimizer, metrics=['accuracy', mcor, f1_score, precision, recall])\n",
    "\n",
    "best_fit = best_network.fit(X_train_best, y_train_best, \n",
    "                            batch_size=batch_size, \n",
    "                            epochs=500, \n",
    "                            verbose=1, \n",
    "                            validation_data=(X_val_best, y_val_best), \n",
    "                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save model\n",
    "best_network.save('best_fit.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to load saved model\n",
    "from keras.models import load_model\n",
    "best_network = load_model('best_fit.h5', custom_objects={'mcor': mcor, 'precision': precision, 'recall': recall, 'f1_score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot metrics\n",
    "plot_metric(best_fit, 'loss')\n",
    "plot_metric(best_fit, 'acc')\n",
    "plot_metric(best_fit, 'mcor')\n",
    "plot_metric(best_fit, 'f1_score')\n",
    "plot_metric(best_fit, 'precision')\n",
    "plot_metric(best_fit, 'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test Loss\n",
    "test_loss = best_network.evaluate(X_test_best, y_test_best, batch_size)\n",
    "plt.plot(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = best_network.predict(X_test_best, batch_size=batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_best.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
