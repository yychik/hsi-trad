{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction As Classification\n",
    "Continuing the 2800-HK price prediction from classification perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Modules and load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Keras module\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pretty plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>P_L1</th>\n",
       "      <th>P_L2</th>\n",
       "      <th>P_L3</th>\n",
       "      <th>P_L4</th>\n",
       "      <th>P_L5</th>\n",
       "      <th>P_L6</th>\n",
       "      <th>P_L7</th>\n",
       "      <th>P_L8</th>\n",
       "      <th>P_L9</th>\n",
       "      <th>...</th>\n",
       "      <th>P_L11</th>\n",
       "      <th>P_L12</th>\n",
       "      <th>P_L13</th>\n",
       "      <th>P_L14</th>\n",
       "      <th>P_L15</th>\n",
       "      <th>P_L16</th>\n",
       "      <th>P_L17</th>\n",
       "      <th>P_L18</th>\n",
       "      <th>P_L19</th>\n",
       "      <th>P_L20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-08-31</th>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>23.35</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.05</td>\n",
       "      <td>21.85</td>\n",
       "      <td>...</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.85</td>\n",
       "      <td>22.15</td>\n",
       "      <td>22.25</td>\n",
       "      <td>22.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-03</th>\n",
       "      <td>24.30</td>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>23.35</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.05</td>\n",
       "      <td>...</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.85</td>\n",
       "      <td>22.15</td>\n",
       "      <td>22.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-04</th>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>23.35</td>\n",
       "      <td>22.70</td>\n",
       "      <td>...</td>\n",
       "      <td>21.85</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.85</td>\n",
       "      <td>22.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-05</th>\n",
       "      <td>24.35</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>23.35</td>\n",
       "      <td>...</td>\n",
       "      <td>22.05</td>\n",
       "      <td>21.85</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-06</th>\n",
       "      <td>24.50</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>...</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.05</td>\n",
       "      <td>21.85</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                P   P_L1   P_L2   P_L3   P_L4   P_L5   P_L6   P_L7   P_L8  \\\n",
       "Date                                                                        \n",
       "2007-08-31  24.35  23.80  23.35  23.70  23.90  23.30  23.35  22.70  22.05   \n",
       "2007-09-03  24.30  24.35  23.80  23.35  23.70  23.90  23.30  23.35  22.70   \n",
       "2007-09-04  24.30  24.30  24.35  23.80  23.35  23.70  23.90  23.30  23.35   \n",
       "2007-09-05  24.35  24.30  24.30  24.35  23.80  23.35  23.70  23.90  23.30   \n",
       "2007-09-06  24.50  24.35  24.30  24.30  24.35  23.80  23.35  23.70  23.90   \n",
       "\n",
       "             P_L9  ...    P_L11  P_L12  P_L13  P_L14  P_L15  P_L16  P_L17  \\\n",
       "Date               ...                                                      \n",
       "2007-08-31  21.85  ...    20.95  21.70  22.35  22.30  22.05  22.70  22.85   \n",
       "2007-09-03  22.05  ...    20.80  20.95  21.70  22.35  22.30  22.05  22.70   \n",
       "2007-09-04  22.70  ...    21.85  20.80  20.95  21.70  22.35  22.30  22.05   \n",
       "2007-09-05  23.35  ...    22.05  21.85  20.80  20.95  21.70  22.35  22.30   \n",
       "2007-09-06  23.30  ...    22.70  22.05  21.85  20.80  20.95  21.70  22.35   \n",
       "\n",
       "            P_L18  P_L19  P_L20  \n",
       "Date                             \n",
       "2007-08-31  22.15  22.25  22.85  \n",
       "2007-09-03  22.85  22.15  22.25  \n",
       "2007-09-04  22.70  22.85  22.15  \n",
       "2007-09-05  22.05  22.70  22.85  \n",
       "2007-09-06  22.30  22.05  22.70  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import price data\n",
    "#Load the historical prices of 2800-HK, with lags 1 to lag 20\n",
    "price_hist_data = pd.read_csv('price_only.csv', skiprows=1, parse_dates=['Date']).set_index(['Date'])\n",
    "price_hist_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Ask</th>\n",
       "      <th>Bid</th>\n",
       "      <th>20D Vol</th>\n",
       "      <th>MA5</th>\n",
       "      <th>MA15</th>\n",
       "      <th>MA12</th>\n",
       "      <th>MA20</th>\n",
       "      <th>...</th>\n",
       "      <th>DY_LTM</th>\n",
       "      <th>DY_NTM</th>\n",
       "      <th>ADV_VOL</th>\n",
       "      <th>PAYOUT</th>\n",
       "      <th>ANALYST_SENTIMENT</th>\n",
       "      <th>EPS_GRW_FY1</th>\n",
       "      <th>EPS_GRW_FY2</th>\n",
       "      <th>PE_NTM</th>\n",
       "      <th>PE_LTM</th>\n",
       "      <th>C2D_LTM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-08-31</th>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>2.367823</td>\n",
       "      <td>23.82</td>\n",
       "      <td>22.841667</td>\n",
       "      <td>22.696667</td>\n",
       "      <td>22.6225</td>\n",
       "      <td>...</td>\n",
       "      <td>2.782797</td>\n",
       "      <td>2.880128</td>\n",
       "      <td>99.419898</td>\n",
       "      <td>48.051962</td>\n",
       "      <td>1.909071</td>\n",
       "      <td>30.724490</td>\n",
       "      <td>2.717280</td>\n",
       "      <td>16.695058</td>\n",
       "      <td>16.975805</td>\n",
       "      <td>58.050474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-03</th>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>2.266743</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.120832</td>\n",
       "      <td>22.830000</td>\n",
       "      <td>22.7250</td>\n",
       "      <td>...</td>\n",
       "      <td>2.784563</td>\n",
       "      <td>2.892308</td>\n",
       "      <td>11.570023</td>\n",
       "      <td>48.305678</td>\n",
       "      <td>1.835359</td>\n",
       "      <td>31.005439</td>\n",
       "      <td>1.876752</td>\n",
       "      <td>16.709085</td>\n",
       "      <td>16.925571</td>\n",
       "      <td>58.154835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-04</th>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>2.259649</td>\n",
       "      <td>24.02</td>\n",
       "      <td>23.412500</td>\n",
       "      <td>22.960000</td>\n",
       "      <td>22.8325</td>\n",
       "      <td>...</td>\n",
       "      <td>2.784339</td>\n",
       "      <td>2.894377</td>\n",
       "      <td>69.555725</td>\n",
       "      <td>48.307102</td>\n",
       "      <td>1.725886</td>\n",
       "      <td>31.040968</td>\n",
       "      <td>1.891823</td>\n",
       "      <td>16.697662</td>\n",
       "      <td>16.919413</td>\n",
       "      <td>58.177640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-05</th>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>2.172140</td>\n",
       "      <td>24.22</td>\n",
       "      <td>23.620832</td>\n",
       "      <td>23.136667</td>\n",
       "      <td>22.9075</td>\n",
       "      <td>...</td>\n",
       "      <td>2.767348</td>\n",
       "      <td>2.867314</td>\n",
       "      <td>24.265623</td>\n",
       "      <td>48.267351</td>\n",
       "      <td>1.741908</td>\n",
       "      <td>30.716929</td>\n",
       "      <td>1.878484</td>\n",
       "      <td>16.841225</td>\n",
       "      <td>17.017692</td>\n",
       "      <td>58.195446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-06</th>\n",
       "      <td>24.50</td>\n",
       "      <td>24.50</td>\n",
       "      <td>24.50</td>\n",
       "      <td>24.50</td>\n",
       "      <td>24.50</td>\n",
       "      <td>2.160638</td>\n",
       "      <td>24.36</td>\n",
       "      <td>23.825000</td>\n",
       "      <td>23.373333</td>\n",
       "      <td>22.9975</td>\n",
       "      <td>...</td>\n",
       "      <td>2.775339</td>\n",
       "      <td>2.878880</td>\n",
       "      <td>88.845002</td>\n",
       "      <td>48.288397</td>\n",
       "      <td>1.696151</td>\n",
       "      <td>31.336520</td>\n",
       "      <td>1.861943</td>\n",
       "      <td>16.780660</td>\n",
       "      <td>16.975485</td>\n",
       "      <td>58.224743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Close   High    Low    Ask    Bid   20D Vol    MA5       MA15  \\\n",
       "Date                                                                        \n",
       "2007-08-31  24.35  24.35  24.35  24.35  24.35  2.367823  23.82  22.841667   \n",
       "2007-09-03  24.30  24.30  24.30  24.30  24.30  2.266743  23.90  23.120832   \n",
       "2007-09-04  24.30  24.30  24.30  24.30  24.30  2.259649  24.02  23.412500   \n",
       "2007-09-05  24.35  24.35  24.35  24.35  24.35  2.172140  24.22  23.620832   \n",
       "2007-09-06  24.50  24.50  24.50  24.50  24.50  2.160638  24.36  23.825000   \n",
       "\n",
       "                 MA12     MA20    ...        DY_LTM    DY_NTM    ADV_VOL  \\\n",
       "Date                              ...                                      \n",
       "2007-08-31  22.696667  22.6225    ...      2.782797  2.880128  99.419898   \n",
       "2007-09-03  22.830000  22.7250    ...      2.784563  2.892308  11.570023   \n",
       "2007-09-04  22.960000  22.8325    ...      2.784339  2.894377  69.555725   \n",
       "2007-09-05  23.136667  22.9075    ...      2.767348  2.867314  24.265623   \n",
       "2007-09-06  23.373333  22.9975    ...      2.775339  2.878880  88.845002   \n",
       "\n",
       "               PAYOUT  ANALYST_SENTIMENT  EPS_GRW_FY1  EPS_GRW_FY2     PE_NTM  \\\n",
       "Date                                                                            \n",
       "2007-08-31  48.051962           1.909071    30.724490     2.717280  16.695058   \n",
       "2007-09-03  48.305678           1.835359    31.005439     1.876752  16.709085   \n",
       "2007-09-04  48.307102           1.725886    31.040968     1.891823  16.697662   \n",
       "2007-09-05  48.267351           1.741908    30.716929     1.878484  16.841225   \n",
       "2007-09-06  48.288397           1.696151    31.336520     1.861943  16.780660   \n",
       "\n",
       "               PE_LTM    C2D_LTM  \n",
       "Date                              \n",
       "2007-08-31  16.975805  58.050474  \n",
       "2007-09-03  16.925571  58.154835  \n",
       "2007-09-04  16.919413  58.177640  \n",
       "2007-09-05  17.017692  58.195446  \n",
       "2007-09-06  16.975485  58.224743  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Fundamentals Data\n",
    "fund_data = pd.read_csv('new_index_data.csv', skiprows=1, parse_dates=['Date']).set_index(['Date'])\n",
    "fund_data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hang Seng Index</th>\n",
       "      <th>SSE Composite Index</th>\n",
       "      <th>ASX All Ordinaries</th>\n",
       "      <th>India S&amp;P BSE SENSEX</th>\n",
       "      <th>TOPIX</th>\n",
       "      <th>KOSPI Composite Index</th>\n",
       "      <th>Taiwan TAIEX</th>\n",
       "      <th>FTSE Bursa Malaysia KLCI</th>\n",
       "      <th>FTSE Straits Times Index</th>\n",
       "      <th>Philippines PSE PSEi</th>\n",
       "      <th>...</th>\n",
       "      <th>Turkey BIST 100</th>\n",
       "      <th>S&amp;P 500</th>\n",
       "      <th>DJ Industrial Average</th>\n",
       "      <th>Colombia IGBC</th>\n",
       "      <th>Canada S&amp;P/TSX Composite</th>\n",
       "      <th>Brazil Bovespa Index</th>\n",
       "      <th>Mexico IPC</th>\n",
       "      <th>Israel TA-125</th>\n",
       "      <th>Saudi Arabia All Share (TASI)</th>\n",
       "      <th>FTSE JSE All Share</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-08-31</th>\n",
       "      <td>23984.14</td>\n",
       "      <td>5218.825</td>\n",
       "      <td>6248.3</td>\n",
       "      <td>15318.60</td>\n",
       "      <td>1608.25</td>\n",
       "      <td>1873.24</td>\n",
       "      <td>8982.16</td>\n",
       "      <td>1273.93</td>\n",
       "      <td>3328.43</td>\n",
       "      <td>3365.29</td>\n",
       "      <td>...</td>\n",
       "      <td>50198.60</td>\n",
       "      <td>1473.99</td>\n",
       "      <td>13357.74</td>\n",
       "      <td>10728.74</td>\n",
       "      <td>13660.48</td>\n",
       "      <td>54637.24</td>\n",
       "      <td>30347.86</td>\n",
       "      <td>1034.67</td>\n",
       "      <td>8226.97</td>\n",
       "      <td>28660.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-03</th>\n",
       "      <td>23904.09</td>\n",
       "      <td>5321.055</td>\n",
       "      <td>6272.5</td>\n",
       "      <td>15422.05</td>\n",
       "      <td>1605.44</td>\n",
       "      <td>1881.81</td>\n",
       "      <td>8979.96</td>\n",
       "      <td>1284.14</td>\n",
       "      <td>3321.36</td>\n",
       "      <td>3369.14</td>\n",
       "      <td>...</td>\n",
       "      <td>49936.94</td>\n",
       "      <td>1473.99</td>\n",
       "      <td>13357.74</td>\n",
       "      <td>10750.79</td>\n",
       "      <td>13660.48</td>\n",
       "      <td>54832.51</td>\n",
       "      <td>30797.60</td>\n",
       "      <td>1047.33</td>\n",
       "      <td>8017.54</td>\n",
       "      <td>28887.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-04</th>\n",
       "      <td>23886.07</td>\n",
       "      <td>5294.045</td>\n",
       "      <td>6297.1</td>\n",
       "      <td>15465.40</td>\n",
       "      <td>1596.74</td>\n",
       "      <td>1874.74</td>\n",
       "      <td>8922.98</td>\n",
       "      <td>1283.75</td>\n",
       "      <td>3308.81</td>\n",
       "      <td>3312.30</td>\n",
       "      <td>...</td>\n",
       "      <td>50032.59</td>\n",
       "      <td>1489.42</td>\n",
       "      <td>13448.86</td>\n",
       "      <td>10880.85</td>\n",
       "      <td>13755.23</td>\n",
       "      <td>55250.47</td>\n",
       "      <td>30932.71</td>\n",
       "      <td>1054.69</td>\n",
       "      <td>7878.70</td>\n",
       "      <td>29051.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-05</th>\n",
       "      <td>24069.17</td>\n",
       "      <td>5310.716</td>\n",
       "      <td>6274.3</td>\n",
       "      <td>15446.15</td>\n",
       "      <td>1569.47</td>\n",
       "      <td>1865.59</td>\n",
       "      <td>8913.85</td>\n",
       "      <td>1297.93</td>\n",
       "      <td>3375.02</td>\n",
       "      <td>3342.35</td>\n",
       "      <td>...</td>\n",
       "      <td>49421.38</td>\n",
       "      <td>1472.29</td>\n",
       "      <td>13305.47</td>\n",
       "      <td>10819.91</td>\n",
       "      <td>13683.28</td>\n",
       "      <td>54407.83</td>\n",
       "      <td>30809.55</td>\n",
       "      <td>1048.70</td>\n",
       "      <td>7853.66</td>\n",
       "      <td>28696.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-06</th>\n",
       "      <td>24050.40</td>\n",
       "      <td>5393.660</td>\n",
       "      <td>6265.3</td>\n",
       "      <td>15616.31</td>\n",
       "      <td>1568.52</td>\n",
       "      <td>1888.81</td>\n",
       "      <td>9017.08</td>\n",
       "      <td>1298.85</td>\n",
       "      <td>3399.49</td>\n",
       "      <td>3326.53</td>\n",
       "      <td>...</td>\n",
       "      <td>49601.39</td>\n",
       "      <td>1478.55</td>\n",
       "      <td>13363.35</td>\n",
       "      <td>10844.40</td>\n",
       "      <td>13795.69</td>\n",
       "      <td>54569.00</td>\n",
       "      <td>30816.95</td>\n",
       "      <td>1033.23</td>\n",
       "      <td>7853.66</td>\n",
       "      <td>28850.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Hang Seng Index  SSE Composite Index  ASX All Ordinaries  \\\n",
       "Date                                                                   \n",
       "2007-08-31         23984.14             5218.825              6248.3   \n",
       "2007-09-03         23904.09             5321.055              6272.5   \n",
       "2007-09-04         23886.07             5294.045              6297.1   \n",
       "2007-09-05         24069.17             5310.716              6274.3   \n",
       "2007-09-06         24050.40             5393.660              6265.3   \n",
       "\n",
       "            India S&P BSE SENSEX    TOPIX  KOSPI Composite Index  \\\n",
       "Date                                                               \n",
       "2007-08-31              15318.60  1608.25                1873.24   \n",
       "2007-09-03              15422.05  1605.44                1881.81   \n",
       "2007-09-04              15465.40  1596.74                1874.74   \n",
       "2007-09-05              15446.15  1569.47                1865.59   \n",
       "2007-09-06              15616.31  1568.52                1888.81   \n",
       "\n",
       "            Taiwan TAIEX  FTSE Bursa Malaysia KLCI  FTSE Straits Times Index  \\\n",
       "Date                                                                           \n",
       "2007-08-31       8982.16                   1273.93                   3328.43   \n",
       "2007-09-03       8979.96                   1284.14                   3321.36   \n",
       "2007-09-04       8922.98                   1283.75                   3308.81   \n",
       "2007-09-05       8913.85                   1297.93                   3375.02   \n",
       "2007-09-06       9017.08                   1298.85                   3399.49   \n",
       "\n",
       "            Philippines PSE PSEi         ...          Turkey BIST 100  \\\n",
       "Date                                     ...                            \n",
       "2007-08-31               3365.29         ...                 50198.60   \n",
       "2007-09-03               3369.14         ...                 49936.94   \n",
       "2007-09-04               3312.30         ...                 50032.59   \n",
       "2007-09-05               3342.35         ...                 49421.38   \n",
       "2007-09-06               3326.53         ...                 49601.39   \n",
       "\n",
       "            S&P 500  DJ Industrial Average  Colombia IGBC  \\\n",
       "Date                                                        \n",
       "2007-08-31  1473.99               13357.74       10728.74   \n",
       "2007-09-03  1473.99               13357.74       10750.79   \n",
       "2007-09-04  1489.42               13448.86       10880.85   \n",
       "2007-09-05  1472.29               13305.47       10819.91   \n",
       "2007-09-06  1478.55               13363.35       10844.40   \n",
       "\n",
       "            Canada S&P/TSX Composite  Brazil Bovespa Index  Mexico IPC  \\\n",
       "Date                                                                     \n",
       "2007-08-31                  13660.48              54637.24    30347.86   \n",
       "2007-09-03                  13660.48              54832.51    30797.60   \n",
       "2007-09-04                  13755.23              55250.47    30932.71   \n",
       "2007-09-05                  13683.28              54407.83    30809.55   \n",
       "2007-09-06                  13795.69              54569.00    30816.95   \n",
       "\n",
       "            Israel TA-125  Saudi Arabia All Share (TASI)  FTSE JSE All Share  \n",
       "Date                                                                          \n",
       "2007-08-31        1034.67                        8226.97            28660.35  \n",
       "2007-09-03        1047.33                        8017.54            28887.48  \n",
       "2007-09-04        1054.69                        7878.70            29051.96  \n",
       "2007-09-05        1048.70                        7853.66            28696.67  \n",
       "2007-09-06        1033.23                        7853.66            28850.19  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Global index data\n",
    "idx_data = pd.read_csv('indices.csv', skiprows=1, parse_dates=['Date']).set_index(['Date'])\n",
    "idx_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pre-process Raw Data\n",
    "1. Generate labels\n",
    "2. Apply lags to global index data\n",
    "3. Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:10: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "#Generate Labels from Price History Data\n",
    "#Generate UP/DOWN labels from log change\n",
    "cutoff_perc = 0.0005 #0.05% return as cuttoff to define UP label\n",
    "lag = 1 #forward returns\n",
    "\n",
    "labels = np.zeros([price_hist_data.shape[0]])\n",
    "\n",
    "#Caluclate log-returns\n",
    "ret = np.log(price_hist_data['P'].shift(-lag)/price_hist_data['P'])\n",
    "labels[ret > cutoff_perc] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Applying lags to index data\n",
    "#Seperate the indices into 2 classes - lag or no_lag\n",
    "no_lag = [0, 1, 2, 4, 5, 6, 9, 10]\n",
    "lag = [i for i in range(0,idx_data.shape[1]) if i not in no_lag]\n",
    "\n",
    "#Processing the dataset by applying appropriate lags\n",
    "lagged_data = idx_data.iloc[:,lag].shift(1)\n",
    "idx_data = pd.concat([idx_data.iloc[:,no_lag], lagged_data], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove first row\n",
    "idx_data = idx_data.iloc[1:, :]\n",
    "price_hist_data = price_hist_data.iloc[1:, :]\n",
    "fund_data = fund_data.iloc[1:, :]\n",
    "labels=labels[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of idx_data:  (2483, 42)\n",
      "Shape of price_hist_data:  (2483, 21)\n",
      "Shape of fund_data:  (2483, 30)\n",
      "Shape of labels:  (2483,)\n"
     ]
    }
   ],
   "source": [
    "#Check Dimensions to make sure everythings right before continuing..\n",
    "print(\"Shape of idx_data: \", idx_data.shape)\n",
    "print(\"Shape of price_hist_data: \", price_hist_data.shape)\n",
    "print(\"Shape of fund_data: \", fund_data.shape)\n",
    "print(\"Shape of labels: \", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training data\n",
    "X = pd.concat([price_hist_data, idx_data, fund_data], axis=1)\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking whether there are NAs\n",
    "[np.sum(np.isnan(X), axis=0) > 0] == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.array(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Split training, validation and test set\n",
    "\n",
    "In this stage we have a look-ahead bias free set of data (X) and the labels y. Next, we will need to:\n",
    "- Normalize the input data. To avoid look ahead bias, we will z-score the features, using ONLY the training set.\n",
    "- Next, generate input data into LSTM network. We will need an overlapping sequence at 1-day window as input samples. Specifically suppose the *timestep* is 240, we will have a list of array consists of *number of rows of X* - 240 entries, each element has dimentions (240, num_of_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to split raw data into training, validation and test set, returns a numpy array.\n",
    "\n",
    "def split_data(input_data=[], train_size=0.8, val_size=0.2, test_size=0):\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    #PARAM: input_data: numpy nd array\n",
    "    #PARAM: training_size: size of training set in decimal\n",
    "    #PARAM: val_size: size of validation set in decimal\n",
    "    #PARAM: test_size: size of test set in decimal\n",
    "    #------------------------------------------------\n",
    "    \n",
    "    #First check whether traing_size + val_size + test_size = 1 and each of the entries are positive\n",
    "    assert(training_size + val_size + test_size==1), \"Sum of training, validation and test size needs to be 1!\"\n",
    "    assert(training_size * val_size * test_size > 0), \"Sizes have to be positive!\"\n",
    "    \n",
    "    #Check input_data type is numpy array, after casting\n",
    "    if type(input_data) != 'numpy.ndarray':\n",
    "        input_data = np.array(input_data)\n",
    "    else:\n",
    "        \n",
    "    assert(type(input_data) == 'numpy.ndarray'), \"Input has to be a numpy array!\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Calculate cut-off points\n",
    "    train_cut_index = int(train_size * input_data.shape[0])\n",
    "    val_cut_index = train_cut_index + int(val_size * input_data.shape[0])\n",
    "    \n",
    "    #Split the data\n",
    "    train, val, test = input_data[:train_cut_index,:], input_data[train_cut_index:val_cut_index, :], input_data[val_cut_index:, :]\n",
    "    \n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split into training, val and test set\n",
    "#Parameters\n",
    "train_size = 0.9\n",
    "val_size = ( 1- train_size) * 0.5\n",
    "timestep = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2230\n",
      "2350\n",
      "2480\n"
     ]
    }
   ],
   "source": [
    "#Calculate cutoff index values\n",
    "train_cut_index = int(train_size * X.shape[0] - train_size * X.shape[0] % timestep)\n",
    "val_cut_index = train_cut_index + int(val_size * X.shape[0] - val_size * X.shape[0] % timestep)\n",
    "last_test_index = int(X.shape[0] - X.shape[0] % timestep)\n",
    "print(train_cut_index)\n",
    "print(val_cut_index)\n",
    "print(last_test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2230, 93)\n",
      "(120, 93)\n",
      "(130, 93)\n",
      "(2230,)\n",
      "(120,)\n",
      "(130,)\n"
     ]
    }
   ],
   "source": [
    "#Spit Training, Validation and Test\n",
    "X_train = np.array(X.iloc[0:train_cut_index,:])\n",
    "X_val = np.array(X.iloc[train_cut_index:val_cut_index,:])\n",
    "X_test = np.array(X.iloc[val_cut_index:last_test_index,:])\n",
    "\n",
    "y_train = y[0:(train_cut_index)]\n",
    "y_val = y[train_cut_index:val_cut_index]\n",
    "y_test = y[val_cut_index:last_test_index]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preproc the y_train and y_test using to_categorical function\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_val = to_categorical(y_val, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reshape data for inputting into LSTM\n",
    "X_train = np.reshape(X_train, (int(X_train.shape[0]/timestep), timestep, X_train.shape[1]))\n",
    "X_val = np.reshape(X_val, (int(X_val.shape[0]/timestep), timestep, X_val.shape[1]))\n",
    "X_test = np.reshape(X_test, (int(X_test.shape[0]/timestep), timestep, X_test.shape[1]))\n",
    "\n",
    "y_train = np.reshape(y_train, (int(y_train.shape[0]/timestep), timestep, 2))\n",
    "y_val = np.reshape(y_val, (int(y_val.shape[0]/timestep), timestep, 2))\n",
    "y_test = np.reshape(y_test, (int(y_test.shape[0]/timestep), timestep, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(446, 5, 93)\n",
      "(24, 5, 93)\n",
      "(26, 5, 93)\n",
      "(446, 5, 2)\n",
      "(24, 5, 2)\n",
      "(26, 5, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Fit model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Library\n",
    "from keras.layers import Activation, Dense, LSTM, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define Parameters\n",
    "learning_rate = 0.001\n",
    "epochs= 100\n",
    "loss = 'categorical_crossentropy'\n",
    "dropout=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Optimizer\n",
    "optimizer = optimizers.Adam(lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (1, 5, 128)               113664    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (1, 5, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, 5, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, 5, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (1, 5, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 169,378\n",
      "Trainable params: 169,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model Architect\n",
    "#-----------------\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#Stack LSTM Cells\n",
    "model.add(LSTM(100, input_shape=X_train.shape[1:], batch_size=1, return_sequences=True, stateful=True))\n",
    "\n",
    "model.add(LSTM(50, batch_size=1, return_sequences=True, stateful=True))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compile\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 446 samples, validate on 24 samples\n",
      "Epoch 1/100\n",
      "446/446 [==============================] - 8s - loss: 0.6962 - acc: 0.5135 - val_loss: 0.6931 - val_acc: 0.5083\n",
      "Epoch 2/100\n",
      "446/446 [==============================] - 5s - loss: 0.6942 - acc: 0.5202 - val_loss: 0.6937 - val_acc: 0.5083\n",
      "Epoch 3/100\n",
      "446/446 [==============================] - 5s - loss: 0.6930 - acc: 0.5112 - val_loss: 0.6938 - val_acc: 0.5083\n",
      "Epoch 4/100\n",
      "446/446 [==============================] - 6s - loss: 0.6928 - acc: 0.5220 - val_loss: 0.6940 - val_acc: 0.5083\n",
      "Epoch 5/100\n",
      "446/446 [==============================] - 5s - loss: 0.6926 - acc: 0.5220 - val_loss: 0.6938 - val_acc: 0.5083\n",
      "Epoch 6/100\n",
      "446/446 [==============================] - 5s - loss: 0.6932 - acc: 0.5184 - val_loss: 0.6939 - val_acc: 0.5083\n",
      "Epoch 7/100\n",
      "446/446 [==============================] - 6s - loss: 0.6926 - acc: 0.5220 - val_loss: 0.6938 - val_acc: 0.5083\n",
      "Epoch 8/100\n",
      "446/446 [==============================] - 5s - loss: 0.6925 - acc: 0.5220 - val_loss: 0.6937 - val_acc: 0.5083\n",
      "Epoch 9/100\n",
      "446/446 [==============================] - 6s - loss: 0.6927 - acc: 0.5220 - val_loss: 0.6937 - val_acc: 0.5083\n",
      "Epoch 10/100\n",
      "446/446 [==============================] - 5s - loss: 0.6925 - acc: 0.5220 - val_loss: 0.6937 - val_acc: 0.5083\n",
      "Epoch 11/100\n",
      "446/446 [==============================] - 5s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6937 - val_acc: 0.5083\n",
      "Epoch 12/100\n",
      "446/446 [==============================] - 6s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6937 - val_acc: 0.5083\n",
      "Epoch 13/100\n",
      "446/446 [==============================] - 6s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6937 - val_acc: 0.5083\n",
      "Epoch 14/100\n",
      "446/446 [==============================] - 6s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6937 - val_acc: 0.5083\n",
      "Epoch 15/100\n",
      "446/446 [==============================] - 6s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6937 - val_acc: 0.5083\n",
      "Epoch 16/100\n",
      "446/446 [==============================] - 5s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6937 - val_acc: 0.5083\n",
      "Epoch 17/100\n",
      "446/446 [==============================] - 6s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6937 - val_acc: 0.5083\n",
      "Epoch 18/100\n",
      "446/446 [==============================] - 6s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6937 - val_acc: 0.5083\n",
      "Epoch 19/100\n",
      "446/446 [==============================] - 5s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6937 - val_acc: 0.5083\n",
      "Epoch 20/100\n",
      "446/446 [==============================] - 6s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6937 - val_acc: 0.5083\n",
      "Epoch 21/100\n",
      "446/446 [==============================] - 5s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 22/100\n",
      "446/446 [==============================] - 6s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 23/100\n",
      "446/446 [==============================] - 6s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 24/100\n",
      "446/446 [==============================] - 6s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 25/100\n",
      "446/446 [==============================] - 6s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 26/100\n",
      "446/446 [==============================] - 5s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 27/100\n",
      "446/446 [==============================] - 6s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 28/100\n",
      "446/446 [==============================] - 6s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 29/100\n",
      "446/446 [==============================] - 5s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 30/100\n",
      "446/446 [==============================] - 6s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 31/100\n",
      "446/446 [==============================] - 5s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 32/100\n",
      "446/446 [==============================] - 5s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 33/100\n",
      "446/446 [==============================] - 6s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 34/100\n",
      "446/446 [==============================] - 5s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 35/100\n",
      "446/446 [==============================] - 6s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 36/100\n",
      "446/446 [==============================] - 5s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 37/100\n",
      "446/446 [==============================] - 5s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 38/100\n",
      "446/446 [==============================] - 6s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 39/100\n",
      "446/446 [==============================] - 5s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 40/100\n",
      "446/446 [==============================] - 5s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 41/100\n",
      "446/446 [==============================] - 6s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 42/100\n",
      "446/446 [==============================] - 6s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 43/100\n",
      "446/446 [==============================] - 6s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 44/100\n",
      "446/446 [==============================] - 6s - loss: 0.6924 - acc: 0.5220 - val_loss: 0.6936 - val_acc: 0.5083\n",
      "Epoch 45/100\n",
      "167/446 [==========>...................] - ETA: 4s - loss: 0.6938 - acc: 0.5054"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-611d6cb3f805>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfitted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\bchik\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    861\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 863\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\Users\\bchik\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1430\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bchik\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1079\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1080\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bchik\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2268\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2269\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bchik\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bchik\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bchik\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\bchik\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bchik\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Fit\n",
    "fitted = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val), batch_size=1, verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 5: Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Plot Training vs Validation Curve\n",
    "plt.figure()\n",
    "plt.plot(fitted.history['loss'])\n",
    "plt.plot(fitted.history['val_loss'])\n",
    "plt.title('Training Loss & Validation Loss')\n",
    "plt.ylabel('Binary Cross Entropy Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Loss', 'Validation Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Plot Training vs Validation Accuracy\n",
    "plt.figure()\n",
    "plt.plot(fitted.history['acc'])\n",
    "plt.plot(fitted.history['val_acc'])\n",
    "plt.title('Training Accuracy & Validation Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
