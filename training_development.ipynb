{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Training fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model research script. This is a testing ground for the production functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from data_proc import *\n",
    "from model import *\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Keras module\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Establish Cloud SQL connection\n",
    "import MySQLdb\n",
    "conn = MySQLdb.connect(host=\"35.194.155.252\", user=\"root\", passwd=\"60761271\", db=\"HSI_DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check GPU\n",
    "#from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pretty plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define forward return step size\n",
    "step = 1\n",
    "upper_cutoff = 0.0005\n",
    "lower_cutoff = -0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define Training, Validation, test date\n",
    "training_start = datetime.date(2007, 8, 31)\n",
    "training_end = datetime.date(2015, 12, 30)\n",
    "validation_start = datetime.date(2015, 12, 31)\n",
    "validation_end = datetime.date(2016, 5, 31)\n",
    "test_start = datetime.date(2017, 6, 1)\n",
    "latest_close = datetime.date.today() + datetime.timedelta(days=-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2017, 12, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fetch training data\n",
    "X = pd.concat([get_data(conn, training_start, latest_close, 'GLOBAL_INDICES', '*'), \n",
    "                      get_data(conn, training_start, latest_close, 'hsi_data', '*')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fetch forward returns\n",
    "y = get_fwd_ret(conn, training_start, latest_close, step)\n",
    "y = get_labels_from_fwd_ret(y, upper_cutoff=upper_cutoff, lower_cutoff=lower_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Close connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the input and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split data into train, validation and test\n",
    "_, X_train, X_val, X_test = train_val_test_split(X, training_start, validation_start, test_start, latest_close, 'RobustScaler')\n",
    "_, y_train, y_val, y_test = train_val_test_split(y, training_start, validation_start, test_start, latest_close, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train, y_val, y_test = to_categorical(y_train, 3), to_categorical(y_val, 3), to_categorical(y_test, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit GRU Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit GRU model based on previous research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model Parameters\n",
    "#DROP_OUT = (0.26036828997037875, 0.10581996841419966)\n",
    "DROP_OUT = 0.5\n",
    "LEARN_RATE = 0.0005\n",
    "GRU_SIZE = 100\n",
    "DENSE_SIZE = 90\n",
    "N_FEATURES = X.shape[1]\n",
    "TIMESTEP = 11\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 500\n",
    "NUM_LAYERS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/500    Training Step: 0   Training loss: 1.0974   0.6289 sec/batch\n",
      "Val acc: 0.266 Val Loss: 1.1230\n",
      "Epoch: 4/500    Training Step: 50   Training loss: 0.7081   0.0777 sec/batch\n",
      "Val acc: 0.490 Val Loss: 0.7093\n",
      "Epoch: 7/500    Training Step: 100   Training loss: 0.7098   0.0831 sec/batch\n",
      "Val acc: 0.490 Val Loss: 0.7005\n",
      "Epoch: 11/500    Training Step: 150   Training loss: 0.7677   0.0791 sec/batch\n",
      "Val acc: 0.490 Val Loss: 0.7107\n",
      "Epoch: 14/500    Training Step: 200   Training loss: 0.6992   0.0786 sec/batch\n",
      "Val acc: 0.490 Val Loss: 0.6982\n",
      "Epoch: 17/500    Training Step: 250   Training loss: 0.6954   0.0793 sec/batch\n",
      "Val acc: 0.490 Val Loss: 0.6998\n",
      "Epoch: 21/500    Training Step: 300   Training loss: 0.7285   0.0986 sec/batch\n",
      "Val acc: 0.491 Val Loss: 0.6970\n",
      "Epoch: 24/500    Training Step: 350   Training loss: 0.6987   0.0776 sec/batch\n",
      "Val acc: 0.495 Val Loss: 0.6957\n",
      "Epoch: 27/500    Training Step: 400   Training loss: 0.6940   0.0781 sec/batch\n",
      "Val acc: 0.517 Val Loss: 0.6936\n",
      "Epoch: 31/500    Training Step: 450   Training loss: 0.7230   0.0774 sec/batch\n",
      "Val acc: 0.490 Val Loss: 0.7073\n",
      "Epoch: 34/500    Training Step: 500   Training loss: 0.6932   0.0766 sec/batch\n",
      "Val acc: 0.515 Val Loss: 0.6934\n",
      "Epoch: 37/500    Training Step: 550   Training loss: 0.7048   0.0776 sec/batch\n",
      "Val acc: 0.495 Val Loss: 0.7003\n",
      "Epoch: 41/500    Training Step: 600   Training loss: 0.6998   0.0776 sec/batch\n",
      "Val acc: 0.514 Val Loss: 0.6940\n",
      "Epoch: 44/500    Training Step: 650   Training loss: 0.6901   0.0790 sec/batch\n",
      "Val acc: 0.509 Val Loss: 0.6990\n"
     ]
    }
   ],
   "source": [
    "# Save every N iterations\n",
    "#save_every_n = 200\n",
    "\n",
    "model = DirectionLSTM(n_class=3, batch_size=BATCH_SIZE, num_steps=TIMESTEP,\n",
    "                gru_size=GRU_SIZE, dense_size=DENSE_SIZE, num_layers=NUM_LAYERS, \n",
    "                learning_rate=LEARN_RATE)\n",
    "\n",
    "#saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    \n",
    "    for e in range(EPOCHS):\n",
    "        # Train network\n",
    "        val_acc = []\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batch(X_train, y_train, BATCH_SIZE, TIMESTEP):\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: DROP_OUT,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            \n",
    "            if counter % 50 == 0:\n",
    "                print('Epoch: {}/{}   '.format(e+1, EPOCHS),\n",
    "                      'Training Step: {}  '.format(counter),\n",
    "                      'Training loss: {:.4f}  '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "                           \n",
    "                val_state = sess.run(model.initial_state)\n",
    "                \n",
    "                for xx, yy in get_batch(X_val, y_val, BATCH_SIZE, TIMESTEP):\n",
    "                    feed = {model.inputs:xx, \n",
    "                            model.targets:yy,\n",
    "                            model.keep_prob: 1.0,\n",
    "                            model.initial_state: val_state}\n",
    "                    \n",
    "                    batch_loss, batch_acc, val_state = sess.run([model.loss, model.acc, model.final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                    #print(batch_acc)\n",
    "                    \n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)),\n",
    "                      \"Val Loss: {:.4f}\".format(batch_loss))\n",
    "            counter +=1\n",
    "        \n",
    "            #if (counter % save_every_n == 0):\n",
    "                #saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    #saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
