{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction As Classification\n",
    "Continuing the 2800-HK price prediction from classification perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Modules and load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "np.random.seed(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import sklearn\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import Keras module\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15520730019394190121\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11308947866\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 17480548225656454194\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#Check GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pretty plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>P_L1</th>\n",
       "      <th>P_L2</th>\n",
       "      <th>P_L3</th>\n",
       "      <th>P_L4</th>\n",
       "      <th>P_L5</th>\n",
       "      <th>P_L6</th>\n",
       "      <th>P_L7</th>\n",
       "      <th>P_L8</th>\n",
       "      <th>P_L9</th>\n",
       "      <th>...</th>\n",
       "      <th>P_L11</th>\n",
       "      <th>P_L12</th>\n",
       "      <th>P_L13</th>\n",
       "      <th>P_L14</th>\n",
       "      <th>P_L15</th>\n",
       "      <th>P_L16</th>\n",
       "      <th>P_L17</th>\n",
       "      <th>P_L18</th>\n",
       "      <th>P_L19</th>\n",
       "      <th>P_L20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-08-31</th>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>23.35</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.05</td>\n",
       "      <td>21.85</td>\n",
       "      <td>...</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.85</td>\n",
       "      <td>22.15</td>\n",
       "      <td>22.25</td>\n",
       "      <td>22.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-03</th>\n",
       "      <td>24.30</td>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>23.35</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.05</td>\n",
       "      <td>...</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.85</td>\n",
       "      <td>22.15</td>\n",
       "      <td>22.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-04</th>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>23.35</td>\n",
       "      <td>22.70</td>\n",
       "      <td>...</td>\n",
       "      <td>21.85</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.85</td>\n",
       "      <td>22.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-05</th>\n",
       "      <td>24.35</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>23.35</td>\n",
       "      <td>...</td>\n",
       "      <td>22.05</td>\n",
       "      <td>21.85</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-06</th>\n",
       "      <td>24.50</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>...</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.05</td>\n",
       "      <td>21.85</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                P   P_L1   P_L2   P_L3   P_L4   P_L5   P_L6   P_L7   P_L8  \\\n",
       "Date                                                                        \n",
       "2007-08-31  24.35  23.80  23.35  23.70  23.90  23.30  23.35  22.70  22.05   \n",
       "2007-09-03  24.30  24.35  23.80  23.35  23.70  23.90  23.30  23.35  22.70   \n",
       "2007-09-04  24.30  24.30  24.35  23.80  23.35  23.70  23.90  23.30  23.35   \n",
       "2007-09-05  24.35  24.30  24.30  24.35  23.80  23.35  23.70  23.90  23.30   \n",
       "2007-09-06  24.50  24.35  24.30  24.30  24.35  23.80  23.35  23.70  23.90   \n",
       "\n",
       "             P_L9  ...    P_L11  P_L12  P_L13  P_L14  P_L15  P_L16  P_L17  \\\n",
       "Date               ...                                                      \n",
       "2007-08-31  21.85  ...    20.95  21.70  22.35  22.30  22.05  22.70  22.85   \n",
       "2007-09-03  22.05  ...    20.80  20.95  21.70  22.35  22.30  22.05  22.70   \n",
       "2007-09-04  22.70  ...    21.85  20.80  20.95  21.70  22.35  22.30  22.05   \n",
       "2007-09-05  23.35  ...    22.05  21.85  20.80  20.95  21.70  22.35  22.30   \n",
       "2007-09-06  23.30  ...    22.70  22.05  21.85  20.80  20.95  21.70  22.35   \n",
       "\n",
       "            P_L18  P_L19  P_L20  \n",
       "Date                             \n",
       "2007-08-31  22.15  22.25  22.85  \n",
       "2007-09-03  22.85  22.15  22.25  \n",
       "2007-09-04  22.70  22.85  22.15  \n",
       "2007-09-05  22.05  22.70  22.85  \n",
       "2007-09-06  22.30  22.05  22.70  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import price data\n",
    "#Load the historical prices of 2800-HK, with lags 1 to lag 20\n",
    "price_hist_data = pd.read_csv('price_only.csv', skiprows=1, parse_dates=['Date']).set_index(['Date'])\n",
    "price_hist_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save dates for later use\n",
    "dates = price_hist_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Ask</th>\n",
       "      <th>Bid</th>\n",
       "      <th>Open</th>\n",
       "      <th>20D Vol</th>\n",
       "      <th>MA5</th>\n",
       "      <th>MA15</th>\n",
       "      <th>MA12</th>\n",
       "      <th>...</th>\n",
       "      <th>DY_LTM</th>\n",
       "      <th>DY_NTM</th>\n",
       "      <th>ADV_VOL</th>\n",
       "      <th>PAYOUT</th>\n",
       "      <th>ANALYST_SENTIMENT</th>\n",
       "      <th>EPS_GRW_FY1</th>\n",
       "      <th>EPS_GRW_FY2</th>\n",
       "      <th>PE_NTM</th>\n",
       "      <th>PE_LTM</th>\n",
       "      <th>C2D_LTM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-08-31</th>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>23.95</td>\n",
       "      <td>2.367823</td>\n",
       "      <td>23.82</td>\n",
       "      <td>22.841667</td>\n",
       "      <td>22.696667</td>\n",
       "      <td>...</td>\n",
       "      <td>2.782797</td>\n",
       "      <td>2.880128</td>\n",
       "      <td>99.419898</td>\n",
       "      <td>48.051962</td>\n",
       "      <td>1.909071</td>\n",
       "      <td>30.724490</td>\n",
       "      <td>2.717280</td>\n",
       "      <td>16.695058</td>\n",
       "      <td>16.975805</td>\n",
       "      <td>58.050474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-03</th>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>2.266743</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.120832</td>\n",
       "      <td>22.830000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.784563</td>\n",
       "      <td>2.892308</td>\n",
       "      <td>11.570023</td>\n",
       "      <td>48.305678</td>\n",
       "      <td>1.835359</td>\n",
       "      <td>31.005439</td>\n",
       "      <td>1.876752</td>\n",
       "      <td>16.709085</td>\n",
       "      <td>16.925571</td>\n",
       "      <td>58.154835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-04</th>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>2.259649</td>\n",
       "      <td>24.02</td>\n",
       "      <td>23.412500</td>\n",
       "      <td>22.960000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.784339</td>\n",
       "      <td>2.894377</td>\n",
       "      <td>69.555725</td>\n",
       "      <td>48.307102</td>\n",
       "      <td>1.725886</td>\n",
       "      <td>31.040968</td>\n",
       "      <td>1.891823</td>\n",
       "      <td>16.697662</td>\n",
       "      <td>16.919413</td>\n",
       "      <td>58.177640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-05</th>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.55</td>\n",
       "      <td>2.172140</td>\n",
       "      <td>24.22</td>\n",
       "      <td>23.620832</td>\n",
       "      <td>23.136667</td>\n",
       "      <td>...</td>\n",
       "      <td>2.767348</td>\n",
       "      <td>2.867314</td>\n",
       "      <td>24.265623</td>\n",
       "      <td>48.267351</td>\n",
       "      <td>1.741908</td>\n",
       "      <td>30.716929</td>\n",
       "      <td>1.878484</td>\n",
       "      <td>16.841225</td>\n",
       "      <td>17.017692</td>\n",
       "      <td>58.195446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-06</th>\n",
       "      <td>24.50</td>\n",
       "      <td>24.50</td>\n",
       "      <td>24.50</td>\n",
       "      <td>24.50</td>\n",
       "      <td>24.50</td>\n",
       "      <td>24.30</td>\n",
       "      <td>2.160638</td>\n",
       "      <td>24.36</td>\n",
       "      <td>23.825000</td>\n",
       "      <td>23.373333</td>\n",
       "      <td>...</td>\n",
       "      <td>2.775339</td>\n",
       "      <td>2.878880</td>\n",
       "      <td>88.845002</td>\n",
       "      <td>48.288397</td>\n",
       "      <td>1.696151</td>\n",
       "      <td>31.336520</td>\n",
       "      <td>1.861943</td>\n",
       "      <td>16.780660</td>\n",
       "      <td>16.975485</td>\n",
       "      <td>58.224743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Close   High    Low    Ask    Bid   Open   20D Vol    MA5  \\\n",
       "Date                                                                    \n",
       "2007-08-31  24.35  24.35  24.35  24.35  24.35  23.95  2.367823  23.82   \n",
       "2007-09-03  24.30  24.30  24.30  24.30  24.30  24.30  2.266743  23.90   \n",
       "2007-09-04  24.30  24.30  24.30  24.30  24.30  24.30  2.259649  24.02   \n",
       "2007-09-05  24.35  24.35  24.35  24.35  24.35  24.55  2.172140  24.22   \n",
       "2007-09-06  24.50  24.50  24.50  24.50  24.50  24.30  2.160638  24.36   \n",
       "\n",
       "                 MA15       MA12    ...        DY_LTM    DY_NTM    ADV_VOL  \\\n",
       "Date                                ...                                      \n",
       "2007-08-31  22.841667  22.696667    ...      2.782797  2.880128  99.419898   \n",
       "2007-09-03  23.120832  22.830000    ...      2.784563  2.892308  11.570023   \n",
       "2007-09-04  23.412500  22.960000    ...      2.784339  2.894377  69.555725   \n",
       "2007-09-05  23.620832  23.136667    ...      2.767348  2.867314  24.265623   \n",
       "2007-09-06  23.825000  23.373333    ...      2.775339  2.878880  88.845002   \n",
       "\n",
       "               PAYOUT  ANALYST_SENTIMENT  EPS_GRW_FY1  EPS_GRW_FY2     PE_NTM  \\\n",
       "Date                                                                            \n",
       "2007-08-31  48.051962           1.909071    30.724490     2.717280  16.695058   \n",
       "2007-09-03  48.305678           1.835359    31.005439     1.876752  16.709085   \n",
       "2007-09-04  48.307102           1.725886    31.040968     1.891823  16.697662   \n",
       "2007-09-05  48.267351           1.741908    30.716929     1.878484  16.841225   \n",
       "2007-09-06  48.288397           1.696151    31.336520     1.861943  16.780660   \n",
       "\n",
       "               PE_LTM    C2D_LTM  \n",
       "Date                              \n",
       "2007-08-31  16.975805  58.050474  \n",
       "2007-09-03  16.925571  58.154835  \n",
       "2007-09-04  16.919413  58.177640  \n",
       "2007-09-05  17.017692  58.195446  \n",
       "2007-09-06  16.975485  58.224743  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Fundamentals Data\n",
    "fund_data = pd.read_csv('new_index_data.csv', skiprows=1, parse_dates=['Date']).set_index(['Date'])\n",
    "fund_data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hang Seng Index</th>\n",
       "      <th>SSE Composite Index</th>\n",
       "      <th>ASX All Ordinaries</th>\n",
       "      <th>India S&amp;P BSE SENSEX</th>\n",
       "      <th>TOPIX</th>\n",
       "      <th>KOSPI Composite Index</th>\n",
       "      <th>Taiwan TAIEX</th>\n",
       "      <th>FTSE Bursa Malaysia KLCI</th>\n",
       "      <th>FTSE Straits Times Index</th>\n",
       "      <th>Philippines PSE PSEi</th>\n",
       "      <th>...</th>\n",
       "      <th>Turkey BIST 100</th>\n",
       "      <th>S&amp;P 500</th>\n",
       "      <th>DJ Industrial Average</th>\n",
       "      <th>Colombia IGBC</th>\n",
       "      <th>Canada S&amp;P/TSX Composite</th>\n",
       "      <th>Brazil Bovespa Index</th>\n",
       "      <th>Mexico IPC</th>\n",
       "      <th>Israel TA-125</th>\n",
       "      <th>Saudi Arabia All Share (TASI)</th>\n",
       "      <th>FTSE JSE All Share</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-08-31</th>\n",
       "      <td>23984.14</td>\n",
       "      <td>5218.825</td>\n",
       "      <td>6248.3</td>\n",
       "      <td>15318.60</td>\n",
       "      <td>1608.25</td>\n",
       "      <td>1873.24</td>\n",
       "      <td>8982.16</td>\n",
       "      <td>1273.93</td>\n",
       "      <td>3328.43</td>\n",
       "      <td>3365.29</td>\n",
       "      <td>...</td>\n",
       "      <td>50198.60</td>\n",
       "      <td>1473.99</td>\n",
       "      <td>13357.74</td>\n",
       "      <td>10728.74</td>\n",
       "      <td>13660.48</td>\n",
       "      <td>54637.24</td>\n",
       "      <td>30347.86</td>\n",
       "      <td>1034.67</td>\n",
       "      <td>8226.97</td>\n",
       "      <td>28660.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-03</th>\n",
       "      <td>23904.09</td>\n",
       "      <td>5321.055</td>\n",
       "      <td>6272.5</td>\n",
       "      <td>15422.05</td>\n",
       "      <td>1605.44</td>\n",
       "      <td>1881.81</td>\n",
       "      <td>8979.96</td>\n",
       "      <td>1284.14</td>\n",
       "      <td>3321.36</td>\n",
       "      <td>3369.14</td>\n",
       "      <td>...</td>\n",
       "      <td>49936.94</td>\n",
       "      <td>1473.99</td>\n",
       "      <td>13357.74</td>\n",
       "      <td>10750.79</td>\n",
       "      <td>13660.48</td>\n",
       "      <td>54832.51</td>\n",
       "      <td>30797.60</td>\n",
       "      <td>1047.33</td>\n",
       "      <td>8017.54</td>\n",
       "      <td>28887.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-04</th>\n",
       "      <td>23886.07</td>\n",
       "      <td>5294.045</td>\n",
       "      <td>6297.1</td>\n",
       "      <td>15465.40</td>\n",
       "      <td>1596.74</td>\n",
       "      <td>1874.74</td>\n",
       "      <td>8922.98</td>\n",
       "      <td>1283.75</td>\n",
       "      <td>3308.81</td>\n",
       "      <td>3312.30</td>\n",
       "      <td>...</td>\n",
       "      <td>50032.59</td>\n",
       "      <td>1489.42</td>\n",
       "      <td>13448.86</td>\n",
       "      <td>10880.85</td>\n",
       "      <td>13755.23</td>\n",
       "      <td>55250.47</td>\n",
       "      <td>30932.71</td>\n",
       "      <td>1054.69</td>\n",
       "      <td>7878.70</td>\n",
       "      <td>29051.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-05</th>\n",
       "      <td>24069.17</td>\n",
       "      <td>5310.716</td>\n",
       "      <td>6274.3</td>\n",
       "      <td>15446.15</td>\n",
       "      <td>1569.47</td>\n",
       "      <td>1865.59</td>\n",
       "      <td>8913.85</td>\n",
       "      <td>1297.93</td>\n",
       "      <td>3375.02</td>\n",
       "      <td>3342.35</td>\n",
       "      <td>...</td>\n",
       "      <td>49421.38</td>\n",
       "      <td>1472.29</td>\n",
       "      <td>13305.47</td>\n",
       "      <td>10819.91</td>\n",
       "      <td>13683.28</td>\n",
       "      <td>54407.83</td>\n",
       "      <td>30809.55</td>\n",
       "      <td>1048.70</td>\n",
       "      <td>7853.66</td>\n",
       "      <td>28696.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-06</th>\n",
       "      <td>24050.40</td>\n",
       "      <td>5393.660</td>\n",
       "      <td>6265.3</td>\n",
       "      <td>15616.31</td>\n",
       "      <td>1568.52</td>\n",
       "      <td>1888.81</td>\n",
       "      <td>9017.08</td>\n",
       "      <td>1298.85</td>\n",
       "      <td>3399.49</td>\n",
       "      <td>3326.53</td>\n",
       "      <td>...</td>\n",
       "      <td>49601.39</td>\n",
       "      <td>1478.55</td>\n",
       "      <td>13363.35</td>\n",
       "      <td>10844.40</td>\n",
       "      <td>13795.69</td>\n",
       "      <td>54569.00</td>\n",
       "      <td>30816.95</td>\n",
       "      <td>1033.23</td>\n",
       "      <td>7853.66</td>\n",
       "      <td>28850.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Hang Seng Index  SSE Composite Index  ASX All Ordinaries  \\\n",
       "Date                                                                   \n",
       "2007-08-31         23984.14             5218.825              6248.3   \n",
       "2007-09-03         23904.09             5321.055              6272.5   \n",
       "2007-09-04         23886.07             5294.045              6297.1   \n",
       "2007-09-05         24069.17             5310.716              6274.3   \n",
       "2007-09-06         24050.40             5393.660              6265.3   \n",
       "\n",
       "            India S&P BSE SENSEX    TOPIX  KOSPI Composite Index  \\\n",
       "Date                                                               \n",
       "2007-08-31              15318.60  1608.25                1873.24   \n",
       "2007-09-03              15422.05  1605.44                1881.81   \n",
       "2007-09-04              15465.40  1596.74                1874.74   \n",
       "2007-09-05              15446.15  1569.47                1865.59   \n",
       "2007-09-06              15616.31  1568.52                1888.81   \n",
       "\n",
       "            Taiwan TAIEX  FTSE Bursa Malaysia KLCI  FTSE Straits Times Index  \\\n",
       "Date                                                                           \n",
       "2007-08-31       8982.16                   1273.93                   3328.43   \n",
       "2007-09-03       8979.96                   1284.14                   3321.36   \n",
       "2007-09-04       8922.98                   1283.75                   3308.81   \n",
       "2007-09-05       8913.85                   1297.93                   3375.02   \n",
       "2007-09-06       9017.08                   1298.85                   3399.49   \n",
       "\n",
       "            Philippines PSE PSEi         ...          Turkey BIST 100  \\\n",
       "Date                                     ...                            \n",
       "2007-08-31               3365.29         ...                 50198.60   \n",
       "2007-09-03               3369.14         ...                 49936.94   \n",
       "2007-09-04               3312.30         ...                 50032.59   \n",
       "2007-09-05               3342.35         ...                 49421.38   \n",
       "2007-09-06               3326.53         ...                 49601.39   \n",
       "\n",
       "            S&P 500  DJ Industrial Average  Colombia IGBC  \\\n",
       "Date                                                        \n",
       "2007-08-31  1473.99               13357.74       10728.74   \n",
       "2007-09-03  1473.99               13357.74       10750.79   \n",
       "2007-09-04  1489.42               13448.86       10880.85   \n",
       "2007-09-05  1472.29               13305.47       10819.91   \n",
       "2007-09-06  1478.55               13363.35       10844.40   \n",
       "\n",
       "            Canada S&P/TSX Composite  Brazil Bovespa Index  Mexico IPC  \\\n",
       "Date                                                                     \n",
       "2007-08-31                  13660.48              54637.24    30347.86   \n",
       "2007-09-03                  13660.48              54832.51    30797.60   \n",
       "2007-09-04                  13755.23              55250.47    30932.71   \n",
       "2007-09-05                  13683.28              54407.83    30809.55   \n",
       "2007-09-06                  13795.69              54569.00    30816.95   \n",
       "\n",
       "            Israel TA-125  Saudi Arabia All Share (TASI)  FTSE JSE All Share  \n",
       "Date                                                                          \n",
       "2007-08-31        1034.67                        8226.97            28660.35  \n",
       "2007-09-03        1047.33                        8017.54            28887.48  \n",
       "2007-09-04        1054.69                        7878.70            29051.96  \n",
       "2007-09-05        1048.70                        7853.66            28696.67  \n",
       "2007-09-06        1033.23                        7853.66            28850.19  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Global index data\n",
    "idx_data = pd.read_csv('indices.csv', skiprows=1, parse_dates=['Date']).set_index(['Date'])\n",
    "idx_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pre-process Raw Data\n",
    "1. Generate labels\n",
    "2. Apply lags to global index data\n",
    "3. Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate Labels from Price History Data\n",
    "#Generate UP/DOWN labels from log change\n",
    "cutoff_perc = 0.0005 #0.05% return as cuttoff to define UP label\n",
    "lag = 1 #forward returns\n",
    "\n",
    "labels = np.zeros([price_hist_data.shape[0]])\n",
    "\n",
    "#Caluclate log-returns\n",
    "ret = np.log(price_hist_data['P'].shift(-lag)/price_hist_data['P'])\n",
    "labels = [2 if r > cutoff_perc else 0 if r < -cutoff_perc else 1 for r in ret]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2017-10-16    0.001688\n",
       "2017-10-17    0.000000\n",
       "2017-10-18   -0.017007\n",
       "2017-10-19    0.010239\n",
       "2017-10-20   -0.006814\n",
       "2017-10-23   -0.003425\n",
       "2017-10-24    0.003425\n",
       "2017-10-25   -0.003425\n",
       "2017-10-26    0.006838\n",
       "2017-10-27         NaN\n",
       "Name: P, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Applying lags to index data\n",
    "#Seperate the indices into 2 classes - lag or no_lag\n",
    "no_lag = [0, 1, 2, 4, 5, 6, 9, 10]\n",
    "lag = [i for i in range(0,idx_data.shape[1]) if i not in no_lag]\n",
    "\n",
    "#Processing the dataset by applying appropriate lags\n",
    "lagged_data = idx_data.iloc[:,lag].shift(1)\n",
    "idx_data = pd.concat([idx_data.iloc[:,no_lag], lagged_data], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove first row and last row\n",
    "idx_data = idx_data.iloc[1:-1, :]\n",
    "price_hist_data = price_hist_data.iloc[1:-1, :]\n",
    "fund_data = fund_data.iloc[1:-1, :]\n",
    "labels=labels[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of idx_data:  (2502, 42)\n",
      "Shape of price_hist_data:  (2502, 21)\n",
      "Shape of fund_data:  (2502, 31)\n",
      "Shape of labels:  2502\n"
     ]
    }
   ],
   "source": [
    "#Check Dimensions to make sure everythings right before continuing..\n",
    "print(\"Shape of idx_data: \", idx_data.shape)\n",
    "print(\"Shape of price_hist_data: \", price_hist_data.shape)\n",
    "print(\"Shape of fund_data: \", fund_data.shape)\n",
    "print(\"Shape of labels: \", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training data\n",
    "X = np.array(pd.concat([price_hist_data, idx_data, fund_data], axis=1))\n",
    "y = to_categorical(labels, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking whether there are NAs\n",
    "[np.sum(np.isnan(X), axis=0) > 0] == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Split training, validation and test set\n",
    "\n",
    "In this stage we have a look-ahead bias free set of data (X) and the labels y. Next, we will need to:\n",
    "- Normalize the input data. To avoid look ahead bias, we will z-score the features, using ONLY the training set.\n",
    "- Next, generate input data into LSTM network. We will need an overlapping sequence at 1-day window as input samples. Specifically suppose the *timestep* is 240, we will have a list of array consists of *number of rows of X* - 240 entries, each element has dimentions (240, num_of_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to split raw data into training, validation and test set, returns a numpy array.\n",
    "def split_data(input_data=[], train_size=0.8, val_size=0.2, test_size=0):\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    #PARAM: input_data: numpy nd array\n",
    "    #PARAM: training_size: size of training set in decimal\n",
    "    #PARAM: val_size: size of validation set in decimal\n",
    "    #PARAM: test_size: size of test set in decimal\n",
    "    #OUTPUT: tuple (train_set, validation_set, test_set)\n",
    "    #------------------------------------------------\n",
    "    \n",
    "    #First check whether traing_size + val_size + test_size = 1 and each of the entries are positive\n",
    "    assert(train_size + val_size + test_size==1), \"Sum of training, validation and test size needs to be 1!\"\n",
    "    assert(train_size * val_size * test_size > 0), \"Sizes have to be positive!\"\n",
    "    \n",
    "    #Check input_data type is numpy array, after casting\n",
    "    if type(input_data) != 'numpy.ndarray':\n",
    "        input_data = np.array(input_data) \n",
    "    \n",
    "    assert(isinstance(input_data, np.ndarray)), \"Input has to be a numpy array!\"\n",
    "    \n",
    "    \n",
    "    #Calculate cut-off points\n",
    "    train_cut_index = int(train_size * input_data.shape[0])\n",
    "    val_cut_index = train_cut_index + int(val_size * input_data.shape[0])\n",
    "    \n",
    "    #Split the data\n",
    "    if len(input_data.shape) == 1:\n",
    "        train, val, test = input_data[:train_cut_index], input_data[train_cut_index:val_cut_index], input_data[val_cut_index:]\n",
    "    else:\n",
    "        train, val, test = input_data[:train_cut_index,:], input_data[train_cut_index:val_cut_index, :], input_data[val_cut_index:, :]\n",
    "    \n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1751, 94)\n",
      "(375, 94)\n",
      "(376, 94)\n",
      "(1751, 3)\n",
      "(375, 3)\n",
      "(376, 3)\n"
     ]
    }
   ],
   "source": [
    "#------------\n",
    "#TEST OUTPUT\n",
    "#------------\n",
    "#Split data\n",
    "X_train, X_val, X_test = split_data(X, train_size=0.7, val_size=0.15, test_size=0.15)\n",
    "y_train, y_val, y_test = split_data(y, train_size=0.7, val_size=0.15, test_size=0.15)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to re-structure the data to get batches. Re-shape the data to have overlapping training set for time-series learning.\n",
    "def get_inputs(input_data, labels, batch_size, timesteps):\n",
    "    \n",
    "    #First get the total number of samples generated\n",
    "    n_seq = input_data.shape[0] - timesteps + 1\n",
    "    \n",
    "    #features, classes\n",
    "    n_dim = input_data.shape[1]\n",
    "    n_class = labels.shape[1]\n",
    "    \n",
    "    #Calculate the number of batches possible\n",
    "    n_samples = n_seq * timesteps\n",
    "    n_batches = n_samples // (batch_size * timesteps)\n",
    "    \n",
    "    #Assert n_batches > 0\n",
    "    assert(n_batches > 0), 'Not enough data to form 1 batch!'\n",
    "    \n",
    "    #Generate labels to have (batch_size, 1)\n",
    "    targets=labels[:n_batches * batch_size,:]\n",
    "    \n",
    "    \n",
    "    #Generate training data with dim (batch_size, timesteps, n_features)\n",
    "    output = []\n",
    "    \n",
    "        \n",
    "    for jj in range(0, n_batches):\n",
    "    #Generate the sequences\n",
    "        \n",
    "        #if jj == n_batches:\n",
    "            \n",
    "            #output.append([input_data[jj*batch_size:, :]])\n",
    "            #targets.append([labels[jj*batch_size:, :]])            \n",
    "            #yield np.vstack(output), np.vstack(targets  )\n",
    "            \n",
    "        #else:            \n",
    "        for ii in range(jj * batch_size, (jj + 1) * batch_size):                    \n",
    "            #Getting the overlapping samples\n",
    "            output.append([scale(input_data[ii:ii+timesteps, :])])\n",
    "            #targets.append([labels[ii:ii+timesteps, :]])\n",
    "\n",
    "    return np.vstack(output), targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 4, 5)\n",
      "(6, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryan_yychik/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#------------\n",
    "#TEST OUTPUT\n",
    "#------------\n",
    "#get inputs\n",
    "t = np.reshape(np.arange(1,51), (10,5))\n",
    "s = np.reshape(np.arange(1,21), (10,2))\n",
    "\n",
    "#for (x, y) in get_inputs(t,s, 3, 4):\n",
    "#    print('training size: ', x.shape)\n",
    "#    print('training: \\n', x)\n",
    "#    print('label size: ', y.shape)\n",
    "#    print('labels: \\n', y)\n",
    "\n",
    "test_train, test_label = get_inputs(t,s,3,4)\n",
    "    \n",
    "\n",
    "#print(test_train[0:1])\n",
    "#print(test_label[0:1])\n",
    "#print(t)\n",
    "print(test_train.shape)\n",
    "print(test_label.shape)\n",
    "#print(test_train)\n",
    "#test_train.reshape((-1, 4, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define Parameters\n",
    "learning_rate = 1e-5\n",
    "epochs= 500\n",
    "loss = 'categorical_crossentropy'\n",
    "batch_size = 256\n",
    "timesteps = 14\n",
    "n_dim = X.shape[1]\n",
    "n_classes = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Optimizer - using Adam Optimizer\n",
    "optimizer = optimizers.Adam(lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate Inputs\n",
    "X_train_1, y_train_1 = get_inputs(X_train, y_train, batch_size, timesteps)\n",
    "X_val_1, y_val_1 = get_inputs(X_val, y_val, batch_size, timesteps)\n",
    "X_test_1, y_test_1 = get_inputs(X_test, y_test, batch_size, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define LSTM Network object\n",
    "def build_network(n_hidden_layer, n_classes, dropout, input_shape, batch_size, return_sequences=True, stateful=True):\n",
    "    \n",
    "    #Define network architect\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_hidden_layer[0], \n",
    "                   input_shape=input_shape,\n",
    "                   kernel_initializer='truncated_normal',\n",
    "                   recurrent_initializer='truncated_normal',\n",
    "                   bias_initializer='RandomUniform',\n",
    "                   batch_size=batch_size, \n",
    "                   return_sequences=return_sequences, \n",
    "                   stateful=stateful))\n",
    "    \n",
    "    model.add(Dropout(dropout[0]))\n",
    "    \n",
    "    model.add(LSTM(n_hidden_layer[1], \n",
    "                   return_sequences=False,\n",
    "                   kernel_initializer='truncated_normal',\n",
    "                   recurrent_initializer='truncated_normal',\n",
    "                   bias_initializer='RandomUniform',\n",
    "                   stateful=stateful))\n",
    "    \n",
    "    model.add(Dropout(dropout[1]))\n",
    "    \n",
    "    model.add(Dense(n_hidden_layer[2], activation='relu', kernel_initializer='truncated_normal'))\n",
    "    \n",
    "    model.add(Dense(n_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lstm1 = build_network(best_n_hidden, n_classes, [0,0], input_shape=(timesteps, n_dim), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compile\n",
    "#lstm1.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fitted = lstm1.fit(X_train_1, y_train_1, batch_size=batch_size, epochs=1000, verbose=1, validation_data=(X_val_1, y_val_1), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot_metric(fitted, 'loss')\n",
    "#plot_metric(fitted, 'acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 5: Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Plot Training vs Validation Curve\n",
    "def plot_metric(fitted, metric):\n",
    "    plt.figure()\n",
    "    plt.plot(fitted.history[metric])\n",
    "    plt.plot(fitted.history['val_' + metric])\n",
    "    plt.title('Training ' + metric + ' & Validation ' + metric)\n",
    "    plt.ylabel(metric)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training ' + metric, 'Validation ' + metric])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 6: Parameter Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search the following parameters:\n",
    "- Learning Rate & decay rate\n",
    "- Number of Hidden Layers\n",
    "- Dropout Rate\n",
    "- Timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "best_drop_out             -> (0.16431643902664886, 0.10116406923482804)\n",
      "best_lr                   -> 6.2257133109410074e-05\n",
      "best_n_hidden             -> (104, 50, 40)\n",
      "best_timestep             -> 11\n",
      "do                        -> [(0.30654380648074059, 0.15518723342569127), (0.20\n",
      "do_loss                   -> [0.96192556619644165, 0.89160728454589844, 0.98137\n",
      "hd                        -> [(167, 129, 46), (69, 86, 107), (53, 182, 132), (1\n",
      "hd_loss                   -> [0.86914741992950439, 0.84087979793548584, 8.56273\n",
      "learnrate                 -> [6.7700028967629204e-05, 6.8839231364996276e-05, 1\n",
      "lr_loss                   -> [0.81691169738769531, 0.79937803745269775, 0.82949\n",
      "t_loss                    -> [0.95131367444992065, 0.93839895725250244, 0.90483\n",
      "tt                        -> array([ 37,  65,  96,  46, 101, 103,  82, 107,  77\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define number of random trials\n",
    "n_trials = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initiate empty list to store best validation loss for a particular number of hidden layers\n",
    "hd = []\n",
    "hd_loss = []\n",
    "steps = 1\n",
    "\n",
    "#Loop through the learning rate and train network\n",
    "for h in zip(np.random.randint(50,250, n_trials), np.random.randint(50,250, n_trials), np.random.randint(10,200, n_trials)): \n",
    "    \n",
    "    hd_network = build_network(h, n_classes, [0,0], input_shape=(timesteps, n_dim), batch_size=batch_size)\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optimizers.Adam(lr=0.1, decay=1e-5)\n",
    "    \n",
    "    #Compile\n",
    "    hd_network.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    #Train\n",
    "    fitted_hidden = hd_network.fit(X_train_1, y_train_1, batch_size=batch_size, epochs=20, verbose=0, validation_data=(X_val_1, y_val_1), shuffle=True)\n",
    "    \n",
    "    #Min loss\n",
    "    val_loss = np.min(fitted_hidden.history['val_loss'])\n",
    "    \n",
    "    #Display\n",
    "    print('Steps: %d,' % steps, 'Number of layers: (%d, %d, %d), ' % (h[0], h[1], h[2]), 'Best Validation Loss: %.4f' %  val_loss)\n",
    "    \n",
    "    #Save\n",
    "    hd.append(h)\n",
    "    hd_loss.append(val_loss)\n",
    "    \n",
    "    #Update\n",
    "    steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Best number of hidden layer\n",
    "best_hidden_loss = np.min(hd_loss)\n",
    "best_n_hidden = hd[np.argmin(hd_loss)]\n",
    "print('Best Number of Hidden Layer: ', best_n_hidden, \" Validation Loss: %.4f \" % best_hidden_loss)\n",
    "%store best_n_hidden\n",
    "%store hd_loss\n",
    "%store hd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the figure, add a 3d axis, set the viewing angle\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x = np.array(hd)[:,0]\n",
    "y = np.array(hd)[:,1]\n",
    "z = np.array(hd)[:,2]\n",
    "\n",
    "p = ax.scatter(x, y, z, c=hd_loss, s=200)\n",
    "ax.set_xlabel('1st LSTM Layer')\n",
    "ax.set_ylabel('2nd LSTM Layer')\n",
    "ax.set_zlabel('Dense Layer')\n",
    "plt.colorbar(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define early Stop call back\n",
    "earlyStopping = EarlyStopping(patience=10, min_delta=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initiate empty list to store best validation loss for a particular learning rate\n",
    "learnrate = []\n",
    "lr_loss = []\n",
    "steps = 1\n",
    "\n",
    "for l in np.random.uniform(1e-4, 1e-6, n_trials):\n",
    "\n",
    "    lr_network = build_network(best_n_hidden, n_classes, [0,0], input_shape=(timesteps, n_dim), batch_size=batch_size)\n",
    "\n",
    "    #Optimizer\n",
    "    optimizer = optimizers.Adam(lr=l)\n",
    "\n",
    "    #Compile\n",
    "    lr_network.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    #Train\n",
    "    fitted_lr = lr_network.fit(X_train_1, y_train_1, \n",
    "                               batch_size=batch_size, \n",
    "                               epochs=300, verbose=0, \n",
    "                               validation_data=(X_val_1, y_val_1), \n",
    "                               shuffle=True, \n",
    "                               callbacks=[earlyStopping])\n",
    "    \n",
    "    #Min loss\n",
    "    val_loss = np.min(fitted_lr.history['val_loss'])\n",
    "    \n",
    "    #Display\n",
    "    print('Steps: %d,' % steps, 'Learning Rate: %.6f, ' % l, 'Best Validation Loss: %.4f' %  val_loss)\n",
    "    \n",
    "    #Save\n",
    "    learnrate.append(l)\n",
    "    lr_loss.append(val_loss)\n",
    "    \n",
    "    #Update\n",
    "    steps += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Best leanring rate\n",
    "best_lr_loss = np.min(lr_loss)\n",
    "best_lr = learnrate[np.argmin(lr_loss)]\n",
    "print('Best Learning Rate: %f' % best_lr, \" Validation Loss: %.4f \" % best_lr_loss)\n",
    "%store best_lr\n",
    "%store lr_loss\n",
    "%store learnrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_loss = [l for _, l in sorted(zip(learnrate, lr_loss))]\n",
    "sorted_lr = sorted(learnrate)\n",
    "\n",
    "plt.plot(sorted_lr, sorted_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initiate empty list to store best validation loss for a particular learning rate\n",
    "do = []\n",
    "do_loss = []\n",
    "steps = 1\n",
    "\n",
    "#Loop through the learning rate and train network\n",
    "for d in zip(np.random.uniform(0.1, 0.7, n_trials), np.random.uniform(0.1, 0.7, n_trials)):\n",
    "\n",
    "    do_network = build_network(best_n_hidden, n_classes, d, input_shape=(timesteps, n_dim), batch_size=batch_size)\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optimizers.Adam(lr=best_lr)\n",
    "    \n",
    "    #Compile\n",
    "    do_network.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    #Train\n",
    "    fitted_dropout = do_network.fit(X_train_1, y_train_1, batch_size=batch_size, epochs=20, verbose=0, validation_data=(X_val_1, y_val_1), shuffle=True)\n",
    "    \n",
    "    #Min loss\n",
    "    val_loss = np.min(fitted_dropout.history['val_loss'])\n",
    "    \n",
    "    #Display\n",
    "    print('Steps: %d,' % steps, 'Dropout: (%.4f, %.4f), ' % (d[0], d[1]), 'Best Validation Loss: %.4f' %  val_loss)\n",
    "    \n",
    "    #Save\n",
    "    do.append(d)\n",
    "    do_loss.append(val_loss)\n",
    "    \n",
    "    #Update\n",
    "    steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_do_loss = np.min(do_loss)\n",
    "best_drop_out = do[np.argmin(do_loss)]\n",
    "print('Best Drop out probability: (%.2f, %.2f) ' % (best_drop_out[0], best_drop_out[1]), 'Validation Loss: %.4f' % best_do_loss)\n",
    "%store best_drop_out\n",
    "%store do_loss\n",
    "%store do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the figure, add a 3d axis, set the viewing angle\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "x = np.array(do)[:,0]\n",
    "y = np.array(do)[:,1]\n",
    "\n",
    "p = ax.scatter(x, y, c=do_loss, s=200)\n",
    "ax.set_xlabel('1st Dropout')\n",
    "ax.set_ylabel('2nd Dropout')\n",
    "ax.set_title('Dropout')\n",
    "plt.colorbar(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = np.random.randint(10, 120, n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initiate empty list to store best validation loss for a particular timestep\n",
    "t_loss = []\n",
    "steps = 1\n",
    "ts = []\n",
    "\n",
    "#Loop through the learning rate and train network\n",
    "for t in tt:\n",
    "    \n",
    "    #Generate inputs on timesteps\n",
    "    X_train_2, y_train_2 = get_inputs(X_train, y_train, batch_size, t)\n",
    "    X_val_2, y_val_2 = get_inputs(X_val, y_val, batch_size, t)\n",
    "    \n",
    "    #Build network\n",
    "    t_network = build_network(best_n_hidden, n_classes, best_drop_out, input_shape=(t, n_dim), batch_size=batch_size)\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optimizers.Adam(lr=best_lr)\n",
    "    \n",
    "    #Compile\n",
    "    t_network.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    #Train\n",
    "    fitted_timesteps = t_network.fit(X_train_2, y_train_2, batch_size=batch_size, epochs=20, verbose=0, validation_data=(X_val_2, y_val_2), shuffle=True)\n",
    "    \n",
    "    #Min Validation loss\n",
    "    val_loss = np.min(fitted_timesteps.history['val_loss'])\n",
    "    \n",
    "    #Display\n",
    "    print('Steps: %d,' % steps, 'Timesteps: %d' % t, 'Best Validation Loss: %.4f' % val_loss)\n",
    "    \n",
    "    #Save\n",
    "    t_loss.append(val_loss)\n",
    "    \n",
    "    #Update\n",
    "    steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_t_loss = np.min(t_loss)\n",
    "best_timestep = tt[np.argmin(t_loss)]\n",
    "print('Best Timestep: %d , ' % best_timestep, 'Validation Loss: %.4f' % best_t_loss)\n",
    "\n",
    "%store best_timestep\n",
    "%store t_loss\n",
    "%store tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_tloss = [l for _, l in sorted(zip(tt, t_loss))]\n",
    "sorted_t = sorted(tt)\n",
    "\n",
    "plt.plot(sorted_t, sorted_tloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Summary')\n",
    "print('-------------')\n",
    "print('Learning Rate: %.8f' % best_lr)\n",
    "print('Number of layers: ', best_n_hidden)\n",
    "print('Dropout Probability: (%.2f, %.2f)' % best_drop_out)\n",
    "print('Timesteps: ', best_timestep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 7: Refit using best parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In case the best parameters are not loaded..\n",
    "#best_drop_out = (0.16, 0.1)\n",
    "#best_lr = 0.00006226\n",
    "#best_n_hidden = (104, 50, 40)\n",
    "#best_timestep = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_best, y_train_best = get_inputs(X_train, y_train, batch_size, best_timestep)\n",
    "X_val_best, y_val_best = get_inputs(X_val, y_val, batch_size, best_timestep)\n",
    "X_test_best, y_test_best = get_inputs(X_test, y_test, 1, best_timestep) #Batch-size is 1 for online prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "#Precision\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "#Recall\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "#F1\n",
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score\n",
    "\n",
    "#Matthews Correlation\n",
    "def mcor(y_true, y_pred):\n",
    "    #matthews_correlation\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    " \n",
    " \n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    " \n",
    " \n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    " \n",
    " \n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    " \n",
    " \n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) \n",
    " \n",
    "    return numerator / (denominator + K.epsilon())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define early Stop call back\n",
    "earlyStopping = EarlyStopping(patience=20, min_delta=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Decay Rate\n",
    "#decay_rate=0.00009\n",
    "\n",
    "#Refit using tuned parameters\n",
    "best_network = build_network(best_n_hidden, n_classes, best_drop_out, input_shape=(best_timestep, n_dim), batch_size=batch_size)\n",
    "\n",
    "optimizer = optimizers.Adam(lr=best_lr)\n",
    "\n",
    "best_network.compile(loss=loss, optimizer=optimizer, metrics=['accuracy', mcor, f1_score, precision, recall])\n",
    "\n",
    "best_fit = best_network.fit(X_train_best, y_train_best, \n",
    "                            batch_size=batch_size, \n",
    "                            epochs=300, \n",
    "                            verbose=1,\n",
    "                            validation_data=(X_val_best, y_val_best),\n",
    "                            callbacks=[earlyStopping],\n",
    "                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save model\n",
    "best_network.save('best_fit.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Check Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function to load saved model\n",
    "from keras.models import load_model\n",
    "best_network = load_model('best_fit.h5', custom_objects={'mcor': mcor, 'precision': precision, 'recall': recall, 'f1_score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_best, y_train_best = get_inputs(X_train, y_train, batch_size, best_timestep)\n",
    "X_val_best, y_val_best = get_inputs(X_val, y_val, batch_size, best_timestep)\n",
    "X_test_best, y_test_best = get_inputs(X_test, y_test, 1, best_timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(366, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_best.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Loss Curves and other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-1f394ff264d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Plot metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplot_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mcor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'f1_score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_fit' is not defined"
     ]
    }
   ],
   "source": [
    "#Plot metrics\n",
    "plot_metric(best_fit, 'loss')\n",
    "plot_metric(best_fit, 'acc')\n",
    "plot_metric(best_fit, 'mcor')\n",
    "plot_metric(best_fit, 'f1_score')\n",
    "plot_metric(best_fit, 'precision')\n",
    "plot_metric(best_fit, 'recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Turn the batch-trained model capable of running online forecast instead of batches\n",
    "batched_weights = best_network.get_weights()\n",
    "\n",
    "#Redefine the Model\n",
    "new_best_fit = build_network(best_n_hidden, n_classes, best_drop_out, input_shape=(best_timestep, n_dim), batch_size=1)\n",
    "\n",
    "#Compile\n",
    "optimizer = optimizers.Nadam(lr=8e-6, clipvalue=1.)\n",
    "new_best_fit.compile(loss=loss, optimizer=optimizer, metrics=['accuracy', mcor, f1_score, precision, recall])\n",
    "\n",
    "#Set weights\n",
    "new_best_fit.set_weights(batched_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Get prediction probability\n",
    "pred_prob = new_best_fit.predict(X_test_best, batch_size=1, verbose=0)\n",
    "\n",
    "#Calculate prediction and true labels\n",
    "pred = np.argmax(pred_prob, axis=1)\n",
    "true = np.argmax(y_test_best, axis=1)\n",
    "\n",
    "#Get confusion matrix\n",
    "cm = confusion_matrix(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot confusion matrix\n",
    "def plot_confusion_matrix(cm):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix for Test Data')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(3)\n",
    "    plt.xticks(tick_marks, ['Down', 'Neutral', 'Up'], rotation=45)\n",
    "    plt.yticks(tick_marks, ['Down', 'Neutral', 'Up'])\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "        horizontalalignment=\"center\",\n",
    "        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVMAAAEmCAYAAADfpHMGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f39827de550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.63  Recall: 0.63 \n"
     ]
    }
   ],
   "source": [
    "#Precision & Recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision = precision_score(true, pred, average='micro')\n",
    "recall = recall_score(true, pred, average='micro')\n",
    "\n",
    "print(\"Precision: %.2f \" % precision, \"Recall: %.2f \" % recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zooming in whats happening to the mis-labeled ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get dates of the validation set\n",
    "_, _, dates_test = split_data(dates, train_size=0.7, val_size=0.15, test_size=0.15)\n",
    "_, _, ret_test = split_data(ret, train_size=0.7, val_size=0.15, test_size=0.15)\n",
    "dates_test = dates_test[:len(true)]\n",
    "ret_test = ret_test[:len(true)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(366, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_best.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2017-09-27T00:00:00.000000000', '2017-09-28T00:00:00.000000000',\n",
       "       '2017-09-29T00:00:00.000000000', '2017-10-03T00:00:00.000000000',\n",
       "       '2017-10-04T00:00:00.000000000', '2017-10-06T00:00:00.000000000',\n",
       "       '2017-10-09T00:00:00.000000000', '2017-10-10T00:00:00.000000000',\n",
       "       '2017-10-11T00:00:00.000000000', '2017-10-12T00:00:00.000000000'], dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_test[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_best[:10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ret_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, 2, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0207142 , -0.00685717, -0.01153415,  0.00693644, -0.00230681,\n",
       "       -0.00695252, -0.01641303, -0.01909366, -0.00241255, -0.00727276])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get the mislabelled dates\n",
    "mislabelled = pred_prob[true != pred]\n",
    "mislabelled_dates = dates_test[true != pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missed_ret = pd.DataFrame(ret[mislabelled_dates])\n",
    "missed_ret.columns = ['FR']\n",
    "missed_prob = pd.DataFrame(mislabelled, index=mislabelled_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = pd.concat([price_hist_data, idx_data, fund_data], axis=1)\n",
    "missed_features = features[features.index.isin(mislabelled_dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missed = pd.concat([missed_prob, missed_ret, missed_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>FR</th>\n",
       "      <th>P</th>\n",
       "      <th>P_L1</th>\n",
       "      <th>P_L2</th>\n",
       "      <th>P_L3</th>\n",
       "      <th>P_L4</th>\n",
       "      <th>P_L5</th>\n",
       "      <th>...</th>\n",
       "      <th>DY_LTM</th>\n",
       "      <th>DY_NTM</th>\n",
       "      <th>ADV_VOL</th>\n",
       "      <th>PAYOUT</th>\n",
       "      <th>ANALYST_SENTIMENT</th>\n",
       "      <th>EPS_GRW_FY1</th>\n",
       "      <th>EPS_GRW_FY2</th>\n",
       "      <th>PE_NTM</th>\n",
       "      <th>PE_LTM</th>\n",
       "      <th>C2D_LTM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-04-20</th>\n",
       "      <td>0.434756</td>\n",
       "      <td>0.146788</td>\n",
       "      <td>0.418456</td>\n",
       "      <td>0.020714</td>\n",
       "      <td>21.50</td>\n",
       "      <td>21.75</td>\n",
       "      <td>21.45</td>\n",
       "      <td>21.60</td>\n",
       "      <td>21.70</td>\n",
       "      <td>21.50</td>\n",
       "      <td>...</td>\n",
       "      <td>4.068474</td>\n",
       "      <td>3.828978</td>\n",
       "      <td>11.590799</td>\n",
       "      <td>42.461478</td>\n",
       "      <td>-1.963735</td>\n",
       "      <td>-1.073369</td>\n",
       "      <td>10.123532</td>\n",
       "      <td>11.103367</td>\n",
       "      <td>10.603759</td>\n",
       "      <td>67.595408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-25</th>\n",
       "      <td>0.515386</td>\n",
       "      <td>0.044461</td>\n",
       "      <td>0.440153</td>\n",
       "      <td>0.006936</td>\n",
       "      <td>21.55</td>\n",
       "      <td>21.80</td>\n",
       "      <td>21.95</td>\n",
       "      <td>21.50</td>\n",
       "      <td>21.75</td>\n",
       "      <td>21.45</td>\n",
       "      <td>...</td>\n",
       "      <td>4.057479</td>\n",
       "      <td>3.811668</td>\n",
       "      <td>16.482112</td>\n",
       "      <td>42.785891</td>\n",
       "      <td>-2.043126</td>\n",
       "      <td>-2.152342</td>\n",
       "      <td>10.778901</td>\n",
       "      <td>11.239259</td>\n",
       "      <td>10.675289</td>\n",
       "      <td>67.427745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-06</th>\n",
       "      <td>0.614321</td>\n",
       "      <td>0.086325</td>\n",
       "      <td>0.299354</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>20.20</td>\n",
       "      <td>20.55</td>\n",
       "      <td>20.70</td>\n",
       "      <td>20.75</td>\n",
       "      <td>21.15</td>\n",
       "      <td>21.50</td>\n",
       "      <td>...</td>\n",
       "      <td>4.118244</td>\n",
       "      <td>4.038814</td>\n",
       "      <td>1.691289</td>\n",
       "      <td>42.584806</td>\n",
       "      <td>-1.604566</td>\n",
       "      <td>-0.976716</td>\n",
       "      <td>10.912583</td>\n",
       "      <td>10.558240</td>\n",
       "      <td>10.484934</td>\n",
       "      <td>67.499839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-09</th>\n",
       "      <td>0.524602</td>\n",
       "      <td>0.100390</td>\n",
       "      <td>0.375008</td>\n",
       "      <td>0.007362</td>\n",
       "      <td>20.30</td>\n",
       "      <td>20.20</td>\n",
       "      <td>20.55</td>\n",
       "      <td>20.70</td>\n",
       "      <td>20.75</td>\n",
       "      <td>21.15</td>\n",
       "      <td>...</td>\n",
       "      <td>4.110919</td>\n",
       "      <td>4.026861</td>\n",
       "      <td>28.483613</td>\n",
       "      <td>42.550607</td>\n",
       "      <td>-1.198167</td>\n",
       "      <td>-1.345243</td>\n",
       "      <td>10.840772</td>\n",
       "      <td>10.581369</td>\n",
       "      <td>10.501459</td>\n",
       "      <td>67.377066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-13</th>\n",
       "      <td>0.583513</td>\n",
       "      <td>0.043036</td>\n",
       "      <td>0.373451</td>\n",
       "      <td>0.008515</td>\n",
       "      <td>19.88</td>\n",
       "      <td>20.05</td>\n",
       "      <td>20.20</td>\n",
       "      <td>20.45</td>\n",
       "      <td>20.30</td>\n",
       "      <td>20.20</td>\n",
       "      <td>...</td>\n",
       "      <td>4.139031</td>\n",
       "      <td>4.116072</td>\n",
       "      <td>7.450106</td>\n",
       "      <td>42.710777</td>\n",
       "      <td>-1.511836</td>\n",
       "      <td>-0.992540</td>\n",
       "      <td>10.530333</td>\n",
       "      <td>10.390811</td>\n",
       "      <td>10.437566</td>\n",
       "      <td>67.484042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-16</th>\n",
       "      <td>0.621057</td>\n",
       "      <td>0.037761</td>\n",
       "      <td>0.341182</td>\n",
       "      <td>0.012392</td>\n",
       "      <td>20.05</td>\n",
       "      <td>19.88</td>\n",
       "      <td>20.05</td>\n",
       "      <td>20.20</td>\n",
       "      <td>20.45</td>\n",
       "      <td>20.30</td>\n",
       "      <td>...</td>\n",
       "      <td>4.124824</td>\n",
       "      <td>4.079034</td>\n",
       "      <td>30.256050</td>\n",
       "      <td>42.691693</td>\n",
       "      <td>-1.496387</td>\n",
       "      <td>-1.121846</td>\n",
       "      <td>10.446739</td>\n",
       "      <td>10.479852</td>\n",
       "      <td>10.478562</td>\n",
       "      <td>70.105337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-19</th>\n",
       "      <td>0.484771</td>\n",
       "      <td>0.040109</td>\n",
       "      <td>0.475120</td>\n",
       "      <td>0.013490</td>\n",
       "      <td>19.88</td>\n",
       "      <td>19.96</td>\n",
       "      <td>20.30</td>\n",
       "      <td>20.05</td>\n",
       "      <td>19.88</td>\n",
       "      <td>20.05</td>\n",
       "      <td>...</td>\n",
       "      <td>4.131142</td>\n",
       "      <td>4.099246</td>\n",
       "      <td>6.892680</td>\n",
       "      <td>42.630444</td>\n",
       "      <td>-1.144312</td>\n",
       "      <td>-1.416077</td>\n",
       "      <td>10.559742</td>\n",
       "      <td>10.412988</td>\n",
       "      <td>10.461944</td>\n",
       "      <td>70.146143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-20</th>\n",
       "      <td>0.294101</td>\n",
       "      <td>0.041132</td>\n",
       "      <td>0.664767</td>\n",
       "      <td>-0.007472</td>\n",
       "      <td>20.15</td>\n",
       "      <td>19.88</td>\n",
       "      <td>19.96</td>\n",
       "      <td>20.30</td>\n",
       "      <td>20.05</td>\n",
       "      <td>19.88</td>\n",
       "      <td>...</td>\n",
       "      <td>4.121650</td>\n",
       "      <td>4.075130</td>\n",
       "      <td>93.051118</td>\n",
       "      <td>42.661645</td>\n",
       "      <td>-1.082689</td>\n",
       "      <td>-1.405518</td>\n",
       "      <td>10.403447</td>\n",
       "      <td>10.481982</td>\n",
       "      <td>10.490051</td>\n",
       "      <td>70.116864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-31</th>\n",
       "      <td>0.269983</td>\n",
       "      <td>0.063072</td>\n",
       "      <td>0.666945</td>\n",
       "      <td>-0.004751</td>\n",
       "      <td>21.10</td>\n",
       "      <td>20.85</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.65</td>\n",
       "      <td>20.60</td>\n",
       "      <td>20.10</td>\n",
       "      <td>...</td>\n",
       "      <td>4.028097</td>\n",
       "      <td>3.863667</td>\n",
       "      <td>73.114029</td>\n",
       "      <td>42.515740</td>\n",
       "      <td>-1.101235</td>\n",
       "      <td>-2.046212</td>\n",
       "      <td>10.193240</td>\n",
       "      <td>11.017405</td>\n",
       "      <td>10.750139</td>\n",
       "      <td>70.098490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-14</th>\n",
       "      <td>0.583772</td>\n",
       "      <td>0.033358</td>\n",
       "      <td>0.382871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.90</td>\n",
       "      <td>21.40</td>\n",
       "      <td>21.65</td>\n",
       "      <td>21.60</td>\n",
       "      <td>21.35</td>\n",
       "      <td>...</td>\n",
       "      <td>4.067740</td>\n",
       "      <td>4.002557</td>\n",
       "      <td>62.081115</td>\n",
       "      <td>42.510538</td>\n",
       "      <td>-1.247220</td>\n",
       "      <td>-2.229791</td>\n",
       "      <td>9.891184</td>\n",
       "      <td>10.634381</td>\n",
       "      <td>10.628345</td>\n",
       "      <td>68.770851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-15</th>\n",
       "      <td>0.357967</td>\n",
       "      <td>0.034925</td>\n",
       "      <td>0.607108</td>\n",
       "      <td>-0.016970</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.90</td>\n",
       "      <td>21.40</td>\n",
       "      <td>21.65</td>\n",
       "      <td>21.60</td>\n",
       "      <td>...</td>\n",
       "      <td>4.061584</td>\n",
       "      <td>3.992046</td>\n",
       "      <td>38.042128</td>\n",
       "      <td>42.502798</td>\n",
       "      <td>-1.208713</td>\n",
       "      <td>-2.097228</td>\n",
       "      <td>9.931172</td>\n",
       "      <td>10.660625</td>\n",
       "      <td>10.645222</td>\n",
       "      <td>68.812679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-27</th>\n",
       "      <td>0.324972</td>\n",
       "      <td>0.049839</td>\n",
       "      <td>0.625189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.70</td>\n",
       "      <td>20.75</td>\n",
       "      <td>21.35</td>\n",
       "      <td>21.30</td>\n",
       "      <td>21.05</td>\n",
       "      <td>20.85</td>\n",
       "      <td>...</td>\n",
       "      <td>4.058989</td>\n",
       "      <td>4.009248</td>\n",
       "      <td>68.220858</td>\n",
       "      <td>42.507743</td>\n",
       "      <td>-0.606301</td>\n",
       "      <td>-3.404875</td>\n",
       "      <td>9.792721</td>\n",
       "      <td>10.615301</td>\n",
       "      <td>10.664263</td>\n",
       "      <td>68.756887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-06</th>\n",
       "      <td>0.568602</td>\n",
       "      <td>0.044992</td>\n",
       "      <td>0.386406</td>\n",
       "      <td>0.009456</td>\n",
       "      <td>21.05</td>\n",
       "      <td>21.30</td>\n",
       "      <td>21.65</td>\n",
       "      <td>21.35</td>\n",
       "      <td>21.00</td>\n",
       "      <td>20.70</td>\n",
       "      <td>...</td>\n",
       "      <td>4.032749</td>\n",
       "      <td>3.977937</td>\n",
       "      <td>0.822993</td>\n",
       "      <td>42.746715</td>\n",
       "      <td>-0.715416</td>\n",
       "      <td>-2.327262</td>\n",
       "      <td>9.467096</td>\n",
       "      <td>10.758762</td>\n",
       "      <td>10.767108</td>\n",
       "      <td>68.816983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-07</th>\n",
       "      <td>0.442658</td>\n",
       "      <td>0.051733</td>\n",
       "      <td>0.505609</td>\n",
       "      <td>-0.004717</td>\n",
       "      <td>21.25</td>\n",
       "      <td>21.05</td>\n",
       "      <td>21.30</td>\n",
       "      <td>21.65</td>\n",
       "      <td>21.35</td>\n",
       "      <td>21.00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.014153</td>\n",
       "      <td>3.942874</td>\n",
       "      <td>96.504057</td>\n",
       "      <td>42.755401</td>\n",
       "      <td>-0.634819</td>\n",
       "      <td>-2.313849</td>\n",
       "      <td>9.428279</td>\n",
       "      <td>10.856539</td>\n",
       "      <td>10.823133</td>\n",
       "      <td>68.720730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-18</th>\n",
       "      <td>0.293543</td>\n",
       "      <td>0.141104</td>\n",
       "      <td>0.565353</td>\n",
       "      <td>-0.006734</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.15</td>\n",
       "      <td>21.90</td>\n",
       "      <td>21.80</td>\n",
       "      <td>21.40</td>\n",
       "      <td>...</td>\n",
       "      <td>3.896225</td>\n",
       "      <td>3.742301</td>\n",
       "      <td>86.426057</td>\n",
       "      <td>42.800635</td>\n",
       "      <td>-0.548463</td>\n",
       "      <td>-4.001861</td>\n",
       "      <td>9.533611</td>\n",
       "      <td>11.449891</td>\n",
       "      <td>11.217064</td>\n",
       "      <td>68.479372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-21</th>\n",
       "      <td>0.243431</td>\n",
       "      <td>0.096977</td>\n",
       "      <td>0.659593</td>\n",
       "      <td>-0.002220</td>\n",
       "      <td>22.55</td>\n",
       "      <td>22.50</td>\n",
       "      <td>22.20</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.15</td>\n",
       "      <td>...</td>\n",
       "      <td>3.874341</td>\n",
       "      <td>3.715036</td>\n",
       "      <td>54.203875</td>\n",
       "      <td>42.684862</td>\n",
       "      <td>-0.051039</td>\n",
       "      <td>-3.870005</td>\n",
       "      <td>9.442621</td>\n",
       "      <td>11.502959</td>\n",
       "      <td>11.264566</td>\n",
       "      <td>68.621841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-27</th>\n",
       "      <td>0.602132</td>\n",
       "      <td>0.070608</td>\n",
       "      <td>0.327260</td>\n",
       "      <td>0.004405</td>\n",
       "      <td>22.65</td>\n",
       "      <td>22.75</td>\n",
       "      <td>22.55</td>\n",
       "      <td>22.50</td>\n",
       "      <td>22.55</td>\n",
       "      <td>22.50</td>\n",
       "      <td>...</td>\n",
       "      <td>3.848879</td>\n",
       "      <td>3.682004</td>\n",
       "      <td>70.242360</td>\n",
       "      <td>42.783514</td>\n",
       "      <td>-0.736839</td>\n",
       "      <td>-4.238736</td>\n",
       "      <td>9.211291</td>\n",
       "      <td>11.631650</td>\n",
       "      <td>11.374069</td>\n",
       "      <td>68.544845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-29</th>\n",
       "      <td>0.621668</td>\n",
       "      <td>0.058729</td>\n",
       "      <td>0.319603</td>\n",
       "      <td>0.013275</td>\n",
       "      <td>22.45</td>\n",
       "      <td>22.75</td>\n",
       "      <td>22.65</td>\n",
       "      <td>22.75</td>\n",
       "      <td>22.55</td>\n",
       "      <td>22.50</td>\n",
       "      <td>...</td>\n",
       "      <td>3.873491</td>\n",
       "      <td>3.734549</td>\n",
       "      <td>1.801310</td>\n",
       "      <td>42.749931</td>\n",
       "      <td>-0.502164</td>\n",
       "      <td>-3.894912</td>\n",
       "      <td>9.140280</td>\n",
       "      <td>11.459047</td>\n",
       "      <td>11.281163</td>\n",
       "      <td>68.648117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-08</th>\n",
       "      <td>0.179899</td>\n",
       "      <td>0.132803</td>\n",
       "      <td>0.687297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.05</td>\n",
       "      <td>22.75</td>\n",
       "      <td>22.40</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.75</td>\n",
       "      <td>22.45</td>\n",
       "      <td>...</td>\n",
       "      <td>3.797758</td>\n",
       "      <td>3.646763</td>\n",
       "      <td>95.400603</td>\n",
       "      <td>42.853116</td>\n",
       "      <td>0.453372</td>\n",
       "      <td>-4.758081</td>\n",
       "      <td>9.113851</td>\n",
       "      <td>11.764012</td>\n",
       "      <td>11.540564</td>\n",
       "      <td>68.644820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-09</th>\n",
       "      <td>0.237916</td>\n",
       "      <td>0.168380</td>\n",
       "      <td>0.593705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.05</td>\n",
       "      <td>23.05</td>\n",
       "      <td>22.75</td>\n",
       "      <td>22.40</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.75</td>\n",
       "      <td>...</td>\n",
       "      <td>3.801507</td>\n",
       "      <td>3.657023</td>\n",
       "      <td>60.209760</td>\n",
       "      <td>42.909473</td>\n",
       "      <td>0.332051</td>\n",
       "      <td>-4.562621</td>\n",
       "      <td>8.886384</td>\n",
       "      <td>11.746115</td>\n",
       "      <td>11.527616</td>\n",
       "      <td>68.652588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-15</th>\n",
       "      <td>0.343844</td>\n",
       "      <td>0.205940</td>\n",
       "      <td>0.450217</td>\n",
       "      <td>-0.004246</td>\n",
       "      <td>23.60</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.15</td>\n",
       "      <td>23.05</td>\n",
       "      <td>23.05</td>\n",
       "      <td>23.05</td>\n",
       "      <td>...</td>\n",
       "      <td>3.738400</td>\n",
       "      <td>3.570570</td>\n",
       "      <td>80.264400</td>\n",
       "      <td>42.719155</td>\n",
       "      <td>0.084260</td>\n",
       "      <td>-4.652847</td>\n",
       "      <td>8.830968</td>\n",
       "      <td>11.976580</td>\n",
       "      <td>11.721907</td>\n",
       "      <td>74.640373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-16</th>\n",
       "      <td>0.319827</td>\n",
       "      <td>0.214834</td>\n",
       "      <td>0.465339</td>\n",
       "      <td>-0.004264</td>\n",
       "      <td>23.50</td>\n",
       "      <td>23.60</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.15</td>\n",
       "      <td>23.05</td>\n",
       "      <td>23.05</td>\n",
       "      <td>...</td>\n",
       "      <td>3.739604</td>\n",
       "      <td>3.576500</td>\n",
       "      <td>27.703002</td>\n",
       "      <td>42.665724</td>\n",
       "      <td>0.155620</td>\n",
       "      <td>-4.982358</td>\n",
       "      <td>8.747498</td>\n",
       "      <td>11.941627</td>\n",
       "      <td>11.703520</td>\n",
       "      <td>74.583263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-18</th>\n",
       "      <td>0.354538</td>\n",
       "      <td>0.217631</td>\n",
       "      <td>0.427831</td>\n",
       "      <td>-0.006349</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.40</td>\n",
       "      <td>23.50</td>\n",
       "      <td>23.60</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.15</td>\n",
       "      <td>...</td>\n",
       "      <td>3.723879</td>\n",
       "      <td>3.556804</td>\n",
       "      <td>83.161009</td>\n",
       "      <td>42.541049</td>\n",
       "      <td>0.459563</td>\n",
       "      <td>-4.835706</td>\n",
       "      <td>8.579218</td>\n",
       "      <td>11.972210</td>\n",
       "      <td>11.732709</td>\n",
       "      <td>74.616545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-22</th>\n",
       "      <td>0.380013</td>\n",
       "      <td>0.196823</td>\n",
       "      <td>0.423164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.65</td>\n",
       "      <td>23.55</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.40</td>\n",
       "      <td>23.50</td>\n",
       "      <td>23.60</td>\n",
       "      <td>...</td>\n",
       "      <td>3.714395</td>\n",
       "      <td>3.558742</td>\n",
       "      <td>48.633109</td>\n",
       "      <td>42.557820</td>\n",
       "      <td>-0.146778</td>\n",
       "      <td>-5.049368</td>\n",
       "      <td>8.793031</td>\n",
       "      <td>11.970724</td>\n",
       "      <td>11.774704</td>\n",
       "      <td>74.617148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-24</th>\n",
       "      <td>0.504711</td>\n",
       "      <td>0.115036</td>\n",
       "      <td>0.380253</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.45</td>\n",
       "      <td>23.65</td>\n",
       "      <td>23.65</td>\n",
       "      <td>23.55</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.40</td>\n",
       "      <td>...</td>\n",
       "      <td>3.736232</td>\n",
       "      <td>3.587063</td>\n",
       "      <td>2.403120</td>\n",
       "      <td>42.413307</td>\n",
       "      <td>-0.296639</td>\n",
       "      <td>-4.446546</td>\n",
       "      <td>9.050503</td>\n",
       "      <td>11.834736</td>\n",
       "      <td>11.710599</td>\n",
       "      <td>74.660142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-25</th>\n",
       "      <td>0.486045</td>\n",
       "      <td>0.096507</td>\n",
       "      <td>0.417447</td>\n",
       "      <td>0.006376</td>\n",
       "      <td>23.45</td>\n",
       "      <td>23.45</td>\n",
       "      <td>23.65</td>\n",
       "      <td>23.65</td>\n",
       "      <td>23.55</td>\n",
       "      <td>23.70</td>\n",
       "      <td>...</td>\n",
       "      <td>3.731922</td>\n",
       "      <td>3.586912</td>\n",
       "      <td>58.464180</td>\n",
       "      <td>42.486310</td>\n",
       "      <td>-0.262240</td>\n",
       "      <td>-4.734493</td>\n",
       "      <td>8.938010</td>\n",
       "      <td>11.855741</td>\n",
       "      <td>11.727858</td>\n",
       "      <td>74.683131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-26</th>\n",
       "      <td>0.394776</td>\n",
       "      <td>0.090482</td>\n",
       "      <td>0.514742</td>\n",
       "      <td>-0.004246</td>\n",
       "      <td>23.60</td>\n",
       "      <td>23.45</td>\n",
       "      <td>23.45</td>\n",
       "      <td>23.65</td>\n",
       "      <td>23.65</td>\n",
       "      <td>23.55</td>\n",
       "      <td>...</td>\n",
       "      <td>3.719861</td>\n",
       "      <td>3.572408</td>\n",
       "      <td>61.846261</td>\n",
       "      <td>42.456250</td>\n",
       "      <td>-0.470951</td>\n",
       "      <td>-4.636828</td>\n",
       "      <td>8.667592</td>\n",
       "      <td>11.895182</td>\n",
       "      <td>11.750312</td>\n",
       "      <td>74.630838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-30</th>\n",
       "      <td>0.272712</td>\n",
       "      <td>0.067554</td>\n",
       "      <td>0.659735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.65</td>\n",
       "      <td>23.50</td>\n",
       "      <td>23.60</td>\n",
       "      <td>23.45</td>\n",
       "      <td>23.45</td>\n",
       "      <td>23.65</td>\n",
       "      <td>...</td>\n",
       "      <td>3.702938</td>\n",
       "      <td>3.557426</td>\n",
       "      <td>87.880909</td>\n",
       "      <td>42.411226</td>\n",
       "      <td>-0.501551</td>\n",
       "      <td>-4.969687</td>\n",
       "      <td>9.233946</td>\n",
       "      <td>11.932816</td>\n",
       "      <td>11.837590</td>\n",
       "      <td>74.672598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-01</th>\n",
       "      <td>0.155024</td>\n",
       "      <td>0.040142</td>\n",
       "      <td>0.804835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.65</td>\n",
       "      <td>23.65</td>\n",
       "      <td>23.50</td>\n",
       "      <td>23.60</td>\n",
       "      <td>23.45</td>\n",
       "      <td>...</td>\n",
       "      <td>3.678416</td>\n",
       "      <td>3.530737</td>\n",
       "      <td>87.039141</td>\n",
       "      <td>42.401615</td>\n",
       "      <td>-0.353195</td>\n",
       "      <td>-5.271282</td>\n",
       "      <td>9.328197</td>\n",
       "      <td>12.020694</td>\n",
       "      <td>11.921952</td>\n",
       "      <td>74.733438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-06</th>\n",
       "      <td>0.186674</td>\n",
       "      <td>0.037496</td>\n",
       "      <td>0.775831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.45</td>\n",
       "      <td>24.30</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.65</td>\n",
       "      <td>23.65</td>\n",
       "      <td>...</td>\n",
       "      <td>3.623335</td>\n",
       "      <td>3.466537</td>\n",
       "      <td>85.502358</td>\n",
       "      <td>42.227494</td>\n",
       "      <td>-0.802626</td>\n",
       "      <td>-5.976187</td>\n",
       "      <td>9.105479</td>\n",
       "      <td>12.192590</td>\n",
       "      <td>12.079621</td>\n",
       "      <td>74.352021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-28</th>\n",
       "      <td>0.502573</td>\n",
       "      <td>0.053877</td>\n",
       "      <td>0.443551</td>\n",
       "      <td>0.005709</td>\n",
       "      <td>26.20</td>\n",
       "      <td>26.25</td>\n",
       "      <td>26.30</td>\n",
       "      <td>26.20</td>\n",
       "      <td>26.15</td>\n",
       "      <td>26.05</td>\n",
       "      <td>...</td>\n",
       "      <td>3.370408</td>\n",
       "      <td>3.477889</td>\n",
       "      <td>17.132389</td>\n",
       "      <td>41.096181</td>\n",
       "      <td>0.481106</td>\n",
       "      <td>8.968729</td>\n",
       "      <td>8.595468</td>\n",
       "      <td>11.827076</td>\n",
       "      <td>12.579908</td>\n",
       "      <td>73.422626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-30</th>\n",
       "      <td>0.675601</td>\n",
       "      <td>0.038186</td>\n",
       "      <td>0.286213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.25</td>\n",
       "      <td>26.35</td>\n",
       "      <td>26.20</td>\n",
       "      <td>26.25</td>\n",
       "      <td>26.30</td>\n",
       "      <td>26.20</td>\n",
       "      <td>...</td>\n",
       "      <td>3.365910</td>\n",
       "      <td>3.464296</td>\n",
       "      <td>11.246004</td>\n",
       "      <td>41.126573</td>\n",
       "      <td>0.602469</td>\n",
       "      <td>8.933548</td>\n",
       "      <td>8.627647</td>\n",
       "      <td>11.881687</td>\n",
       "      <td>12.608755</td>\n",
       "      <td>73.429781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-04</th>\n",
       "      <td>0.660966</td>\n",
       "      <td>0.035362</td>\n",
       "      <td>0.303672</td>\n",
       "      <td>0.003839</td>\n",
       "      <td>26.00</td>\n",
       "      <td>26.25</td>\n",
       "      <td>26.25</td>\n",
       "      <td>26.35</td>\n",
       "      <td>26.20</td>\n",
       "      <td>26.25</td>\n",
       "      <td>...</td>\n",
       "      <td>3.391336</td>\n",
       "      <td>3.515769</td>\n",
       "      <td>25.375210</td>\n",
       "      <td>41.191884</td>\n",
       "      <td>0.519743</td>\n",
       "      <td>8.953591</td>\n",
       "      <td>8.945875</td>\n",
       "      <td>11.726664</td>\n",
       "      <td>12.536163</td>\n",
       "      <td>73.416351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-05</th>\n",
       "      <td>0.527076</td>\n",
       "      <td>0.040588</td>\n",
       "      <td>0.432336</td>\n",
       "      <td>0.001914</td>\n",
       "      <td>26.10</td>\n",
       "      <td>26.00</td>\n",
       "      <td>26.25</td>\n",
       "      <td>26.25</td>\n",
       "      <td>26.35</td>\n",
       "      <td>26.20</td>\n",
       "      <td>...</td>\n",
       "      <td>3.381339</td>\n",
       "      <td>3.498940</td>\n",
       "      <td>68.737359</td>\n",
       "      <td>41.199053</td>\n",
       "      <td>0.304445</td>\n",
       "      <td>8.891622</td>\n",
       "      <td>9.054457</td>\n",
       "      <td>11.785711</td>\n",
       "      <td>12.570410</td>\n",
       "      <td>73.393528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-06</th>\n",
       "      <td>0.421707</td>\n",
       "      <td>0.044342</td>\n",
       "      <td>0.533952</td>\n",
       "      <td>-0.005753</td>\n",
       "      <td>26.15</td>\n",
       "      <td>26.10</td>\n",
       "      <td>26.00</td>\n",
       "      <td>26.25</td>\n",
       "      <td>26.25</td>\n",
       "      <td>26.35</td>\n",
       "      <td>...</td>\n",
       "      <td>3.387833</td>\n",
       "      <td>3.513433</td>\n",
       "      <td>22.035525</td>\n",
       "      <td>41.202696</td>\n",
       "      <td>0.312739</td>\n",
       "      <td>9.478852</td>\n",
       "      <td>9.107990</td>\n",
       "      <td>11.738412</td>\n",
       "      <td>12.546637</td>\n",
       "      <td>73.425475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-17</th>\n",
       "      <td>0.244009</td>\n",
       "      <td>0.199388</td>\n",
       "      <td>0.556603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.15</td>\n",
       "      <td>27.05</td>\n",
       "      <td>27.00</td>\n",
       "      <td>26.70</td>\n",
       "      <td>26.55</td>\n",
       "      <td>26.15</td>\n",
       "      <td>...</td>\n",
       "      <td>3.327791</td>\n",
       "      <td>3.405849</td>\n",
       "      <td>70.272318</td>\n",
       "      <td>41.211419</td>\n",
       "      <td>0.819512</td>\n",
       "      <td>9.460414</td>\n",
       "      <td>9.057971</td>\n",
       "      <td>12.112557</td>\n",
       "      <td>12.757573</td>\n",
       "      <td>73.556962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-24</th>\n",
       "      <td>0.255162</td>\n",
       "      <td>0.214417</td>\n",
       "      <td>0.530421</td>\n",
       "      <td>-0.001817</td>\n",
       "      <td>27.55</td>\n",
       "      <td>27.40</td>\n",
       "      <td>27.35</td>\n",
       "      <td>27.30</td>\n",
       "      <td>27.15</td>\n",
       "      <td>27.15</td>\n",
       "      <td>...</td>\n",
       "      <td>3.303408</td>\n",
       "      <td>3.360399</td>\n",
       "      <td>56.941908</td>\n",
       "      <td>41.000239</td>\n",
       "      <td>1.379623</td>\n",
       "      <td>9.543608</td>\n",
       "      <td>8.973857</td>\n",
       "      <td>12.212648</td>\n",
       "      <td>12.822361</td>\n",
       "      <td>72.604425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-27</th>\n",
       "      <td>0.214070</td>\n",
       "      <td>0.157735</td>\n",
       "      <td>0.628195</td>\n",
       "      <td>-0.005410</td>\n",
       "      <td>27.80</td>\n",
       "      <td>27.60</td>\n",
       "      <td>27.50</td>\n",
       "      <td>27.55</td>\n",
       "      <td>27.40</td>\n",
       "      <td>27.35</td>\n",
       "      <td>...</td>\n",
       "      <td>3.281197</td>\n",
       "      <td>3.322277</td>\n",
       "      <td>66.211939</td>\n",
       "      <td>40.944073</td>\n",
       "      <td>1.153041</td>\n",
       "      <td>9.642761</td>\n",
       "      <td>8.939044</td>\n",
       "      <td>12.335731</td>\n",
       "      <td>12.899516</td>\n",
       "      <td>72.575859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-02</th>\n",
       "      <td>0.163995</td>\n",
       "      <td>0.043491</td>\n",
       "      <td>0.792514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.25</td>\n",
       "      <td>28.20</td>\n",
       "      <td>27.90</td>\n",
       "      <td>27.65</td>\n",
       "      <td>27.80</td>\n",
       "      <td>27.60</td>\n",
       "      <td>...</td>\n",
       "      <td>3.248259</td>\n",
       "      <td>3.269069</td>\n",
       "      <td>59.136275</td>\n",
       "      <td>40.998791</td>\n",
       "      <td>1.112476</td>\n",
       "      <td>10.724015</td>\n",
       "      <td>8.107127</td>\n",
       "      <td>12.551985</td>\n",
       "      <td>13.011554</td>\n",
       "      <td>72.620409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-03</th>\n",
       "      <td>0.231062</td>\n",
       "      <td>0.043146</td>\n",
       "      <td>0.725792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.25</td>\n",
       "      <td>28.25</td>\n",
       "      <td>28.20</td>\n",
       "      <td>27.90</td>\n",
       "      <td>27.65</td>\n",
       "      <td>27.80</td>\n",
       "      <td>...</td>\n",
       "      <td>3.255519</td>\n",
       "      <td>3.283851</td>\n",
       "      <td>49.558071</td>\n",
       "      <td>40.993472</td>\n",
       "      <td>1.263580</td>\n",
       "      <td>10.667937</td>\n",
       "      <td>8.154486</td>\n",
       "      <td>12.494020</td>\n",
       "      <td>12.980196</td>\n",
       "      <td>72.612047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-07</th>\n",
       "      <td>0.615638</td>\n",
       "      <td>0.043688</td>\n",
       "      <td>0.340675</td>\n",
       "      <td>0.007030</td>\n",
       "      <td>28.35</td>\n",
       "      <td>28.25</td>\n",
       "      <td>28.25</td>\n",
       "      <td>28.25</td>\n",
       "      <td>28.20</td>\n",
       "      <td>27.90</td>\n",
       "      <td>...</td>\n",
       "      <td>3.243014</td>\n",
       "      <td>3.273378</td>\n",
       "      <td>76.035894</td>\n",
       "      <td>40.759710</td>\n",
       "      <td>1.286746</td>\n",
       "      <td>10.368553</td>\n",
       "      <td>9.477894</td>\n",
       "      <td>12.464991</td>\n",
       "      <td>13.026673</td>\n",
       "      <td>72.627768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-11</th>\n",
       "      <td>0.659690</td>\n",
       "      <td>0.057861</td>\n",
       "      <td>0.282449</td>\n",
       "      <td>0.009001</td>\n",
       "      <td>27.65</td>\n",
       "      <td>28.15</td>\n",
       "      <td>28.45</td>\n",
       "      <td>28.55</td>\n",
       "      <td>28.35</td>\n",
       "      <td>28.25</td>\n",
       "      <td>...</td>\n",
       "      <td>3.444638</td>\n",
       "      <td>3.470811</td>\n",
       "      <td>2.930372</td>\n",
       "      <td>41.754694</td>\n",
       "      <td>1.767213</td>\n",
       "      <td>11.683401</td>\n",
       "      <td>9.291095</td>\n",
       "      <td>12.025265</td>\n",
       "      <td>12.746579</td>\n",
       "      <td>72.602445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-16</th>\n",
       "      <td>0.326919</td>\n",
       "      <td>0.199614</td>\n",
       "      <td>0.473467</td>\n",
       "      <td>-0.001781</td>\n",
       "      <td>28.10</td>\n",
       "      <td>27.85</td>\n",
       "      <td>27.90</td>\n",
       "      <td>27.65</td>\n",
       "      <td>28.15</td>\n",
       "      <td>28.45</td>\n",
       "      <td>...</td>\n",
       "      <td>3.404074</td>\n",
       "      <td>3.406656</td>\n",
       "      <td>92.922049</td>\n",
       "      <td>41.764499</td>\n",
       "      <td>1.739282</td>\n",
       "      <td>11.122872</td>\n",
       "      <td>9.222073</td>\n",
       "      <td>12.255270</td>\n",
       "      <td>12.902772</td>\n",
       "      <td>71.539184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-17</th>\n",
       "      <td>0.296425</td>\n",
       "      <td>0.228994</td>\n",
       "      <td>0.474581</td>\n",
       "      <td>-0.010753</td>\n",
       "      <td>28.05</td>\n",
       "      <td>28.10</td>\n",
       "      <td>27.85</td>\n",
       "      <td>27.90</td>\n",
       "      <td>27.65</td>\n",
       "      <td>28.15</td>\n",
       "      <td>...</td>\n",
       "      <td>3.413805</td>\n",
       "      <td>3.417802</td>\n",
       "      <td>19.085708</td>\n",
       "      <td>41.747738</td>\n",
       "      <td>2.064660</td>\n",
       "      <td>11.258703</td>\n",
       "      <td>9.202595</td>\n",
       "      <td>12.209994</td>\n",
       "      <td>12.874139</td>\n",
       "      <td>71.577103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-28</th>\n",
       "      <td>0.315784</td>\n",
       "      <td>0.137690</td>\n",
       "      <td>0.546525</td>\n",
       "      <td>-0.005249</td>\n",
       "      <td>28.65</td>\n",
       "      <td>28.55</td>\n",
       "      <td>28.20</td>\n",
       "      <td>28.15</td>\n",
       "      <td>27.90</td>\n",
       "      <td>27.75</td>\n",
       "      <td>...</td>\n",
       "      <td>3.470232</td>\n",
       "      <td>3.419260</td>\n",
       "      <td>60.941742</td>\n",
       "      <td>42.089834</td>\n",
       "      <td>2.552754</td>\n",
       "      <td>12.596304</td>\n",
       "      <td>9.034095</td>\n",
       "      <td>12.296748</td>\n",
       "      <td>12.950307</td>\n",
       "      <td>71.600163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-31</th>\n",
       "      <td>0.688742</td>\n",
       "      <td>0.053117</td>\n",
       "      <td>0.258141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.75</td>\n",
       "      <td>28.85</td>\n",
       "      <td>28.50</td>\n",
       "      <td>28.65</td>\n",
       "      <td>28.55</td>\n",
       "      <td>28.20</td>\n",
       "      <td>...</td>\n",
       "      <td>3.482192</td>\n",
       "      <td>3.429063</td>\n",
       "      <td>14.842826</td>\n",
       "      <td>41.920406</td>\n",
       "      <td>3.575071</td>\n",
       "      <td>13.470731</td>\n",
       "      <td>8.985164</td>\n",
       "      <td>12.212043</td>\n",
       "      <td>12.895686</td>\n",
       "      <td>71.610520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-04</th>\n",
       "      <td>0.621339</td>\n",
       "      <td>0.041242</td>\n",
       "      <td>0.337419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.60</td>\n",
       "      <td>28.75</td>\n",
       "      <td>28.75</td>\n",
       "      <td>28.85</td>\n",
       "      <td>28.50</td>\n",
       "      <td>28.65</td>\n",
       "      <td>...</td>\n",
       "      <td>3.544998</td>\n",
       "      <td>3.507692</td>\n",
       "      <td>7.177266</td>\n",
       "      <td>41.957628</td>\n",
       "      <td>3.701359</td>\n",
       "      <td>14.165952</td>\n",
       "      <td>8.973110</td>\n",
       "      <td>11.948565</td>\n",
       "      <td>12.715435</td>\n",
       "      <td>70.318127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-06</th>\n",
       "      <td>0.355388</td>\n",
       "      <td>0.044440</td>\n",
       "      <td>0.600172</td>\n",
       "      <td>-0.001759</td>\n",
       "      <td>28.45</td>\n",
       "      <td>28.60</td>\n",
       "      <td>28.60</td>\n",
       "      <td>28.75</td>\n",
       "      <td>28.75</td>\n",
       "      <td>28.85</td>\n",
       "      <td>...</td>\n",
       "      <td>3.560984</td>\n",
       "      <td>3.526456</td>\n",
       "      <td>11.388969</td>\n",
       "      <td>41.931090</td>\n",
       "      <td>3.737196</td>\n",
       "      <td>14.556902</td>\n",
       "      <td>8.928424</td>\n",
       "      <td>11.877607</td>\n",
       "      <td>12.664891</td>\n",
       "      <td>70.339969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-11</th>\n",
       "      <td>0.303335</td>\n",
       "      <td>0.091630</td>\n",
       "      <td>0.605035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.85</td>\n",
       "      <td>28.55</td>\n",
       "      <td>28.40</td>\n",
       "      <td>28.45</td>\n",
       "      <td>28.60</td>\n",
       "      <td>28.60</td>\n",
       "      <td>...</td>\n",
       "      <td>3.534110</td>\n",
       "      <td>3.483848</td>\n",
       "      <td>85.617496</td>\n",
       "      <td>41.812067</td>\n",
       "      <td>3.356299</td>\n",
       "      <td>14.367601</td>\n",
       "      <td>8.796453</td>\n",
       "      <td>11.989535</td>\n",
       "      <td>12.747040</td>\n",
       "      <td>70.346184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-12</th>\n",
       "      <td>0.377620</td>\n",
       "      <td>0.115544</td>\n",
       "      <td>0.506836</td>\n",
       "      <td>-0.003472</td>\n",
       "      <td>28.85</td>\n",
       "      <td>28.85</td>\n",
       "      <td>28.55</td>\n",
       "      <td>28.40</td>\n",
       "      <td>28.45</td>\n",
       "      <td>28.60</td>\n",
       "      <td>...</td>\n",
       "      <td>3.533356</td>\n",
       "      <td>3.480221</td>\n",
       "      <td>32.076145</td>\n",
       "      <td>41.805430</td>\n",
       "      <td>3.347032</td>\n",
       "      <td>14.499912</td>\n",
       "      <td>8.793700</td>\n",
       "      <td>12.000091</td>\n",
       "      <td>12.756402</td>\n",
       "      <td>70.318433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-13</th>\n",
       "      <td>0.404449</td>\n",
       "      <td>0.122566</td>\n",
       "      <td>0.472985</td>\n",
       "      <td>-0.001741</td>\n",
       "      <td>28.75</td>\n",
       "      <td>28.85</td>\n",
       "      <td>28.85</td>\n",
       "      <td>28.55</td>\n",
       "      <td>28.40</td>\n",
       "      <td>28.45</td>\n",
       "      <td>...</td>\n",
       "      <td>3.541250</td>\n",
       "      <td>3.490442</td>\n",
       "      <td>15.101237</td>\n",
       "      <td>41.787155</td>\n",
       "      <td>3.377684</td>\n",
       "      <td>14.629248</td>\n",
       "      <td>8.819479</td>\n",
       "      <td>11.959836</td>\n",
       "      <td>12.729659</td>\n",
       "      <td>70.321576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-14</th>\n",
       "      <td>0.420702</td>\n",
       "      <td>0.119056</td>\n",
       "      <td>0.460242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.70</td>\n",
       "      <td>28.75</td>\n",
       "      <td>28.85</td>\n",
       "      <td>28.85</td>\n",
       "      <td>28.55</td>\n",
       "      <td>28.40</td>\n",
       "      <td>...</td>\n",
       "      <td>3.547365</td>\n",
       "      <td>3.495693</td>\n",
       "      <td>11.810046</td>\n",
       "      <td>41.731563</td>\n",
       "      <td>3.179734</td>\n",
       "      <td>14.906947</td>\n",
       "      <td>8.759065</td>\n",
       "      <td>11.925989</td>\n",
       "      <td>12.702644</td>\n",
       "      <td>70.338739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-19</th>\n",
       "      <td>0.684930</td>\n",
       "      <td>0.055103</td>\n",
       "      <td>0.259967</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>28.95</td>\n",
       "      <td>29.00</td>\n",
       "      <td>28.70</td>\n",
       "      <td>28.70</td>\n",
       "      <td>28.75</td>\n",
       "      <td>28.85</td>\n",
       "      <td>...</td>\n",
       "      <td>3.525355</td>\n",
       "      <td>3.458252</td>\n",
       "      <td>31.710808</td>\n",
       "      <td>41.523884</td>\n",
       "      <td>2.786476</td>\n",
       "      <td>14.072927</td>\n",
       "      <td>8.232707</td>\n",
       "      <td>11.996128</td>\n",
       "      <td>12.732913</td>\n",
       "      <td>70.330956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-20</th>\n",
       "      <td>0.752393</td>\n",
       "      <td>0.038212</td>\n",
       "      <td>0.209395</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.00</td>\n",
       "      <td>28.95</td>\n",
       "      <td>29.00</td>\n",
       "      <td>28.70</td>\n",
       "      <td>28.70</td>\n",
       "      <td>28.75</td>\n",
       "      <td>...</td>\n",
       "      <td>3.519467</td>\n",
       "      <td>3.448640</td>\n",
       "      <td>71.997060</td>\n",
       "      <td>41.537747</td>\n",
       "      <td>2.837431</td>\n",
       "      <td>14.089280</td>\n",
       "      <td>8.570956</td>\n",
       "      <td>12.033175</td>\n",
       "      <td>12.782918</td>\n",
       "      <td>70.353419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-25</th>\n",
       "      <td>0.696926</td>\n",
       "      <td>0.031692</td>\n",
       "      <td>0.271382</td>\n",
       "      <td>0.001756</td>\n",
       "      <td>28.45</td>\n",
       "      <td>28.75</td>\n",
       "      <td>29.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>28.95</td>\n",
       "      <td>29.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.579470</td>\n",
       "      <td>3.522255</td>\n",
       "      <td>4.066479</td>\n",
       "      <td>41.476129</td>\n",
       "      <td>2.109530</td>\n",
       "      <td>15.204982</td>\n",
       "      <td>8.614512</td>\n",
       "      <td>11.764659</td>\n",
       "      <td>12.585849</td>\n",
       "      <td>70.282954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26</th>\n",
       "      <td>0.543056</td>\n",
       "      <td>0.037126</td>\n",
       "      <td>0.419819</td>\n",
       "      <td>0.003503</td>\n",
       "      <td>28.50</td>\n",
       "      <td>28.45</td>\n",
       "      <td>28.75</td>\n",
       "      <td>29.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>28.95</td>\n",
       "      <td>...</td>\n",
       "      <td>3.573574</td>\n",
       "      <td>3.514086</td>\n",
       "      <td>69.334588</td>\n",
       "      <td>41.459115</td>\n",
       "      <td>2.025636</td>\n",
       "      <td>14.852043</td>\n",
       "      <td>8.615487</td>\n",
       "      <td>11.787333</td>\n",
       "      <td>12.604815</td>\n",
       "      <td>70.233417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-27</th>\n",
       "      <td>0.376486</td>\n",
       "      <td>0.042067</td>\n",
       "      <td>0.581447</td>\n",
       "      <td>-0.010545</td>\n",
       "      <td>28.60</td>\n",
       "      <td>28.50</td>\n",
       "      <td>28.45</td>\n",
       "      <td>28.75</td>\n",
       "      <td>29.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.565100</td>\n",
       "      <td>3.501760</td>\n",
       "      <td>90.892027</td>\n",
       "      <td>41.527257</td>\n",
       "      <td>1.986992</td>\n",
       "      <td>14.427145</td>\n",
       "      <td>8.630719</td>\n",
       "      <td>11.848342</td>\n",
       "      <td>12.654949</td>\n",
       "      <td>70.223525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-06</th>\n",
       "      <td>0.328420</td>\n",
       "      <td>0.204651</td>\n",
       "      <td>0.466929</td>\n",
       "      <td>-0.003413</td>\n",
       "      <td>29.35</td>\n",
       "      <td>29.25</td>\n",
       "      <td>29.05</td>\n",
       "      <td>28.40</td>\n",
       "      <td>28.30</td>\n",
       "      <td>28.60</td>\n",
       "      <td>...</td>\n",
       "      <td>3.499198</td>\n",
       "      <td>3.398304</td>\n",
       "      <td>66.291905</td>\n",
       "      <td>41.440106</td>\n",
       "      <td>0.074004</td>\n",
       "      <td>12.898732</td>\n",
       "      <td>8.706145</td>\n",
       "      <td>12.183516</td>\n",
       "      <td>12.946614</td>\n",
       "      <td>70.229726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-10</th>\n",
       "      <td>0.332126</td>\n",
       "      <td>0.190799</td>\n",
       "      <td>0.477075</td>\n",
       "      <td>-0.005115</td>\n",
       "      <td>29.40</td>\n",
       "      <td>29.25</td>\n",
       "      <td>29.35</td>\n",
       "      <td>29.25</td>\n",
       "      <td>29.05</td>\n",
       "      <td>28.40</td>\n",
       "      <td>...</td>\n",
       "      <td>3.508207</td>\n",
       "      <td>3.403824</td>\n",
       "      <td>77.259550</td>\n",
       "      <td>41.358688</td>\n",
       "      <td>-0.152586</td>\n",
       "      <td>13.496835</td>\n",
       "      <td>8.732445</td>\n",
       "      <td>12.140123</td>\n",
       "      <td>12.923466</td>\n",
       "      <td>70.252045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-12</th>\n",
       "      <td>0.336334</td>\n",
       "      <td>0.177811</td>\n",
       "      <td>0.485855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.40</td>\n",
       "      <td>29.25</td>\n",
       "      <td>29.40</td>\n",
       "      <td>29.25</td>\n",
       "      <td>29.35</td>\n",
       "      <td>29.25</td>\n",
       "      <td>...</td>\n",
       "      <td>3.517974</td>\n",
       "      <td>3.407815</td>\n",
       "      <td>40.944210</td>\n",
       "      <td>41.371488</td>\n",
       "      <td>0.201694</td>\n",
       "      <td>13.527734</td>\n",
       "      <td>8.739555</td>\n",
       "      <td>12.129446</td>\n",
       "      <td>12.920379</td>\n",
       "      <td>70.258010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2        FR      P   P_L1   P_L2  \\\n",
       "Date                                                                      \n",
       "2016-04-20  0.434756  0.146788  0.418456  0.020714  21.50  21.75  21.45   \n",
       "2016-04-25  0.515386  0.044461  0.440153  0.006936  21.55  21.80  21.95   \n",
       "2016-05-06  0.614321  0.086325  0.299354  0.004938  20.20  20.55  20.70   \n",
       "2016-05-09  0.524602  0.100390  0.375008  0.007362  20.30  20.20  20.55   \n",
       "2016-05-13  0.583513  0.043036  0.373451  0.008515  19.88  20.05  20.20   \n",
       "2016-05-16  0.621057  0.037761  0.341182  0.012392  20.05  19.88  20.05   \n",
       "2016-05-19  0.484771  0.040109  0.475120  0.013490  19.88  19.96  20.30   \n",
       "2016-05-20  0.294101  0.041132  0.664767 -0.007472  20.15  19.88  19.96   \n",
       "2016-05-31  0.269983  0.063072  0.666945 -0.004751  21.10  20.85  20.80   \n",
       "2016-06-14  0.583772  0.033358  0.382871  0.000000  20.80  20.90  21.40   \n",
       "2016-06-15  0.357967  0.034925  0.607108 -0.016970  20.80  20.80  20.90   \n",
       "2016-06-27  0.324972  0.049839  0.625189  0.000000  20.70  20.75  21.35   \n",
       "2016-07-06  0.568602  0.044992  0.386406  0.009456  21.05  21.30  21.65   \n",
       "2016-07-07  0.442658  0.051733  0.505609 -0.004717  21.25  21.05  21.30   \n",
       "2016-07-18  0.293543  0.141104  0.565353 -0.006734  22.35  22.30  22.15   \n",
       "2016-07-21  0.243431  0.096977  0.659593 -0.002220  22.55  22.50  22.20   \n",
       "2016-07-27  0.602132  0.070608  0.327260  0.004405  22.65  22.75  22.55   \n",
       "2016-07-29  0.621668  0.058729  0.319603  0.013275  22.45  22.75  22.65   \n",
       "2016-08-08  0.179899  0.132803  0.687297  0.000000  23.05  22.75  22.40   \n",
       "2016-08-09  0.237916  0.168380  0.593705  0.000000  23.05  23.05  22.75   \n",
       "2016-08-15  0.343844  0.205940  0.450217 -0.004246  23.60  23.35  23.15   \n",
       "2016-08-16  0.319827  0.214834  0.465339 -0.004264  23.50  23.60  23.35   \n",
       "2016-08-18  0.354538  0.217631  0.427831 -0.006349  23.70  23.40  23.50   \n",
       "2016-08-22  0.380013  0.196823  0.423164  0.000000  23.65  23.55  23.70   \n",
       "2016-08-24  0.504711  0.115036  0.380253  0.000000  23.45  23.65  23.65   \n",
       "2016-08-25  0.486045  0.096507  0.417447  0.006376  23.45  23.45  23.65   \n",
       "2016-08-26  0.394776  0.090482  0.514742 -0.004246  23.60  23.45  23.45   \n",
       "2016-08-30  0.272712  0.067554  0.659735  0.000000  23.65  23.50  23.60   \n",
       "2016-09-01  0.155024  0.040142  0.804835  0.000000  23.90  23.65  23.65   \n",
       "2016-09-06  0.186674  0.037496  0.775831  0.000000  24.45  24.30  23.90   \n",
       "...              ...       ...       ...       ...    ...    ...    ...   \n",
       "2017-06-28  0.502573  0.053877  0.443551  0.005709  26.20  26.25  26.30   \n",
       "2017-06-30  0.675601  0.038186  0.286213  0.000000  26.25  26.35  26.20   \n",
       "2017-07-04  0.660966  0.035362  0.303672  0.003839  26.00  26.25  26.25   \n",
       "2017-07-05  0.527076  0.040588  0.432336  0.001914  26.10  26.00  26.25   \n",
       "2017-07-06  0.421707  0.044342  0.533952 -0.005753  26.15  26.10  26.00   \n",
       "2017-07-17  0.244009  0.199388  0.556603  0.000000  27.15  27.05  27.00   \n",
       "2017-07-24  0.255162  0.214417  0.530421 -0.001817  27.55  27.40  27.35   \n",
       "2017-07-27  0.214070  0.157735  0.628195 -0.005410  27.80  27.60  27.50   \n",
       "2017-08-02  0.163995  0.043491  0.792514  0.000000  28.25  28.20  27.90   \n",
       "2017-08-03  0.231062  0.043146  0.725792  0.000000  28.25  28.25  28.20   \n",
       "2017-08-07  0.615638  0.043688  0.340675  0.007030  28.35  28.25  28.25   \n",
       "2017-08-11  0.659690  0.057861  0.282449  0.009001  27.65  28.15  28.45   \n",
       "2017-08-16  0.326919  0.199614  0.473467 -0.001781  28.10  27.85  27.90   \n",
       "2017-08-17  0.296425  0.228994  0.474581 -0.010753  28.05  28.10  27.85   \n",
       "2017-08-28  0.315784  0.137690  0.546525 -0.005249  28.65  28.55  28.20   \n",
       "2017-08-31  0.688742  0.053117  0.258141  0.000000  28.75  28.85  28.50   \n",
       "2017-09-04  0.621339  0.041242  0.337419  0.000000  28.60  28.75  28.75   \n",
       "2017-09-06  0.355388  0.044440  0.600172 -0.001759  28.45  28.60  28.60   \n",
       "2017-09-11  0.303335  0.091630  0.605035  0.000000  28.85  28.55  28.40   \n",
       "2017-09-12  0.377620  0.115544  0.506836 -0.003472  28.85  28.85  28.55   \n",
       "2017-09-13  0.404449  0.122566  0.472985 -0.001741  28.75  28.85  28.85   \n",
       "2017-09-14  0.420702  0.119056  0.460242  0.000000  28.70  28.75  28.85   \n",
       "2017-09-19  0.684930  0.055103  0.259967  0.001726  28.95  29.00  28.70   \n",
       "2017-09-20  0.752393  0.038212  0.209395  0.000000  29.00  28.95  29.00   \n",
       "2017-09-25  0.696926  0.031692  0.271382  0.001756  28.45  28.75  29.00   \n",
       "2017-09-26  0.543056  0.037126  0.419819  0.003503  28.50  28.45  28.75   \n",
       "2017-09-27  0.376486  0.042067  0.581447 -0.010545  28.60  28.50  28.45   \n",
       "2017-10-06  0.328420  0.204651  0.466929 -0.003413  29.35  29.25  29.05   \n",
       "2017-10-10  0.332126  0.190799  0.477075 -0.005115  29.40  29.25  29.35   \n",
       "2017-10-12  0.336334  0.177811  0.485855  0.000000  29.40  29.25  29.40   \n",
       "\n",
       "             P_L3   P_L4   P_L5    ...        DY_LTM    DY_NTM    ADV_VOL  \\\n",
       "Date                               ...                                      \n",
       "2016-04-20  21.60  21.70  21.50    ...      4.068474  3.828978  11.590799   \n",
       "2016-04-25  21.50  21.75  21.45    ...      4.057479  3.811668  16.482112   \n",
       "2016-05-06  20.75  21.15  21.50    ...      4.118244  4.038814   1.691289   \n",
       "2016-05-09  20.70  20.75  21.15    ...      4.110919  4.026861  28.483613   \n",
       "2016-05-13  20.45  20.30  20.20    ...      4.139031  4.116072   7.450106   \n",
       "2016-05-16  20.20  20.45  20.30    ...      4.124824  4.079034  30.256050   \n",
       "2016-05-19  20.05  19.88  20.05    ...      4.131142  4.099246   6.892680   \n",
       "2016-05-20  20.30  20.05  19.88    ...      4.121650  4.075130  93.051118   \n",
       "2016-05-31  20.65  20.60  20.10    ...      4.028097  3.863667  73.114029   \n",
       "2016-06-14  21.65  21.60  21.35    ...      4.067740  4.002557  62.081115   \n",
       "2016-06-15  21.40  21.65  21.60    ...      4.061584  3.992046  38.042128   \n",
       "2016-06-27  21.30  21.05  20.85    ...      4.058989  4.009248  68.220858   \n",
       "2016-07-06  21.35  21.00  20.70    ...      4.032749  3.977937   0.822993   \n",
       "2016-07-07  21.65  21.35  21.00    ...      4.014153  3.942874  96.504057   \n",
       "2016-07-18  21.90  21.80  21.40    ...      3.896225  3.742301  86.426057   \n",
       "2016-07-21  22.35  22.30  22.15    ...      3.874341  3.715036  54.203875   \n",
       "2016-07-27  22.50  22.55  22.50    ...      3.848879  3.682004  70.242360   \n",
       "2016-07-29  22.75  22.55  22.50    ...      3.873491  3.734549   1.801310   \n",
       "2016-08-08  22.30  22.75  22.45    ...      3.797758  3.646763  95.400603   \n",
       "2016-08-09  22.40  22.30  22.75    ...      3.801507  3.657023  60.209760   \n",
       "2016-08-15  23.05  23.05  23.05    ...      3.738400  3.570570  80.264400   \n",
       "2016-08-16  23.15  23.05  23.05    ...      3.739604  3.576500  27.703002   \n",
       "2016-08-18  23.60  23.35  23.15    ...      3.723879  3.556804  83.161009   \n",
       "2016-08-22  23.40  23.50  23.60    ...      3.714395  3.558742  48.633109   \n",
       "2016-08-24  23.55  23.70  23.40    ...      3.736232  3.587063   2.403120   \n",
       "2016-08-25  23.65  23.55  23.70    ...      3.731922  3.586912  58.464180   \n",
       "2016-08-26  23.65  23.65  23.55    ...      3.719861  3.572408  61.846261   \n",
       "2016-08-30  23.45  23.45  23.65    ...      3.702938  3.557426  87.880909   \n",
       "2016-09-01  23.50  23.60  23.45    ...      3.678416  3.530737  87.039141   \n",
       "2016-09-06  23.90  23.65  23.65    ...      3.623335  3.466537  85.502358   \n",
       "...           ...    ...    ...    ...           ...       ...        ...   \n",
       "2017-06-28  26.20  26.15  26.05    ...      3.370408  3.477889  17.132389   \n",
       "2017-06-30  26.25  26.30  26.20    ...      3.365910  3.464296  11.246004   \n",
       "2017-07-04  26.35  26.20  26.25    ...      3.391336  3.515769  25.375210   \n",
       "2017-07-05  26.25  26.35  26.20    ...      3.381339  3.498940  68.737359   \n",
       "2017-07-06  26.25  26.25  26.35    ...      3.387833  3.513433  22.035525   \n",
       "2017-07-17  26.70  26.55  26.15    ...      3.327791  3.405849  70.272318   \n",
       "2017-07-24  27.30  27.15  27.15    ...      3.303408  3.360399  56.941908   \n",
       "2017-07-27  27.55  27.40  27.35    ...      3.281197  3.322277  66.211939   \n",
       "2017-08-02  27.65  27.80  27.60    ...      3.248259  3.269069  59.136275   \n",
       "2017-08-03  27.90  27.65  27.80    ...      3.255519  3.283851  49.558071   \n",
       "2017-08-07  28.25  28.20  27.90    ...      3.243014  3.273378  76.035894   \n",
       "2017-08-11  28.55  28.35  28.25    ...      3.444638  3.470811   2.930372   \n",
       "2017-08-16  27.65  28.15  28.45    ...      3.404074  3.406656  92.922049   \n",
       "2017-08-17  27.90  27.65  28.15    ...      3.413805  3.417802  19.085708   \n",
       "2017-08-28  28.15  27.90  27.75    ...      3.470232  3.419260  60.941742   \n",
       "2017-08-31  28.65  28.55  28.20    ...      3.482192  3.429063  14.842826   \n",
       "2017-09-04  28.85  28.50  28.65    ...      3.544998  3.507692   7.177266   \n",
       "2017-09-06  28.75  28.75  28.85    ...      3.560984  3.526456  11.388969   \n",
       "2017-09-11  28.45  28.60  28.60    ...      3.534110  3.483848  85.617496   \n",
       "2017-09-12  28.40  28.45  28.60    ...      3.533356  3.480221  32.076145   \n",
       "2017-09-13  28.55  28.40  28.45    ...      3.541250  3.490442  15.101237   \n",
       "2017-09-14  28.85  28.55  28.40    ...      3.547365  3.495693  11.810046   \n",
       "2017-09-19  28.70  28.75  28.85    ...      3.525355  3.458252  31.710808   \n",
       "2017-09-20  28.70  28.70  28.75    ...      3.519467  3.448640  71.997060   \n",
       "2017-09-25  29.00  28.95  29.00    ...      3.579470  3.522255   4.066479   \n",
       "2017-09-26  29.00  29.00  28.95    ...      3.573574  3.514086  69.334588   \n",
       "2017-09-27  28.75  29.00  29.00    ...      3.565100  3.501760  90.892027   \n",
       "2017-10-06  28.40  28.30  28.60    ...      3.499198  3.398304  66.291905   \n",
       "2017-10-10  29.25  29.05  28.40    ...      3.508207  3.403824  77.259550   \n",
       "2017-10-12  29.25  29.35  29.25    ...      3.517974  3.407815  40.944210   \n",
       "\n",
       "               PAYOUT  ANALYST_SENTIMENT  EPS_GRW_FY1  EPS_GRW_FY2     PE_NTM  \\\n",
       "Date                                                                            \n",
       "2016-04-20  42.461478          -1.963735    -1.073369    10.123532  11.103367   \n",
       "2016-04-25  42.785891          -2.043126    -2.152342    10.778901  11.239259   \n",
       "2016-05-06  42.584806          -1.604566    -0.976716    10.912583  10.558240   \n",
       "2016-05-09  42.550607          -1.198167    -1.345243    10.840772  10.581369   \n",
       "2016-05-13  42.710777          -1.511836    -0.992540    10.530333  10.390811   \n",
       "2016-05-16  42.691693          -1.496387    -1.121846    10.446739  10.479852   \n",
       "2016-05-19  42.630444          -1.144312    -1.416077    10.559742  10.412988   \n",
       "2016-05-20  42.661645          -1.082689    -1.405518    10.403447  10.481982   \n",
       "2016-05-31  42.515740          -1.101235    -2.046212    10.193240  11.017405   \n",
       "2016-06-14  42.510538          -1.247220    -2.229791     9.891184  10.634381   \n",
       "2016-06-15  42.502798          -1.208713    -2.097228     9.931172  10.660625   \n",
       "2016-06-27  42.507743          -0.606301    -3.404875     9.792721  10.615301   \n",
       "2016-07-06  42.746715          -0.715416    -2.327262     9.467096  10.758762   \n",
       "2016-07-07  42.755401          -0.634819    -2.313849     9.428279  10.856539   \n",
       "2016-07-18  42.800635          -0.548463    -4.001861     9.533611  11.449891   \n",
       "2016-07-21  42.684862          -0.051039    -3.870005     9.442621  11.502959   \n",
       "2016-07-27  42.783514          -0.736839    -4.238736     9.211291  11.631650   \n",
       "2016-07-29  42.749931          -0.502164    -3.894912     9.140280  11.459047   \n",
       "2016-08-08  42.853116           0.453372    -4.758081     9.113851  11.764012   \n",
       "2016-08-09  42.909473           0.332051    -4.562621     8.886384  11.746115   \n",
       "2016-08-15  42.719155           0.084260    -4.652847     8.830968  11.976580   \n",
       "2016-08-16  42.665724           0.155620    -4.982358     8.747498  11.941627   \n",
       "2016-08-18  42.541049           0.459563    -4.835706     8.579218  11.972210   \n",
       "2016-08-22  42.557820          -0.146778    -5.049368     8.793031  11.970724   \n",
       "2016-08-24  42.413307          -0.296639    -4.446546     9.050503  11.834736   \n",
       "2016-08-25  42.486310          -0.262240    -4.734493     8.938010  11.855741   \n",
       "2016-08-26  42.456250          -0.470951    -4.636828     8.667592  11.895182   \n",
       "2016-08-30  42.411226          -0.501551    -4.969687     9.233946  11.932816   \n",
       "2016-09-01  42.401615          -0.353195    -5.271282     9.328197  12.020694   \n",
       "2016-09-06  42.227494          -0.802626    -5.976187     9.105479  12.192590   \n",
       "...               ...                ...          ...          ...        ...   \n",
       "2017-06-28  41.096181           0.481106     8.968729     8.595468  11.827076   \n",
       "2017-06-30  41.126573           0.602469     8.933548     8.627647  11.881687   \n",
       "2017-07-04  41.191884           0.519743     8.953591     8.945875  11.726664   \n",
       "2017-07-05  41.199053           0.304445     8.891622     9.054457  11.785711   \n",
       "2017-07-06  41.202696           0.312739     9.478852     9.107990  11.738412   \n",
       "2017-07-17  41.211419           0.819512     9.460414     9.057971  12.112557   \n",
       "2017-07-24  41.000239           1.379623     9.543608     8.973857  12.212648   \n",
       "2017-07-27  40.944073           1.153041     9.642761     8.939044  12.335731   \n",
       "2017-08-02  40.998791           1.112476    10.724015     8.107127  12.551985   \n",
       "2017-08-03  40.993472           1.263580    10.667937     8.154486  12.494020   \n",
       "2017-08-07  40.759710           1.286746    10.368553     9.477894  12.464991   \n",
       "2017-08-11  41.754694           1.767213    11.683401     9.291095  12.025265   \n",
       "2017-08-16  41.764499           1.739282    11.122872     9.222073  12.255270   \n",
       "2017-08-17  41.747738           2.064660    11.258703     9.202595  12.209994   \n",
       "2017-08-28  42.089834           2.552754    12.596304     9.034095  12.296748   \n",
       "2017-08-31  41.920406           3.575071    13.470731     8.985164  12.212043   \n",
       "2017-09-04  41.957628           3.701359    14.165952     8.973110  11.948565   \n",
       "2017-09-06  41.931090           3.737196    14.556902     8.928424  11.877607   \n",
       "2017-09-11  41.812067           3.356299    14.367601     8.796453  11.989535   \n",
       "2017-09-12  41.805430           3.347032    14.499912     8.793700  12.000091   \n",
       "2017-09-13  41.787155           3.377684    14.629248     8.819479  11.959836   \n",
       "2017-09-14  41.731563           3.179734    14.906947     8.759065  11.925989   \n",
       "2017-09-19  41.523884           2.786476    14.072927     8.232707  11.996128   \n",
       "2017-09-20  41.537747           2.837431    14.089280     8.570956  12.033175   \n",
       "2017-09-25  41.476129           2.109530    15.204982     8.614512  11.764659   \n",
       "2017-09-26  41.459115           2.025636    14.852043     8.615487  11.787333   \n",
       "2017-09-27  41.527257           1.986992    14.427145     8.630719  11.848342   \n",
       "2017-10-06  41.440106           0.074004    12.898732     8.706145  12.183516   \n",
       "2017-10-10  41.358688          -0.152586    13.496835     8.732445  12.140123   \n",
       "2017-10-12  41.371488           0.201694    13.527734     8.739555  12.129446   \n",
       "\n",
       "               PE_LTM    C2D_LTM  \n",
       "Date                              \n",
       "2016-04-20  10.603759  67.595408  \n",
       "2016-04-25  10.675289  67.427745  \n",
       "2016-05-06  10.484934  67.499839  \n",
       "2016-05-09  10.501459  67.377066  \n",
       "2016-05-13  10.437566  67.484042  \n",
       "2016-05-16  10.478562  70.105337  \n",
       "2016-05-19  10.461944  70.146143  \n",
       "2016-05-20  10.490051  70.116864  \n",
       "2016-05-31  10.750139  70.098490  \n",
       "2016-06-14  10.628345  68.770851  \n",
       "2016-06-15  10.645222  68.812679  \n",
       "2016-06-27  10.664263  68.756887  \n",
       "2016-07-06  10.767108  68.816983  \n",
       "2016-07-07  10.823133  68.720730  \n",
       "2016-07-18  11.217064  68.479372  \n",
       "2016-07-21  11.264566  68.621841  \n",
       "2016-07-27  11.374069  68.544845  \n",
       "2016-07-29  11.281163  68.648117  \n",
       "2016-08-08  11.540564  68.644820  \n",
       "2016-08-09  11.527616  68.652588  \n",
       "2016-08-15  11.721907  74.640373  \n",
       "2016-08-16  11.703520  74.583263  \n",
       "2016-08-18  11.732709  74.616545  \n",
       "2016-08-22  11.774704  74.617148  \n",
       "2016-08-24  11.710599  74.660142  \n",
       "2016-08-25  11.727858  74.683131  \n",
       "2016-08-26  11.750312  74.630838  \n",
       "2016-08-30  11.837590  74.672598  \n",
       "2016-09-01  11.921952  74.733438  \n",
       "2016-09-06  12.079621  74.352021  \n",
       "...               ...        ...  \n",
       "2017-06-28  12.579908  73.422626  \n",
       "2017-06-30  12.608755  73.429781  \n",
       "2017-07-04  12.536163  73.416351  \n",
       "2017-07-05  12.570410  73.393528  \n",
       "2017-07-06  12.546637  73.425475  \n",
       "2017-07-17  12.757573  73.556962  \n",
       "2017-07-24  12.822361  72.604425  \n",
       "2017-07-27  12.899516  72.575859  \n",
       "2017-08-02  13.011554  72.620409  \n",
       "2017-08-03  12.980196  72.612047  \n",
       "2017-08-07  13.026673  72.627768  \n",
       "2017-08-11  12.746579  72.602445  \n",
       "2017-08-16  12.902772  71.539184  \n",
       "2017-08-17  12.874139  71.577103  \n",
       "2017-08-28  12.950307  71.600163  \n",
       "2017-08-31  12.895686  71.610520  \n",
       "2017-09-04  12.715435  70.318127  \n",
       "2017-09-06  12.664891  70.339969  \n",
       "2017-09-11  12.747040  70.346184  \n",
       "2017-09-12  12.756402  70.318433  \n",
       "2017-09-13  12.729659  70.321576  \n",
       "2017-09-14  12.702644  70.338739  \n",
       "2017-09-19  12.732913  70.330956  \n",
       "2017-09-20  12.782918  70.353419  \n",
       "2017-09-25  12.585849  70.282954  \n",
       "2017-09-26  12.604815  70.233417  \n",
       "2017-09-27  12.654949  70.223525  \n",
       "2017-10-06  12.946614  70.229726  \n",
       "2017-10-10  12.923466  70.252045  \n",
       "2017-10-12  12.920379  70.258010  \n",
       "\n",
       "[136 rows x 98 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missed.to_csv('missed_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(pred_prob, index=dates_test).to_csv('pred_prob.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
