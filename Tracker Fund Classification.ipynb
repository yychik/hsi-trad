{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction As Classification\n",
    "Continuing the 2800-HK price prediction from classification perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Modules and load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import sklearn\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Keras module\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load stored variables upon start\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pretty plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>P_L1</th>\n",
       "      <th>P_L2</th>\n",
       "      <th>P_L3</th>\n",
       "      <th>P_L4</th>\n",
       "      <th>P_L5</th>\n",
       "      <th>P_L6</th>\n",
       "      <th>P_L7</th>\n",
       "      <th>P_L8</th>\n",
       "      <th>P_L9</th>\n",
       "      <th>...</th>\n",
       "      <th>P_L11</th>\n",
       "      <th>P_L12</th>\n",
       "      <th>P_L13</th>\n",
       "      <th>P_L14</th>\n",
       "      <th>P_L15</th>\n",
       "      <th>P_L16</th>\n",
       "      <th>P_L17</th>\n",
       "      <th>P_L18</th>\n",
       "      <th>P_L19</th>\n",
       "      <th>P_L20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-08-31</th>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>23.35</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.05</td>\n",
       "      <td>21.85</td>\n",
       "      <td>...</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.85</td>\n",
       "      <td>22.15</td>\n",
       "      <td>22.25</td>\n",
       "      <td>22.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-03</th>\n",
       "      <td>24.30</td>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>23.35</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.05</td>\n",
       "      <td>...</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.85</td>\n",
       "      <td>22.15</td>\n",
       "      <td>22.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-04</th>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>23.35</td>\n",
       "      <td>22.70</td>\n",
       "      <td>...</td>\n",
       "      <td>21.85</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.85</td>\n",
       "      <td>22.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-05</th>\n",
       "      <td>24.35</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>23.35</td>\n",
       "      <td>...</td>\n",
       "      <td>22.05</td>\n",
       "      <td>21.85</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-06</th>\n",
       "      <td>24.50</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.35</td>\n",
       "      <td>23.80</td>\n",
       "      <td>23.35</td>\n",
       "      <td>23.70</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.30</td>\n",
       "      <td>...</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.05</td>\n",
       "      <td>21.85</td>\n",
       "      <td>20.80</td>\n",
       "      <td>20.95</td>\n",
       "      <td>21.70</td>\n",
       "      <td>22.35</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.05</td>\n",
       "      <td>22.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                P   P_L1   P_L2   P_L3   P_L4   P_L5   P_L6   P_L7   P_L8  \\\n",
       "Date                                                                        \n",
       "2007-08-31  24.35  23.80  23.35  23.70  23.90  23.30  23.35  22.70  22.05   \n",
       "2007-09-03  24.30  24.35  23.80  23.35  23.70  23.90  23.30  23.35  22.70   \n",
       "2007-09-04  24.30  24.30  24.35  23.80  23.35  23.70  23.90  23.30  23.35   \n",
       "2007-09-05  24.35  24.30  24.30  24.35  23.80  23.35  23.70  23.90  23.30   \n",
       "2007-09-06  24.50  24.35  24.30  24.30  24.35  23.80  23.35  23.70  23.90   \n",
       "\n",
       "             P_L9  ...    P_L11  P_L12  P_L13  P_L14  P_L15  P_L16  P_L17  \\\n",
       "Date               ...                                                      \n",
       "2007-08-31  21.85  ...    20.95  21.70  22.35  22.30  22.05  22.70  22.85   \n",
       "2007-09-03  22.05  ...    20.80  20.95  21.70  22.35  22.30  22.05  22.70   \n",
       "2007-09-04  22.70  ...    21.85  20.80  20.95  21.70  22.35  22.30  22.05   \n",
       "2007-09-05  23.35  ...    22.05  21.85  20.80  20.95  21.70  22.35  22.30   \n",
       "2007-09-06  23.30  ...    22.70  22.05  21.85  20.80  20.95  21.70  22.35   \n",
       "\n",
       "            P_L18  P_L19  P_L20  \n",
       "Date                             \n",
       "2007-08-31  22.15  22.25  22.85  \n",
       "2007-09-03  22.85  22.15  22.25  \n",
       "2007-09-04  22.70  22.85  22.15  \n",
       "2007-09-05  22.05  22.70  22.85  \n",
       "2007-09-06  22.30  22.05  22.70  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import price data\n",
    "#Load the historical prices of 2800-HK, with lags 1 to lag 20\n",
    "price_hist_data = pd.read_csv('price_only.csv', skiprows=1, parse_dates=['Date']).set_index(['Date'])\n",
    "price_hist_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Ask</th>\n",
       "      <th>Bid</th>\n",
       "      <th>20D Vol</th>\n",
       "      <th>MA5</th>\n",
       "      <th>MA15</th>\n",
       "      <th>MA12</th>\n",
       "      <th>MA20</th>\n",
       "      <th>...</th>\n",
       "      <th>DY_LTM</th>\n",
       "      <th>DY_NTM</th>\n",
       "      <th>ADV_VOL</th>\n",
       "      <th>PAYOUT</th>\n",
       "      <th>ANALYST_SENTIMENT</th>\n",
       "      <th>EPS_GRW_FY1</th>\n",
       "      <th>EPS_GRW_FY2</th>\n",
       "      <th>PE_NTM</th>\n",
       "      <th>PE_LTM</th>\n",
       "      <th>C2D_LTM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-08-31</th>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>2.367823</td>\n",
       "      <td>23.82</td>\n",
       "      <td>22.841667</td>\n",
       "      <td>22.696667</td>\n",
       "      <td>22.6225</td>\n",
       "      <td>...</td>\n",
       "      <td>2.782797</td>\n",
       "      <td>2.880128</td>\n",
       "      <td>99.419898</td>\n",
       "      <td>48.051962</td>\n",
       "      <td>1.909071</td>\n",
       "      <td>30.724490</td>\n",
       "      <td>2.717280</td>\n",
       "      <td>16.695058</td>\n",
       "      <td>16.975805</td>\n",
       "      <td>58.050474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-03</th>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>2.266743</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.120832</td>\n",
       "      <td>22.830000</td>\n",
       "      <td>22.7250</td>\n",
       "      <td>...</td>\n",
       "      <td>2.784563</td>\n",
       "      <td>2.892308</td>\n",
       "      <td>11.570023</td>\n",
       "      <td>48.305678</td>\n",
       "      <td>1.835359</td>\n",
       "      <td>31.005439</td>\n",
       "      <td>1.876752</td>\n",
       "      <td>16.709085</td>\n",
       "      <td>16.925571</td>\n",
       "      <td>58.154835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-04</th>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>24.30</td>\n",
       "      <td>2.259649</td>\n",
       "      <td>24.02</td>\n",
       "      <td>23.412500</td>\n",
       "      <td>22.960000</td>\n",
       "      <td>22.8325</td>\n",
       "      <td>...</td>\n",
       "      <td>2.784339</td>\n",
       "      <td>2.894377</td>\n",
       "      <td>69.555725</td>\n",
       "      <td>48.307102</td>\n",
       "      <td>1.725886</td>\n",
       "      <td>31.040968</td>\n",
       "      <td>1.891823</td>\n",
       "      <td>16.697662</td>\n",
       "      <td>16.919413</td>\n",
       "      <td>58.177640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-05</th>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>24.35</td>\n",
       "      <td>2.172140</td>\n",
       "      <td>24.22</td>\n",
       "      <td>23.620832</td>\n",
       "      <td>23.136667</td>\n",
       "      <td>22.9075</td>\n",
       "      <td>...</td>\n",
       "      <td>2.767348</td>\n",
       "      <td>2.867314</td>\n",
       "      <td>24.265623</td>\n",
       "      <td>48.267351</td>\n",
       "      <td>1.741908</td>\n",
       "      <td>30.716929</td>\n",
       "      <td>1.878484</td>\n",
       "      <td>16.841225</td>\n",
       "      <td>17.017692</td>\n",
       "      <td>58.195446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-06</th>\n",
       "      <td>24.50</td>\n",
       "      <td>24.50</td>\n",
       "      <td>24.50</td>\n",
       "      <td>24.50</td>\n",
       "      <td>24.50</td>\n",
       "      <td>2.160638</td>\n",
       "      <td>24.36</td>\n",
       "      <td>23.825000</td>\n",
       "      <td>23.373333</td>\n",
       "      <td>22.9975</td>\n",
       "      <td>...</td>\n",
       "      <td>2.775339</td>\n",
       "      <td>2.878880</td>\n",
       "      <td>88.845002</td>\n",
       "      <td>48.288397</td>\n",
       "      <td>1.696151</td>\n",
       "      <td>31.336520</td>\n",
       "      <td>1.861943</td>\n",
       "      <td>16.780660</td>\n",
       "      <td>16.975485</td>\n",
       "      <td>58.224743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Close   High    Low    Ask    Bid   20D Vol    MA5       MA15  \\\n",
       "Date                                                                        \n",
       "2007-08-31  24.35  24.35  24.35  24.35  24.35  2.367823  23.82  22.841667   \n",
       "2007-09-03  24.30  24.30  24.30  24.30  24.30  2.266743  23.90  23.120832   \n",
       "2007-09-04  24.30  24.30  24.30  24.30  24.30  2.259649  24.02  23.412500   \n",
       "2007-09-05  24.35  24.35  24.35  24.35  24.35  2.172140  24.22  23.620832   \n",
       "2007-09-06  24.50  24.50  24.50  24.50  24.50  2.160638  24.36  23.825000   \n",
       "\n",
       "                 MA12     MA20    ...        DY_LTM    DY_NTM    ADV_VOL  \\\n",
       "Date                              ...                                      \n",
       "2007-08-31  22.696667  22.6225    ...      2.782797  2.880128  99.419898   \n",
       "2007-09-03  22.830000  22.7250    ...      2.784563  2.892308  11.570023   \n",
       "2007-09-04  22.960000  22.8325    ...      2.784339  2.894377  69.555725   \n",
       "2007-09-05  23.136667  22.9075    ...      2.767348  2.867314  24.265623   \n",
       "2007-09-06  23.373333  22.9975    ...      2.775339  2.878880  88.845002   \n",
       "\n",
       "               PAYOUT  ANALYST_SENTIMENT  EPS_GRW_FY1  EPS_GRW_FY2     PE_NTM  \\\n",
       "Date                                                                            \n",
       "2007-08-31  48.051962           1.909071    30.724490     2.717280  16.695058   \n",
       "2007-09-03  48.305678           1.835359    31.005439     1.876752  16.709085   \n",
       "2007-09-04  48.307102           1.725886    31.040968     1.891823  16.697662   \n",
       "2007-09-05  48.267351           1.741908    30.716929     1.878484  16.841225   \n",
       "2007-09-06  48.288397           1.696151    31.336520     1.861943  16.780660   \n",
       "\n",
       "               PE_LTM    C2D_LTM  \n",
       "Date                              \n",
       "2007-08-31  16.975805  58.050474  \n",
       "2007-09-03  16.925571  58.154835  \n",
       "2007-09-04  16.919413  58.177640  \n",
       "2007-09-05  17.017692  58.195446  \n",
       "2007-09-06  16.975485  58.224743  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Fundamentals Data\n",
    "fund_data = pd.read_csv('new_index_data.csv', skiprows=1, parse_dates=['Date']).set_index(['Date'])\n",
    "fund_data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hang Seng Index</th>\n",
       "      <th>SSE Composite Index</th>\n",
       "      <th>ASX All Ordinaries</th>\n",
       "      <th>India S&amp;P BSE SENSEX</th>\n",
       "      <th>TOPIX</th>\n",
       "      <th>KOSPI Composite Index</th>\n",
       "      <th>Taiwan TAIEX</th>\n",
       "      <th>FTSE Bursa Malaysia KLCI</th>\n",
       "      <th>FTSE Straits Times Index</th>\n",
       "      <th>Philippines PSE PSEi</th>\n",
       "      <th>...</th>\n",
       "      <th>Turkey BIST 100</th>\n",
       "      <th>S&amp;P 500</th>\n",
       "      <th>DJ Industrial Average</th>\n",
       "      <th>Colombia IGBC</th>\n",
       "      <th>Canada S&amp;P/TSX Composite</th>\n",
       "      <th>Brazil Bovespa Index</th>\n",
       "      <th>Mexico IPC</th>\n",
       "      <th>Israel TA-125</th>\n",
       "      <th>Saudi Arabia All Share (TASI)</th>\n",
       "      <th>FTSE JSE All Share</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-08-31</th>\n",
       "      <td>23984.14</td>\n",
       "      <td>5218.825</td>\n",
       "      <td>6248.3</td>\n",
       "      <td>15318.60</td>\n",
       "      <td>1608.25</td>\n",
       "      <td>1873.24</td>\n",
       "      <td>8982.16</td>\n",
       "      <td>1273.93</td>\n",
       "      <td>3328.43</td>\n",
       "      <td>3365.29</td>\n",
       "      <td>...</td>\n",
       "      <td>50198.60</td>\n",
       "      <td>1473.99</td>\n",
       "      <td>13357.74</td>\n",
       "      <td>10728.74</td>\n",
       "      <td>13660.48</td>\n",
       "      <td>54637.24</td>\n",
       "      <td>30347.86</td>\n",
       "      <td>1034.67</td>\n",
       "      <td>8226.97</td>\n",
       "      <td>28660.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-03</th>\n",
       "      <td>23904.09</td>\n",
       "      <td>5321.055</td>\n",
       "      <td>6272.5</td>\n",
       "      <td>15422.05</td>\n",
       "      <td>1605.44</td>\n",
       "      <td>1881.81</td>\n",
       "      <td>8979.96</td>\n",
       "      <td>1284.14</td>\n",
       "      <td>3321.36</td>\n",
       "      <td>3369.14</td>\n",
       "      <td>...</td>\n",
       "      <td>49936.94</td>\n",
       "      <td>1473.99</td>\n",
       "      <td>13357.74</td>\n",
       "      <td>10750.79</td>\n",
       "      <td>13660.48</td>\n",
       "      <td>54832.51</td>\n",
       "      <td>30797.60</td>\n",
       "      <td>1047.33</td>\n",
       "      <td>8017.54</td>\n",
       "      <td>28887.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-04</th>\n",
       "      <td>23886.07</td>\n",
       "      <td>5294.045</td>\n",
       "      <td>6297.1</td>\n",
       "      <td>15465.40</td>\n",
       "      <td>1596.74</td>\n",
       "      <td>1874.74</td>\n",
       "      <td>8922.98</td>\n",
       "      <td>1283.75</td>\n",
       "      <td>3308.81</td>\n",
       "      <td>3312.30</td>\n",
       "      <td>...</td>\n",
       "      <td>50032.59</td>\n",
       "      <td>1489.42</td>\n",
       "      <td>13448.86</td>\n",
       "      <td>10880.85</td>\n",
       "      <td>13755.23</td>\n",
       "      <td>55250.47</td>\n",
       "      <td>30932.71</td>\n",
       "      <td>1054.69</td>\n",
       "      <td>7878.70</td>\n",
       "      <td>29051.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-05</th>\n",
       "      <td>24069.17</td>\n",
       "      <td>5310.716</td>\n",
       "      <td>6274.3</td>\n",
       "      <td>15446.15</td>\n",
       "      <td>1569.47</td>\n",
       "      <td>1865.59</td>\n",
       "      <td>8913.85</td>\n",
       "      <td>1297.93</td>\n",
       "      <td>3375.02</td>\n",
       "      <td>3342.35</td>\n",
       "      <td>...</td>\n",
       "      <td>49421.38</td>\n",
       "      <td>1472.29</td>\n",
       "      <td>13305.47</td>\n",
       "      <td>10819.91</td>\n",
       "      <td>13683.28</td>\n",
       "      <td>54407.83</td>\n",
       "      <td>30809.55</td>\n",
       "      <td>1048.70</td>\n",
       "      <td>7853.66</td>\n",
       "      <td>28696.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-06</th>\n",
       "      <td>24050.40</td>\n",
       "      <td>5393.660</td>\n",
       "      <td>6265.3</td>\n",
       "      <td>15616.31</td>\n",
       "      <td>1568.52</td>\n",
       "      <td>1888.81</td>\n",
       "      <td>9017.08</td>\n",
       "      <td>1298.85</td>\n",
       "      <td>3399.49</td>\n",
       "      <td>3326.53</td>\n",
       "      <td>...</td>\n",
       "      <td>49601.39</td>\n",
       "      <td>1478.55</td>\n",
       "      <td>13363.35</td>\n",
       "      <td>10844.40</td>\n",
       "      <td>13795.69</td>\n",
       "      <td>54569.00</td>\n",
       "      <td>30816.95</td>\n",
       "      <td>1033.23</td>\n",
       "      <td>7853.66</td>\n",
       "      <td>28850.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Hang Seng Index  SSE Composite Index  ASX All Ordinaries  \\\n",
       "Date                                                                   \n",
       "2007-08-31         23984.14             5218.825              6248.3   \n",
       "2007-09-03         23904.09             5321.055              6272.5   \n",
       "2007-09-04         23886.07             5294.045              6297.1   \n",
       "2007-09-05         24069.17             5310.716              6274.3   \n",
       "2007-09-06         24050.40             5393.660              6265.3   \n",
       "\n",
       "            India S&P BSE SENSEX    TOPIX  KOSPI Composite Index  \\\n",
       "Date                                                               \n",
       "2007-08-31              15318.60  1608.25                1873.24   \n",
       "2007-09-03              15422.05  1605.44                1881.81   \n",
       "2007-09-04              15465.40  1596.74                1874.74   \n",
       "2007-09-05              15446.15  1569.47                1865.59   \n",
       "2007-09-06              15616.31  1568.52                1888.81   \n",
       "\n",
       "            Taiwan TAIEX  FTSE Bursa Malaysia KLCI  FTSE Straits Times Index  \\\n",
       "Date                                                                           \n",
       "2007-08-31       8982.16                   1273.93                   3328.43   \n",
       "2007-09-03       8979.96                   1284.14                   3321.36   \n",
       "2007-09-04       8922.98                   1283.75                   3308.81   \n",
       "2007-09-05       8913.85                   1297.93                   3375.02   \n",
       "2007-09-06       9017.08                   1298.85                   3399.49   \n",
       "\n",
       "            Philippines PSE PSEi         ...          Turkey BIST 100  \\\n",
       "Date                                     ...                            \n",
       "2007-08-31               3365.29         ...                 50198.60   \n",
       "2007-09-03               3369.14         ...                 49936.94   \n",
       "2007-09-04               3312.30         ...                 50032.59   \n",
       "2007-09-05               3342.35         ...                 49421.38   \n",
       "2007-09-06               3326.53         ...                 49601.39   \n",
       "\n",
       "            S&P 500  DJ Industrial Average  Colombia IGBC  \\\n",
       "Date                                                        \n",
       "2007-08-31  1473.99               13357.74       10728.74   \n",
       "2007-09-03  1473.99               13357.74       10750.79   \n",
       "2007-09-04  1489.42               13448.86       10880.85   \n",
       "2007-09-05  1472.29               13305.47       10819.91   \n",
       "2007-09-06  1478.55               13363.35       10844.40   \n",
       "\n",
       "            Canada S&P/TSX Composite  Brazil Bovespa Index  Mexico IPC  \\\n",
       "Date                                                                     \n",
       "2007-08-31                  13660.48              54637.24    30347.86   \n",
       "2007-09-03                  13660.48              54832.51    30797.60   \n",
       "2007-09-04                  13755.23              55250.47    30932.71   \n",
       "2007-09-05                  13683.28              54407.83    30809.55   \n",
       "2007-09-06                  13795.69              54569.00    30816.95   \n",
       "\n",
       "            Israel TA-125  Saudi Arabia All Share (TASI)  FTSE JSE All Share  \n",
       "Date                                                                          \n",
       "2007-08-31        1034.67                        8226.97            28660.35  \n",
       "2007-09-03        1047.33                        8017.54            28887.48  \n",
       "2007-09-04        1054.69                        7878.70            29051.96  \n",
       "2007-09-05        1048.70                        7853.66            28696.67  \n",
       "2007-09-06        1033.23                        7853.66            28850.19  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Global index data\n",
    "idx_data = pd.read_csv('indices.csv', skiprows=1, parse_dates=['Date']).set_index(['Date'])\n",
    "idx_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pre-process Raw Data\n",
    "1. Generate labels\n",
    "2. Apply lags to global index data\n",
    "3. Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate Labels from Price History Data\n",
    "#Generate UP/DOWN labels from log change\n",
    "cutoff_perc = 0.0005 #0.05% return as cuttoff to define UP label\n",
    "lag = 1 #forward returns\n",
    "\n",
    "labels = np.zeros([price_hist_data.shape[0]])\n",
    "\n",
    "#Caluclate log-returns\n",
    "ret = np.log(price_hist_data['P'].shift(-lag)/price_hist_data['P'])\n",
    "labels = [1 if r > cutoff_perc else 0 for r in ret]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Applying lags to index data\n",
    "#Seperate the indices into 2 classes - lag or no_lag\n",
    "no_lag = [0, 1, 2, 4, 5, 6, 9, 10]\n",
    "lag = [i for i in range(0,idx_data.shape[1]) if i not in no_lag]\n",
    "\n",
    "#Processing the dataset by applying appropriate lags\n",
    "lagged_data = idx_data.iloc[:,lag].shift(1)\n",
    "idx_data = pd.concat([idx_data.iloc[:,no_lag], lagged_data], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove first row\n",
    "idx_data = idx_data.iloc[1:, :]\n",
    "price_hist_data = price_hist_data.iloc[1:, :]\n",
    "fund_data = fund_data.iloc[1:, :]\n",
    "labels=labels[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of idx_data:  (2483, 42)\n",
      "Shape of price_hist_data:  (2483, 21)\n",
      "Shape of fund_data:  (2483, 30)\n",
      "Shape of labels:  2483\n"
     ]
    }
   ],
   "source": [
    "#Check Dimensions to make sure everythings right before continuing..\n",
    "print(\"Shape of idx_data: \", idx_data.shape)\n",
    "print(\"Shape of price_hist_data: \", price_hist_data.shape)\n",
    "print(\"Shape of fund_data: \", fund_data.shape)\n",
    "print(\"Shape of labels: \", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training data\n",
    "X = np.array(pd.concat([price_hist_data, idx_data, fund_data], axis=1))\n",
    "y = to_categorical(labels, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking whether there are NAs\n",
    "[np.sum(np.isnan(X), axis=0) > 0] == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Split training, validation and test set\n",
    "\n",
    "In this stage we have a look-ahead bias free set of data (X) and the labels y. Next, we will need to:\n",
    "- Normalize the input data. To avoid look ahead bias, we will z-score the features, using ONLY the training set.\n",
    "- Next, generate input data into LSTM network. We will need an overlapping sequence at 1-day window as input samples. Specifically suppose the *timestep* is 240, we will have a list of array consists of *number of rows of X* - 240 entries, each element has dimentions (240, num_of_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to split raw data into training, validation and test set, returns a numpy array.\n",
    "def split_data(input_data=[], train_size=0.8, val_size=0.2, test_size=0):\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    #PARAM: input_data: numpy nd array\n",
    "    #PARAM: training_size: size of training set in decimal\n",
    "    #PARAM: val_size: size of validation set in decimal\n",
    "    #PARAM: test_size: size of test set in decimal\n",
    "    #OUTPUT: tuple (train_set, validation_set, test_set)\n",
    "    #------------------------------------------------\n",
    "    \n",
    "    #First check whether traing_size + val_size + test_size = 1 and each of the entries are positive\n",
    "    assert(train_size + val_size + test_size==1), \"Sum of training, validation and test size needs to be 1!\"\n",
    "    assert(train_size * val_size * test_size > 0), \"Sizes have to be positive!\"\n",
    "    \n",
    "    #Check input_data type is numpy array, after casting\n",
    "    if type(input_data) != 'numpy.ndarray':\n",
    "        input_data = np.array(input_data) \n",
    "    \n",
    "    assert(isinstance(input_data, np.ndarray)), \"Input has to be a numpy array!\"\n",
    "    \n",
    "    \n",
    "    #Calculate cut-off points\n",
    "    train_cut_index = int(train_size * input_data.shape[0])\n",
    "    val_cut_index = train_cut_index + int(val_size * input_data.shape[0])\n",
    "    \n",
    "    #Split the data\n",
    "    if len(input_data.shape) == 1:\n",
    "        train, val, test = input_data[:train_cut_index], input_data[train_cut_index:val_cut_index], input_data[val_cut_index:]\n",
    "    else:\n",
    "        train, val, test = input_data[:train_cut_index,:], input_data[train_cut_index:val_cut_index, :], input_data[val_cut_index:, :]\n",
    "    \n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1489, 93)\n",
      "(496, 93)\n",
      "(498, 93)\n",
      "(1489, 2)\n",
      "(496, 2)\n",
      "(498, 2)\n"
     ]
    }
   ],
   "source": [
    "#------------\n",
    "#TEST OUTPUT\n",
    "#------------\n",
    "#Split data\n",
    "X_train, X_val, X_test = split_data(X, train_size=0.6, val_size=0.2, test_size=0.2)\n",
    "y_train, y_val, y_test = split_data(y, train_size=0.6, val_size=0.2, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to re-structure the data to get batches. Re-shape the data to have overlapping training set for time-series learning.\n",
    "def get_inputs(input_data, labels, batch_size, timesteps):\n",
    "    \n",
    "    #First get the total number of samples generated\n",
    "    n_seq = input_data.shape[0] - timesteps + 1\n",
    "    \n",
    "    #features, classes\n",
    "    n_dim = input_data.shape[1]\n",
    "    n_class = labels.shape[1]\n",
    "    \n",
    "    #Calculate the number of batches possible\n",
    "    n_samples = n_seq * timesteps\n",
    "    n_batches = n_samples // (batch_size * timesteps)\n",
    "\n",
    "    output = []\n",
    "    targets = []\n",
    "        \n",
    "    for jj in range(0, n_batches):\n",
    "    #Generate the sequences\n",
    "        \n",
    "        #if jj == n_batches:\n",
    "            \n",
    "            #output.append([input_data[jj*batch_size:, :]])\n",
    "            #targets.append([labels[jj*batch_size:, :]])            \n",
    "            #yield np.vstack(output), np.vstack(targets  )\n",
    "            \n",
    "        #else:            \n",
    "        for ii in range(jj * batch_size, (jj + 1) * batch_size):                    \n",
    "            #Getting the overlapping samples\n",
    "            output.append([scale(input_data[ii:ii + timesteps, :])])\n",
    "            targets.append([labels[ii:ii+timesteps, :]])\n",
    "\n",
    "    return np.vstack(output), np.vstack(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 4, 5)\n",
      "(6, 4, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/carnd/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/carnd/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/carnd/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/carnd/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/carnd/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#------------\n",
    "#TEST OUTPUT\n",
    "#------------\n",
    "#get inputs\n",
    "t = np.reshape(np.arange(1,51), (10,5))\n",
    "s = np.reshape(np.arange(1,21), (10,2))\n",
    "\n",
    "#for (x, y) in get_inputs(t,s, 3, 4):\n",
    "#    print('training size: ', x.shape)\n",
    "#    print('training: \\n', x)\n",
    "#    print('label size: ', y.shape)\n",
    "#    print('labels: \\n', y)\n",
    "\n",
    "test_train, test_label = get_inputs(t,s,3,4)\n",
    "    \n",
    "\n",
    "#print(test_train[0:1])\n",
    "#print(test_label[0:1])\n",
    "#print(t)\n",
    "print(test_train.shape)\n",
    "print(test_label.shape)\n",
    "#print(test_train)\n",
    "#test_train.reshape((-1, 4, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define Parameters\n",
    "learning_rate = 0.0005\n",
    "epochs= 100\n",
    "loss = 'binary_crossentropy'\n",
    "batch_size = 256\n",
    "timesteps = 22\n",
    "n_dim = X.shape[1]\n",
    "n_classes = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Optimizer - using Adam Optimizer\n",
    "optimizer = optimizers.Adam(lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate Inputs\n",
    "X_train_1, y_train_1 = get_inputs(X_train, y_train, batch_size, timesteps)\n",
    "X_val_1, y_val_1 = get_inputs(X_val, y_val, batch_size, timesteps)\n",
    "X_test_1, y_test_1 = get_inputs(X_test, y_test, batch_size, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define LSTM Network object\n",
    "def build_network(n_hidden_layer, dropout, input_shape, batch_size, return_sequences=True, stateful=True):\n",
    "    \n",
    "    #Define network architect\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_hidden_layer[0], input_shape=input_shape, batch_size=batch_size, return_sequences=return_sequences, stateful=stateful))\n",
    "    model.add(Dropout(dropout[0]))\n",
    "    model.add(LSTM(n_hidden_layer[1], return_sequences=return_sequences, stateful=stateful))\n",
    "    model.add(Dropout(dropout[0]))\n",
    "    model.add(Dense(n_hidden_layer[2], activation='relu'))\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lstm1 = build_network([5,5,10], [0.1,0.1], input_shape=(timesteps, n_dim), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compile\n",
    "#lstm1.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fitted = lstm1.fit(X_train_1, y_train_1, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(X_val_1, y_val_1), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 5: Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Plot Training vs Validation Curve\n",
    "def plot_loss(fitted):\n",
    "    plt.figure()\n",
    "    plt.plot(fitted.history['loss'])\n",
    "    plt.plot(fitted.history['val_loss'])\n",
    "    plt.title('Training Loss & Validation Loss')\n",
    "    plt.ylabel('Binary Cross Entropy Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training Loss', 'Validation Loss'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Plot Training vs Validation Accuracy\n",
    "def plot_accuracy(fitted):\n",
    "    plt.figure()\n",
    "    plt.plot(fitted.history['acc'])\n",
    "    plt.plot(fitted.history['val_acc'])\n",
    "    plt.title('Training Accuracy & Validation Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 6: Parameter Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search the following parameters:\n",
    "- Learning Rate & decay rate\n",
    "- Number of Hidden Layers\n",
    "- Dropout Rate\n",
    "- Timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "best_drop_out             -> (0.25, 0.25)\n",
      "best_lr                   -> 4.9999999999999996e-05\n",
      "best_n_hidden             -> (45, 25, 50)\n",
      "best_timestep             -> 17\n",
      "do_loss                   -> [[0.69356894493103027], [0.68898439407348633], [0.\n",
      "hd_loss                   -> [[0.69319874048233032], [0.69006013870239258], [0.\n",
      "lr_loss                   -> [[0.68081382910410559], [0.67485751708348596], [0.\n",
      "t_loss                    -> [[0.68911886215209961], [0.69271934032440186], [0.\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Grid Search for learning rate, at fixed 100 epochs.\n",
    "#lr = np.linspace(1e-5, 1e-3, 1)\n",
    "lr = np.linspace(1e-5, 1e-3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initiate empty list to store best validation loss for a particular learning rate\n",
    "lr_loss = []\n",
    "steps = 1\n",
    "\n",
    "#Build network\n",
    "lr_network = build_network([5,5,10], [0,0], input_shape=(timesteps, n_dim), batch_size=batch_size)\n",
    "\n",
    "#Loop through the learning rate and train network\n",
    "for learning_rate in lr:\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optimizers.Adam(lr=learning_rate)\n",
    "    \n",
    "    #Compile\n",
    "    lr_network.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    #Train\n",
    "    fitted_lr = lr_network.fit(X_train_1, y_train_1, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=(X_val_1, y_val_1), shuffle=True)\n",
    "    \n",
    "    #Display\n",
    "    print('Steps: %d, learning rate: %.6f, Best Validation Loss: %.4f' %(steps, learning_rate, np.min(fitted_lr.history['val_loss'])))\n",
    "    \n",
    "    #Save\n",
    "    lr_loss.append([np.min(fitted_lr.history['val_loss'])])\n",
    "    \n",
    "    #Update\n",
    "    steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Best Learning Rate\n",
    "best_lr = lr[np.argmin(lr_loss)]\n",
    "best_lr_loss = np.min(lr_loss)\n",
    "print('Best Learning Rate: %.5f' % best_lr)\n",
    "print('Validation Loss: %.5f' % np.argmin(lr_loss))\n",
    "%store best_lr\n",
    "%store lr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot Learning Rate vs Validation Loss\n",
    "plt.plot(lr*1e4, lr_loss)\n",
    "plt.title('Learning Rate vs Validation Loss')\n",
    "plt.xlabel('Learning Rate (x 10^-4)')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate number of layers\n",
    "n_hidden_layers = np.arange(5,51,10)\n",
    "n_dense = np.arange(10,51,10)\n",
    "n_layers = list(itertools.product(n_hidden_layers, n_hidden_layers, n_dense))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1, Number of layers: (5, 5, 10),  Best Validation Loss: 0.6932\n",
      "Steps: 2, Number of layers: (5, 5, 20),  Best Validation Loss: 0.6901\n",
      "Steps: 3, Number of layers: (5, 5, 30),  Best Validation Loss: 0.6933\n",
      "Steps: 4, Number of layers: (5, 5, 40),  Best Validation Loss: 0.6939\n",
      "Steps: 5, Number of layers: (5, 5, 50),  Best Validation Loss: 0.6943\n",
      "Steps: 6, Number of layers: (5, 15, 10),  Best Validation Loss: 0.6943\n",
      "Steps: 7, Number of layers: (5, 15, 20),  Best Validation Loss: 0.6942\n",
      "Steps: 8, Number of layers: (5, 15, 30),  Best Validation Loss: 0.6915\n",
      "Steps: 9, Number of layers: (5, 15, 40),  Best Validation Loss: 0.6943\n",
      "Steps: 10, Number of layers: (5, 15, 50),  Best Validation Loss: 0.6924\n",
      "Steps: 11, Number of layers: (5, 25, 10),  Best Validation Loss: 0.6888\n",
      "Steps: 12, Number of layers: (5, 25, 20),  Best Validation Loss: 0.6918\n",
      "Steps: 13, Number of layers: (5, 25, 30),  Best Validation Loss: 0.6943\n",
      "Steps: 14, Number of layers: (5, 25, 40),  Best Validation Loss: 0.6900\n",
      "Steps: 15, Number of layers: (5, 25, 50),  Best Validation Loss: 0.6950\n",
      "Steps: 16, Number of layers: (5, 35, 10),  Best Validation Loss: 0.6924\n",
      "Steps: 17, Number of layers: (5, 35, 20),  Best Validation Loss: 0.6921\n",
      "Steps: 18, Number of layers: (5, 35, 30),  Best Validation Loss: 0.6903\n",
      "Steps: 19, Number of layers: (5, 35, 40),  Best Validation Loss: 0.6894\n",
      "Steps: 20, Number of layers: (5, 35, 50),  Best Validation Loss: 0.6891\n",
      "Steps: 21, Number of layers: (5, 45, 10),  Best Validation Loss: 0.6904\n",
      "Steps: 22, Number of layers: (5, 45, 20),  Best Validation Loss: 0.6912\n",
      "Steps: 23, Number of layers: (5, 45, 30),  Best Validation Loss: 0.6933\n",
      "Steps: 24, Number of layers: (5, 45, 40),  Best Validation Loss: 0.6884\n",
      "Steps: 25, Number of layers: (5, 45, 50),  Best Validation Loss: 0.6890\n",
      "Steps: 26, Number of layers: (15, 5, 10),  Best Validation Loss: 0.6925\n",
      "Steps: 27, Number of layers: (15, 5, 20),  Best Validation Loss: 0.6912\n",
      "Steps: 28, Number of layers: (15, 5, 30),  Best Validation Loss: 0.6884\n",
      "Steps: 29, Number of layers: (15, 5, 40),  Best Validation Loss: 0.6871\n",
      "Steps: 30, Number of layers: (15, 5, 50),  Best Validation Loss: 0.6887\n",
      "Steps: 31, Number of layers: (15, 15, 10),  Best Validation Loss: 0.6870\n",
      "Steps: 32, Number of layers: (15, 15, 20),  Best Validation Loss: 0.6909\n",
      "Steps: 33, Number of layers: (15, 15, 30),  Best Validation Loss: 0.6896\n",
      "Steps: 34, Number of layers: (15, 15, 40),  Best Validation Loss: 0.6887\n",
      "Steps: 35, Number of layers: (15, 15, 50),  Best Validation Loss: 0.6933\n",
      "Steps: 36, Number of layers: (15, 25, 10),  Best Validation Loss: 0.6935\n",
      "Steps: 37, Number of layers: (15, 25, 20),  Best Validation Loss: 0.6914\n",
      "Steps: 38, Number of layers: (15, 25, 30),  Best Validation Loss: 0.6929\n",
      "Steps: 39, Number of layers: (15, 25, 40),  Best Validation Loss: 0.6888\n",
      "Steps: 40, Number of layers: (15, 25, 50),  Best Validation Loss: 0.6936\n",
      "Steps: 41, Number of layers: (15, 35, 10),  Best Validation Loss: 0.6850\n",
      "Steps: 42, Number of layers: (15, 35, 20),  Best Validation Loss: 0.6871\n",
      "Steps: 43, Number of layers: (15, 35, 30),  Best Validation Loss: 0.6872\n",
      "Steps: 44, Number of layers: (15, 35, 40),  Best Validation Loss: 0.6894\n",
      "Steps: 45, Number of layers: (15, 35, 50),  Best Validation Loss: 0.6859\n",
      "Steps: 46, Number of layers: (15, 45, 10),  Best Validation Loss: 0.6884\n",
      "Steps: 47, Number of layers: (15, 45, 20),  Best Validation Loss: 0.6934\n",
      "Steps: 48, Number of layers: (15, 45, 30),  Best Validation Loss: 0.6921\n",
      "Steps: 49, Number of layers: (15, 45, 40),  Best Validation Loss: 0.6887\n",
      "Steps: 50, Number of layers: (15, 45, 50),  Best Validation Loss: 0.6861\n",
      "Steps: 51, Number of layers: (25, 5, 10),  Best Validation Loss: 0.6925\n",
      "Steps: 52, Number of layers: (25, 5, 20),  Best Validation Loss: 0.6940\n",
      "Steps: 53, Number of layers: (25, 5, 30),  Best Validation Loss: 0.6938\n",
      "Steps: 54, Number of layers: (25, 5, 40),  Best Validation Loss: 0.6893\n",
      "Steps: 55, Number of layers: (25, 5, 50),  Best Validation Loss: 0.6845\n",
      "Steps: 56, Number of layers: (25, 15, 10),  Best Validation Loss: 0.6870\n",
      "Steps: 57, Number of layers: (25, 15, 20),  Best Validation Loss: 0.6912\n",
      "Steps: 58, Number of layers: (25, 15, 30),  Best Validation Loss: 0.6910\n",
      "Steps: 59, Number of layers: (25, 15, 40),  Best Validation Loss: 0.6911\n",
      "Steps: 60, Number of layers: (25, 15, 50),  Best Validation Loss: 0.6801\n",
      "Steps: 61, Number of layers: (25, 25, 10),  Best Validation Loss: 0.6916\n",
      "Steps: 62, Number of layers: (25, 25, 20),  Best Validation Loss: 0.6888\n",
      "Steps: 63, Number of layers: (25, 25, 30),  Best Validation Loss: 0.6901\n",
      "Steps: 64, Number of layers: (25, 25, 40),  Best Validation Loss: 0.6896\n",
      "Steps: 65, Number of layers: (25, 25, 50),  Best Validation Loss: 0.6898\n",
      "Steps: 66, Number of layers: (25, 35, 10),  Best Validation Loss: 0.6907\n",
      "Steps: 67, Number of layers: (25, 35, 20),  Best Validation Loss: 0.6918\n",
      "Steps: 68, Number of layers: (25, 35, 30),  Best Validation Loss: 0.6899\n",
      "Steps: 69, Number of layers: (25, 35, 40),  Best Validation Loss: 0.6893\n",
      "Steps: 70, Number of layers: (25, 35, 50),  Best Validation Loss: 0.6832\n",
      "Steps: 71, Number of layers: (25, 45, 10),  Best Validation Loss: 0.6873\n",
      "Steps: 72, Number of layers: (25, 45, 20),  Best Validation Loss: 0.6849\n",
      "Steps: 73, Number of layers: (25, 45, 30),  Best Validation Loss: 0.6844\n",
      "Steps: 74, Number of layers: (25, 45, 40),  Best Validation Loss: 0.6882\n",
      "Steps: 75, Number of layers: (25, 45, 50),  Best Validation Loss: 0.6880\n",
      "Steps: 76, Number of layers: (35, 5, 10),  Best Validation Loss: 0.6919\n",
      "Steps: 77, Number of layers: (35, 5, 20),  Best Validation Loss: 0.6880\n",
      "Steps: 78, Number of layers: (35, 5, 30),  Best Validation Loss: 0.6903\n",
      "Steps: 79, Number of layers: (35, 5, 40),  Best Validation Loss: 0.6911\n",
      "Steps: 80, Number of layers: (35, 5, 50),  Best Validation Loss: 0.6917\n",
      "Steps: 81, Number of layers: (35, 15, 10),  Best Validation Loss: 0.6878\n",
      "Steps: 82, Number of layers: (35, 15, 20),  Best Validation Loss: 0.6886\n",
      "Steps: 83, Number of layers: (35, 15, 30),  Best Validation Loss: 0.6810\n",
      "Steps: 84, Number of layers: (35, 15, 40),  Best Validation Loss: 0.6814\n",
      "Steps: 85, Number of layers: (35, 15, 50),  Best Validation Loss: 0.6888\n",
      "Steps: 86, Number of layers: (35, 25, 10),  Best Validation Loss: 0.6907\n",
      "Steps: 87, Number of layers: (35, 25, 20),  Best Validation Loss: 0.6866\n",
      "Steps: 88, Number of layers: (35, 25, 30),  Best Validation Loss: 0.6821\n",
      "Steps: 89, Number of layers: (35, 25, 40),  Best Validation Loss: 0.6849\n",
      "Steps: 90, Number of layers: (35, 25, 50),  Best Validation Loss: 0.6906\n",
      "Steps: 91, Number of layers: (35, 35, 10),  Best Validation Loss: 0.6905\n",
      "Steps: 92, Number of layers: (35, 35, 20),  Best Validation Loss: 0.6845\n",
      "Steps: 93, Number of layers: (35, 35, 30),  Best Validation Loss: 0.6843\n",
      "Steps: 94, Number of layers: (35, 35, 40),  Best Validation Loss: 0.6897\n",
      "Steps: 95, Number of layers: (35, 35, 50),  Best Validation Loss: 0.6824\n",
      "Steps: 96, Number of layers: (35, 45, 10),  Best Validation Loss: 0.6844\n",
      "Steps: 97, Number of layers: (35, 45, 20),  Best Validation Loss: 0.6856\n",
      "Steps: 98, Number of layers: (35, 45, 30),  Best Validation Loss: 0.6934\n",
      "Steps: 99, Number of layers: (35, 45, 40),  Best Validation Loss: 0.6896\n",
      "Steps: 100, Number of layers: (35, 45, 50),  Best Validation Loss: 0.6883\n",
      "Steps: 101, Number of layers: (45, 5, 10),  Best Validation Loss: 0.6950\n",
      "Steps: 102, Number of layers: (45, 5, 20),  Best Validation Loss: 0.6900\n",
      "Steps: 103, Number of layers: (45, 5, 30),  Best Validation Loss: 0.6942\n",
      "Steps: 104, Number of layers: (45, 5, 40),  Best Validation Loss: 0.6890\n",
      "Steps: 105, Number of layers: (45, 5, 50),  Best Validation Loss: 0.6881\n",
      "Steps: 106, Number of layers: (45, 15, 10),  Best Validation Loss: 0.6829\n",
      "Steps: 107, Number of layers: (45, 15, 20),  Best Validation Loss: 0.6849\n",
      "Steps: 108, Number of layers: (45, 15, 30),  Best Validation Loss: 0.6801\n",
      "Steps: 109, Number of layers: (45, 15, 40),  Best Validation Loss: 0.6896\n",
      "Steps: 110, Number of layers: (45, 15, 50),  Best Validation Loss: 0.6848\n",
      "Steps: 111, Number of layers: (45, 25, 10),  Best Validation Loss: 0.6873\n",
      "Steps: 112, Number of layers: (45, 25, 20),  Best Validation Loss: 0.6901\n",
      "Steps: 113, Number of layers: (45, 25, 30),  Best Validation Loss: 0.6889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 114, Number of layers: (45, 25, 40),  Best Validation Loss: 0.6872\n",
      "Steps: 115, Number of layers: (45, 25, 50),  Best Validation Loss: 0.6734\n",
      "Steps: 116, Number of layers: (45, 35, 10),  Best Validation Loss: 0.6904\n",
      "Steps: 117, Number of layers: (45, 35, 20),  Best Validation Loss: 0.6881\n",
      "Steps: 118, Number of layers: (45, 35, 30),  Best Validation Loss: 0.6889\n",
      "Steps: 119, Number of layers: (45, 35, 40),  Best Validation Loss: 0.6826\n",
      "Steps: 120, Number of layers: (45, 35, 50),  Best Validation Loss: 0.6882\n",
      "Steps: 121, Number of layers: (45, 45, 10),  Best Validation Loss: 0.6834\n",
      "Steps: 122, Number of layers: (45, 45, 20),  Best Validation Loss: 0.6854\n",
      "Steps: 123, Number of layers: (45, 45, 30),  Best Validation Loss: 0.6918\n",
      "Steps: 124, Number of layers: (45, 45, 40),  Best Validation Loss: 0.6884\n",
      "Steps: 125, Number of layers: (45, 45, 50),  Best Validation Loss: 0.6818\n"
     ]
    }
   ],
   "source": [
    "#Initiate empty list to store best validation loss for a particular learning rate\n",
    "hd_loss = []\n",
    "steps = 1\n",
    "\n",
    "#Build network\n",
    "\n",
    "\n",
    "#Loop through the learning rate and train network\n",
    "for (hd1, hd2, d) in n_layers:\n",
    "    \n",
    "    hd_network = build_network([hd1, hd2, d], [0,0], input_shape=(timesteps, n_dim), batch_size=batch_size)\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optimizers.Adam(lr=best_lr)\n",
    "    \n",
    "    #Compile\n",
    "    hd_network.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    #Train\n",
    "    fitted_hidden = hd_network.fit(X_train_1, y_train_1, batch_size=batch_size, epochs=10, verbose=0, validation_data=(X_val_1, y_val_1), shuffle=True)\n",
    "    \n",
    "    #Display\n",
    "    print('Steps: %d,' % steps, 'Number of layers: (%d, %d, %d), ' % (hd1, hd2, d), 'Best Validation Loss: %.4f' %  np.min(fitted_hidden.history['val_loss']))\n",
    "    \n",
    "    #Save\n",
    "    hd_loss.append([np.min(fitted_hidden.history['val_loss'])])\n",
    "    \n",
    "    #Update\n",
    "    steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Number of Hidden Layer:  (45, 25, 50)  Validation Loss: 0.6734 \n",
      "Stored 'best_n_hidden' (tuple)\n",
      "Stored 'hd_loss' (list)\n"
     ]
    }
   ],
   "source": [
    "#Best number of hidden layer\n",
    "best_n_hidden = n_layers[np.argmin(hd_loss)]\n",
    "best_hidden_loss = np.min(hd_loss)\n",
    "print('Best Number of Hidden Layer: ', best_n_hidden, \" Validation Loss: %.4f \" % best_hidden_loss)\n",
    "%store best_n_hidden\n",
    "%store hd_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting Drop out rate\n",
    "drop_out = list(itertools.combinations_with_replacement(np.linspace(0.1,0.9, num=17),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1, Dropout: (0.1000, 0.1000)  Best Validation Loss: 0.6936\n",
      "Steps: 2, Dropout: (0.1000, 0.1500)  Best Validation Loss: 0.6890\n",
      "Steps: 3, Dropout: (0.1000, 0.2000)  Best Validation Loss: 0.6850\n",
      "Steps: 4, Dropout: (0.1000, 0.2500)  Best Validation Loss: 0.6873\n",
      "Steps: 5, Dropout: (0.1000, 0.3000)  Best Validation Loss: 0.6843\n",
      "Steps: 6, Dropout: (0.1000, 0.3500)  Best Validation Loss: 0.6937\n",
      "Steps: 7, Dropout: (0.1000, 0.4000)  Best Validation Loss: 0.6869\n",
      "Steps: 8, Dropout: (0.1000, 0.4500)  Best Validation Loss: 0.6902\n",
      "Steps: 9, Dropout: (0.1000, 0.5000)  Best Validation Loss: 0.6825\n",
      "Steps: 10, Dropout: (0.1000, 0.5500)  Best Validation Loss: 0.6891\n",
      "Steps: 11, Dropout: (0.1000, 0.6000)  Best Validation Loss: 0.6898\n",
      "Steps: 12, Dropout: (0.1000, 0.6500)  Best Validation Loss: 0.6861\n",
      "Steps: 13, Dropout: (0.1000, 0.7000)  Best Validation Loss: 0.6878\n",
      "Steps: 14, Dropout: (0.1000, 0.7500)  Best Validation Loss: 0.6870\n",
      "Steps: 15, Dropout: (0.1000, 0.8000)  Best Validation Loss: 0.6887\n",
      "Steps: 16, Dropout: (0.1000, 0.8500)  Best Validation Loss: 0.6938\n",
      "Steps: 17, Dropout: (0.1000, 0.9000)  Best Validation Loss: 0.6890\n",
      "Steps: 18, Dropout: (0.1500, 0.1500)  Best Validation Loss: 0.6909\n",
      "Steps: 19, Dropout: (0.1500, 0.2000)  Best Validation Loss: 0.6851\n",
      "Steps: 20, Dropout: (0.1500, 0.2500)  Best Validation Loss: 0.6910\n",
      "Steps: 21, Dropout: (0.1500, 0.3000)  Best Validation Loss: 0.6833\n",
      "Steps: 22, Dropout: (0.1500, 0.3500)  Best Validation Loss: 0.6909\n",
      "Steps: 23, Dropout: (0.1500, 0.4000)  Best Validation Loss: 0.6845\n",
      "Steps: 24, Dropout: (0.1500, 0.4500)  Best Validation Loss: 0.6919\n",
      "Steps: 25, Dropout: (0.1500, 0.5000)  Best Validation Loss: 0.6862\n",
      "Steps: 26, Dropout: (0.1500, 0.5500)  Best Validation Loss: 0.6878\n",
      "Steps: 27, Dropout: (0.1500, 0.6000)  Best Validation Loss: 0.6874\n",
      "Steps: 28, Dropout: (0.1500, 0.6500)  Best Validation Loss: 0.6880\n",
      "Steps: 29, Dropout: (0.1500, 0.7000)  Best Validation Loss: 0.6845\n",
      "Steps: 30, Dropout: (0.1500, 0.7500)  Best Validation Loss: 0.6790\n",
      "Steps: 31, Dropout: (0.1500, 0.8000)  Best Validation Loss: 0.6811\n",
      "Steps: 32, Dropout: (0.1500, 0.8500)  Best Validation Loss: 0.6852\n",
      "Steps: 33, Dropout: (0.1500, 0.9000)  Best Validation Loss: 0.6865\n",
      "Steps: 34, Dropout: (0.2000, 0.2000)  Best Validation Loss: 0.6906\n",
      "Steps: 35, Dropout: (0.2000, 0.2500)  Best Validation Loss: 0.6920\n",
      "Steps: 36, Dropout: (0.2000, 0.3000)  Best Validation Loss: 0.6861\n",
      "Steps: 37, Dropout: (0.2000, 0.3500)  Best Validation Loss: 0.6842\n",
      "Steps: 38, Dropout: (0.2000, 0.4000)  Best Validation Loss: 0.6860\n",
      "Steps: 39, Dropout: (0.2000, 0.4500)  Best Validation Loss: 0.6934\n",
      "Steps: 40, Dropout: (0.2000, 0.5000)  Best Validation Loss: 0.6848\n",
      "Steps: 41, Dropout: (0.2000, 0.5500)  Best Validation Loss: 0.6872\n",
      "Steps: 42, Dropout: (0.2000, 0.6000)  Best Validation Loss: 0.6903\n",
      "Steps: 43, Dropout: (0.2000, 0.6500)  Best Validation Loss: 0.6851\n",
      "Steps: 44, Dropout: (0.2000, 0.7000)  Best Validation Loss: 0.6913\n",
      "Steps: 45, Dropout: (0.2000, 0.7500)  Best Validation Loss: 0.6948\n",
      "Steps: 46, Dropout: (0.2000, 0.8000)  Best Validation Loss: 0.6851\n",
      "Steps: 47, Dropout: (0.2000, 0.8500)  Best Validation Loss: 0.6847\n",
      "Steps: 48, Dropout: (0.2000, 0.9000)  Best Validation Loss: 0.6839\n",
      "Steps: 49, Dropout: (0.2500, 0.2500)  Best Validation Loss: 0.6754\n",
      "Steps: 50, Dropout: (0.2500, 0.3000)  Best Validation Loss: 0.6847\n",
      "Steps: 51, Dropout: (0.2500, 0.3500)  Best Validation Loss: 0.6822\n",
      "Steps: 52, Dropout: (0.2500, 0.4000)  Best Validation Loss: 0.6875\n",
      "Steps: 53, Dropout: (0.2500, 0.4500)  Best Validation Loss: 0.6851\n",
      "Steps: 54, Dropout: (0.2500, 0.5000)  Best Validation Loss: 0.6870\n",
      "Steps: 55, Dropout: (0.2500, 0.5500)  Best Validation Loss: 0.6894\n",
      "Steps: 56, Dropout: (0.2500, 0.6000)  Best Validation Loss: 0.6874\n",
      "Steps: 57, Dropout: (0.2500, 0.6500)  Best Validation Loss: 0.6904\n",
      "Steps: 58, Dropout: (0.2500, 0.7000)  Best Validation Loss: 0.6906\n",
      "Steps: 59, Dropout: (0.2500, 0.7500)  Best Validation Loss: 0.6833\n",
      "Steps: 60, Dropout: (0.2500, 0.8000)  Best Validation Loss: 0.6911\n",
      "Steps: 61, Dropout: (0.2500, 0.8500)  Best Validation Loss: 0.6907\n",
      "Steps: 62, Dropout: (0.2500, 0.9000)  Best Validation Loss: 0.6900\n",
      "Steps: 63, Dropout: (0.3000, 0.3000)  Best Validation Loss: 0.6866\n",
      "Steps: 64, Dropout: (0.3000, 0.3500)  Best Validation Loss: 0.6896\n",
      "Steps: 65, Dropout: (0.3000, 0.4000)  Best Validation Loss: 0.6851\n",
      "Steps: 66, Dropout: (0.3000, 0.4500)  Best Validation Loss: 0.6875\n",
      "Steps: 67, Dropout: (0.3000, 0.5000)  Best Validation Loss: 0.6874\n",
      "Steps: 68, Dropout: (0.3000, 0.5500)  Best Validation Loss: 0.6882\n",
      "Steps: 69, Dropout: (0.3000, 0.6000)  Best Validation Loss: 0.6874\n",
      "Steps: 70, Dropout: (0.3000, 0.6500)  Best Validation Loss: 0.6881\n",
      "Steps: 71, Dropout: (0.3000, 0.7000)  Best Validation Loss: 0.6855\n",
      "Steps: 72, Dropout: (0.3000, 0.7500)  Best Validation Loss: 0.6890\n",
      "Steps: 73, Dropout: (0.3000, 0.8000)  Best Validation Loss: 0.6878\n",
      "Steps: 74, Dropout: (0.3000, 0.8500)  Best Validation Loss: 0.6872\n",
      "Steps: 75, Dropout: (0.3000, 0.9000)  Best Validation Loss: 0.6912\n",
      "Steps: 76, Dropout: (0.3500, 0.3500)  Best Validation Loss: 0.6867\n",
      "Steps: 77, Dropout: (0.3500, 0.4000)  Best Validation Loss: 0.6917\n",
      "Steps: 78, Dropout: (0.3500, 0.4500)  Best Validation Loss: 0.6843\n",
      "Steps: 79, Dropout: (0.3500, 0.5000)  Best Validation Loss: 0.6933\n",
      "Steps: 80, Dropout: (0.3500, 0.5500)  Best Validation Loss: 0.6824\n",
      "Steps: 81, Dropout: (0.3500, 0.6000)  Best Validation Loss: 0.6844\n",
      "Steps: 82, Dropout: (0.3500, 0.6500)  Best Validation Loss: 0.6845\n",
      "Steps: 83, Dropout: (0.3500, 0.7000)  Best Validation Loss: 0.6933\n",
      "Steps: 84, Dropout: (0.3500, 0.7500)  Best Validation Loss: 0.6881\n",
      "Steps: 85, Dropout: (0.3500, 0.8000)  Best Validation Loss: 0.6898\n",
      "Steps: 86, Dropout: (0.3500, 0.8500)  Best Validation Loss: 0.6848\n",
      "Steps: 87, Dropout: (0.3500, 0.9000)  Best Validation Loss: 0.6908\n",
      "Steps: 88, Dropout: (0.4000, 0.4000)  Best Validation Loss: 0.6869\n",
      "Steps: 89, Dropout: (0.4000, 0.4500)  Best Validation Loss: 0.6891\n",
      "Steps: 90, Dropout: (0.4000, 0.5000)  Best Validation Loss: 0.6884\n",
      "Steps: 91, Dropout: (0.4000, 0.5500)  Best Validation Loss: 0.6866\n",
      "Steps: 92, Dropout: (0.4000, 0.6000)  Best Validation Loss: 0.6849\n",
      "Steps: 93, Dropout: (0.4000, 0.6500)  Best Validation Loss: 0.6831\n",
      "Steps: 94, Dropout: (0.4000, 0.7000)  Best Validation Loss: 0.6939\n",
      "Steps: 95, Dropout: (0.4000, 0.7500)  Best Validation Loss: 0.6864\n",
      "Steps: 96, Dropout: (0.4000, 0.8000)  Best Validation Loss: 0.6837\n",
      "Steps: 97, Dropout: (0.4000, 0.8500)  Best Validation Loss: 0.6866\n",
      "Steps: 98, Dropout: (0.4000, 0.9000)  Best Validation Loss: 0.6850\n",
      "Steps: 99, Dropout: (0.4500, 0.4500)  Best Validation Loss: 0.6861\n",
      "Steps: 100, Dropout: (0.4500, 0.5000)  Best Validation Loss: 0.6915\n",
      "Steps: 101, Dropout: (0.4500, 0.5500)  Best Validation Loss: 0.6868\n",
      "Steps: 102, Dropout: (0.4500, 0.6000)  Best Validation Loss: 0.6765\n",
      "Steps: 103, Dropout: (0.4500, 0.6500)  Best Validation Loss: 0.6879\n",
      "Steps: 104, Dropout: (0.4500, 0.7000)  Best Validation Loss: 0.6867\n",
      "Steps: 105, Dropout: (0.4500, 0.7500)  Best Validation Loss: 0.6808\n",
      "Steps: 106, Dropout: (0.4500, 0.8000)  Best Validation Loss: 0.6927\n",
      "Steps: 107, Dropout: (0.4500, 0.8500)  Best Validation Loss: 0.6865\n",
      "Steps: 108, Dropout: (0.4500, 0.9000)  Best Validation Loss: 0.6864\n",
      "Steps: 109, Dropout: (0.5000, 0.5000)  Best Validation Loss: 0.6824\n",
      "Steps: 110, Dropout: (0.5000, 0.5500)  Best Validation Loss: 0.6906\n",
      "Steps: 111, Dropout: (0.5000, 0.6000)  Best Validation Loss: 0.6905\n",
      "Steps: 112, Dropout: (0.5000, 0.6500)  Best Validation Loss: 0.6796\n",
      "Steps: 113, Dropout: (0.5000, 0.7000)  Best Validation Loss: 0.6889\n",
      "Steps: 114, Dropout: (0.5000, 0.7500)  Best Validation Loss: 0.6801\n",
      "Steps: 115, Dropout: (0.5000, 0.8000)  Best Validation Loss: 0.6814\n",
      "Steps: 116, Dropout: (0.5000, 0.8500)  Best Validation Loss: 0.6899\n",
      "Steps: 117, Dropout: (0.5000, 0.9000)  Best Validation Loss: 0.6926\n",
      "Steps: 118, Dropout: (0.5500, 0.5500)  Best Validation Loss: 0.6844\n",
      "Steps: 119, Dropout: (0.5500, 0.6000)  Best Validation Loss: 0.6848\n",
      "Steps: 120, Dropout: (0.5500, 0.6500)  Best Validation Loss: 0.6821\n",
      "Steps: 121, Dropout: (0.5500, 0.7000)  Best Validation Loss: 0.6911\n",
      "Steps: 122, Dropout: (0.5500, 0.7500)  Best Validation Loss: 0.6904\n",
      "Steps: 123, Dropout: (0.5500, 0.8000)  Best Validation Loss: 0.6937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 124, Dropout: (0.5500, 0.8500)  Best Validation Loss: 0.6867\n",
      "Steps: 125, Dropout: (0.5500, 0.9000)  Best Validation Loss: 0.6887\n",
      "Steps: 126, Dropout: (0.6000, 0.6000)  Best Validation Loss: 0.6868\n",
      "Steps: 127, Dropout: (0.6000, 0.6500)  Best Validation Loss: 0.6859\n",
      "Steps: 128, Dropout: (0.6000, 0.7000)  Best Validation Loss: 0.6910\n",
      "Steps: 129, Dropout: (0.6000, 0.7500)  Best Validation Loss: 0.6909\n",
      "Steps: 130, Dropout: (0.6000, 0.8000)  Best Validation Loss: 0.6907\n",
      "Steps: 131, Dropout: (0.6000, 0.8500)  Best Validation Loss: 0.6857\n",
      "Steps: 132, Dropout: (0.6000, 0.9000)  Best Validation Loss: 0.6838\n",
      "Steps: 133, Dropout: (0.6500, 0.6500)  Best Validation Loss: 0.6889\n",
      "Steps: 134, Dropout: (0.6500, 0.7000)  Best Validation Loss: 0.6892\n",
      "Steps: 135, Dropout: (0.6500, 0.7500)  Best Validation Loss: 0.6791\n",
      "Steps: 136, Dropout: (0.6500, 0.8000)  Best Validation Loss: 0.6894\n",
      "Steps: 137, Dropout: (0.6500, 0.8500)  Best Validation Loss: 0.6936\n",
      "Steps: 138, Dropout: (0.6500, 0.9000)  Best Validation Loss: 0.6926\n",
      "Steps: 139, Dropout: (0.7000, 0.7000)  Best Validation Loss: 0.6835\n",
      "Steps: 140, Dropout: (0.7000, 0.7500)  Best Validation Loss: 0.6867\n",
      "Steps: 141, Dropout: (0.7000, 0.8000)  Best Validation Loss: 0.6843\n",
      "Steps: 142, Dropout: (0.7000, 0.8500)  Best Validation Loss: 0.6904\n",
      "Steps: 143, Dropout: (0.7000, 0.9000)  Best Validation Loss: 0.6874\n",
      "Steps: 144, Dropout: (0.7500, 0.7500)  Best Validation Loss: 0.6891\n",
      "Steps: 145, Dropout: (0.7500, 0.8000)  Best Validation Loss: 0.6837\n",
      "Steps: 146, Dropout: (0.7500, 0.8500)  Best Validation Loss: 0.6849\n",
      "Steps: 147, Dropout: (0.7500, 0.9000)  Best Validation Loss: 0.6901\n",
      "Steps: 148, Dropout: (0.8000, 0.8000)  Best Validation Loss: 0.6890\n",
      "Steps: 149, Dropout: (0.8000, 0.8500)  Best Validation Loss: 0.6930\n",
      "Steps: 150, Dropout: (0.8000, 0.9000)  Best Validation Loss: 0.6900\n",
      "Steps: 151, Dropout: (0.8500, 0.8500)  Best Validation Loss: 0.6904\n",
      "Steps: 152, Dropout: (0.8500, 0.9000)  Best Validation Loss: 0.6921\n",
      "Steps: 153, Dropout: (0.9000, 0.9000)  Best Validation Loss: 0.6925\n"
     ]
    }
   ],
   "source": [
    "#Initiate empty list to store best validation loss for a particular learning rate\n",
    "do_loss = []\n",
    "steps = 1\n",
    "\n",
    "#Build network\n",
    "\n",
    "\n",
    "#Loop through the learning rate and train network\n",
    "for d in drop_out:\n",
    "    \n",
    "    do_network = build_network(best_n_hidden, d, input_shape=(timesteps, n_dim), batch_size=batch_size)\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optimizers.Adam(lr=best_lr)\n",
    "    \n",
    "    #Compile\n",
    "    do_network.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    #Train\n",
    "    fitted_dropout = do_network.fit(X_train_1, y_train_1, batch_size=batch_size, epochs=10, verbose=0, validation_data=(X_val_1, y_val_1), shuffle=True)\n",
    "    \n",
    "    #Display\n",
    "    print('Steps: %d,' %steps, 'Dropout: (%.4f, %.4f) ' % d, 'Best Validation Loss: %.4f' % np.min(fitted_dropout.history['val_loss']))\n",
    "    \n",
    "    #Save\n",
    "    do_loss.append([np.min(fitted_dropout.history['val_loss'])])\n",
    "    \n",
    "    #Update\n",
    "    steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Drop out probability: (0.25, 0.25),  Validation Loss: 0.6754\n",
      "Stored 'best_drop_out' (tuple)\n",
      "Stored 'do_loss' (list)\n"
     ]
    }
   ],
   "source": [
    "best_drop_out = drop_out[np.argmin(do_loss)]\n",
    "best_do_loss = np.min(do_loss)\n",
    "print('Best Drop out probability: (%.2f, %.2f), ' % best_drop_out, 'Validation Loss: %.4f' % best_do_loss)\n",
    "\n",
    "%store best_drop_out\n",
    "%store do_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test timesteps from 1 step to 220 days lag\n",
    "time = np.arange(10,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1, Timesteps: 10 Best Validation Loss: 0.6891\n",
      "Steps: 2, Timesteps: 11 Best Validation Loss: 0.6927\n",
      "Steps: 3, Timesteps: 12 Best Validation Loss: 0.6954\n",
      "Steps: 4, Timesteps: 13 Best Validation Loss: 0.6905\n",
      "Steps: 5, Timesteps: 14 Best Validation Loss: 0.6896\n",
      "Steps: 6, Timesteps: 15 Best Validation Loss: 0.6932\n",
      "Steps: 7, Timesteps: 16 Best Validation Loss: 0.6949\n",
      "Steps: 8, Timesteps: 17 Best Validation Loss: 0.6839\n",
      "Steps: 9, Timesteps: 18 Best Validation Loss: 0.6920\n",
      "Steps: 10, Timesteps: 19 Best Validation Loss: 0.6956\n",
      "Steps: 11, Timesteps: 20 Best Validation Loss: 0.6898\n",
      "Steps: 12, Timesteps: 21 Best Validation Loss: 0.6862\n",
      "Steps: 13, Timesteps: 22 Best Validation Loss: 0.6898\n",
      "Steps: 14, Timesteps: 23 Best Validation Loss: 0.6999\n",
      "Steps: 15, Timesteps: 24 Best Validation Loss: 0.6943\n",
      "Steps: 16, Timesteps: 25 Best Validation Loss: 0.6894\n",
      "Steps: 17, Timesteps: 26 Best Validation Loss: 0.6863\n",
      "Steps: 18, Timesteps: 27 Best Validation Loss: 0.6895\n",
      "Steps: 19, Timesteps: 28 Best Validation Loss: 0.6925\n",
      "Steps: 20, Timesteps: 29 Best Validation Loss: 0.6894\n",
      "Steps: 21, Timesteps: 30 Best Validation Loss: 0.6892\n",
      "Steps: 22, Timesteps: 31 Best Validation Loss: 0.6918\n",
      "Steps: 23, Timesteps: 32 Best Validation Loss: 0.6888\n",
      "Steps: 24, Timesteps: 33 Best Validation Loss: 0.6911\n",
      "Steps: 25, Timesteps: 34 Best Validation Loss: 0.6943\n",
      "Steps: 26, Timesteps: 35 Best Validation Loss: 0.6876\n",
      "Steps: 27, Timesteps: 36 Best Validation Loss: 0.6871\n",
      "Steps: 28, Timesteps: 37 Best Validation Loss: 0.6927\n",
      "Steps: 29, Timesteps: 38 Best Validation Loss: 0.6884\n",
      "Steps: 30, Timesteps: 39 Best Validation Loss: 0.6937\n",
      "Steps: 31, Timesteps: 40 Best Validation Loss: 0.6899\n",
      "Steps: 32, Timesteps: 41 Best Validation Loss: 0.6915\n",
      "Steps: 33, Timesteps: 42 Best Validation Loss: 0.6883\n",
      "Steps: 34, Timesteps: 43 Best Validation Loss: 0.6925\n",
      "Steps: 35, Timesteps: 44 Best Validation Loss: 0.6872\n",
      "Steps: 36, Timesteps: 45 Best Validation Loss: 0.6894\n",
      "Steps: 37, Timesteps: 46 Best Validation Loss: 0.6938\n",
      "Steps: 38, Timesteps: 47 Best Validation Loss: 0.6901\n",
      "Steps: 39, Timesteps: 48 Best Validation Loss: 0.6903\n",
      "Steps: 40, Timesteps: 49 Best Validation Loss: 0.6909\n",
      "Steps: 41, Timesteps: 50 Best Validation Loss: 0.6933\n",
      "Steps: 42, Timesteps: 51 Best Validation Loss: 0.6937\n",
      "Steps: 43, Timesteps: 52 Best Validation Loss: 0.6877\n",
      "Steps: 44, Timesteps: 53 Best Validation Loss: 0.6901\n",
      "Steps: 45, Timesteps: 54 Best Validation Loss: 0.6884\n",
      "Steps: 46, Timesteps: 55 Best Validation Loss: 0.6908\n",
      "Steps: 47, Timesteps: 56 Best Validation Loss: 0.6853\n",
      "Steps: 48, Timesteps: 57 Best Validation Loss: 0.6947\n",
      "Steps: 49, Timesteps: 58 Best Validation Loss: 0.6925\n",
      "Steps: 50, Timesteps: 59 Best Validation Loss: 0.6902\n",
      "Steps: 51, Timesteps: 60 Best Validation Loss: 0.6926\n",
      "Steps: 52, Timesteps: 61 Best Validation Loss: 0.6901\n",
      "Steps: 53, Timesteps: 62 Best Validation Loss: 0.6887\n",
      "Steps: 54, Timesteps: 63 Best Validation Loss: 0.6885\n",
      "Steps: 55, Timesteps: 64 Best Validation Loss: 0.6902\n",
      "Steps: 56, Timesteps: 65 Best Validation Loss: 0.6913\n",
      "Steps: 57, Timesteps: 66 Best Validation Loss: 0.6969\n",
      "Steps: 58, Timesteps: 67 Best Validation Loss: 0.6903\n",
      "Steps: 59, Timesteps: 68 Best Validation Loss: 0.6891\n",
      "Steps: 60, Timesteps: 69 Best Validation Loss: 0.6952\n",
      "Steps: 61, Timesteps: 70 Best Validation Loss: 0.6887\n",
      "Steps: 62, Timesteps: 71 Best Validation Loss: 0.6928\n",
      "Steps: 63, Timesteps: 72 Best Validation Loss: 0.6961\n",
      "Steps: 64, Timesteps: 73 Best Validation Loss: 0.6905\n",
      "Steps: 65, Timesteps: 74 Best Validation Loss: 0.6958\n",
      "Steps: 66, Timesteps: 75 Best Validation Loss: 0.6856\n",
      "Steps: 67, Timesteps: 76 Best Validation Loss: 0.6877\n",
      "Steps: 68, Timesteps: 77 Best Validation Loss: 0.6885\n",
      "Steps: 69, Timesteps: 78 Best Validation Loss: 0.6925\n",
      "Steps: 70, Timesteps: 79 Best Validation Loss: 0.6911\n",
      "Steps: 71, Timesteps: 80 Best Validation Loss: 0.6906\n",
      "Steps: 72, Timesteps: 81 Best Validation Loss: 0.6951\n",
      "Steps: 73, Timesteps: 82 Best Validation Loss: 0.6905\n",
      "Steps: 74, Timesteps: 83 Best Validation Loss: 0.6894\n",
      "Steps: 75, Timesteps: 84 Best Validation Loss: 0.6900\n",
      "Steps: 76, Timesteps: 85 Best Validation Loss: 0.6958\n",
      "Steps: 77, Timesteps: 86 Best Validation Loss: 0.6933\n",
      "Steps: 78, Timesteps: 87 Best Validation Loss: 0.6955\n",
      "Steps: 79, Timesteps: 88 Best Validation Loss: 0.6917\n",
      "Steps: 80, Timesteps: 89 Best Validation Loss: 0.6914\n",
      "Steps: 81, Timesteps: 90 Best Validation Loss: 0.6883\n",
      "Steps: 82, Timesteps: 91 Best Validation Loss: 0.6919\n",
      "Steps: 83, Timesteps: 92 Best Validation Loss: 0.6903\n",
      "Steps: 84, Timesteps: 93 Best Validation Loss: 0.6947\n",
      "Steps: 85, Timesteps: 94 Best Validation Loss: 0.6914\n",
      "Steps: 86, Timesteps: 95 Best Validation Loss: 0.6936\n",
      "Steps: 87, Timesteps: 96 Best Validation Loss: 0.6962\n",
      "Steps: 88, Timesteps: 97 Best Validation Loss: 0.6887\n",
      "Steps: 89, Timesteps: 98 Best Validation Loss: 0.6917\n",
      "Steps: 90, Timesteps: 99 Best Validation Loss: 0.6951\n",
      "Steps: 91, Timesteps: 100 Best Validation Loss: 0.6890\n",
      "Steps: 92, Timesteps: 101 Best Validation Loss: 0.6952\n",
      "Steps: 93, Timesteps: 102 Best Validation Loss: 0.6849\n",
      "Steps: 94, Timesteps: 103 Best Validation Loss: 0.6988\n",
      "Steps: 95, Timesteps: 104 Best Validation Loss: 0.6942\n",
      "Steps: 96, Timesteps: 105 Best Validation Loss: 0.6862\n",
      "Steps: 97, Timesteps: 106 Best Validation Loss: 0.6931\n",
      "Steps: 98, Timesteps: 107 Best Validation Loss: 0.6936\n",
      "Steps: 99, Timesteps: 108 Best Validation Loss: 0.6912\n",
      "Steps: 100, Timesteps: 109 Best Validation Loss: 0.6905\n",
      "Steps: 101, Timesteps: 110 Best Validation Loss: 0.6937\n",
      "Steps: 102, Timesteps: 111 Best Validation Loss: 0.6988\n",
      "Steps: 103, Timesteps: 112 Best Validation Loss: 0.6928\n",
      "Steps: 104, Timesteps: 113 Best Validation Loss: 0.6897\n",
      "Steps: 105, Timesteps: 114 Best Validation Loss: 0.6924\n",
      "Steps: 106, Timesteps: 115 Best Validation Loss: 0.6917\n",
      "Steps: 107, Timesteps: 116 Best Validation Loss: 0.6880\n",
      "Steps: 108, Timesteps: 117 Best Validation Loss: 0.6956\n",
      "Steps: 109, Timesteps: 118 Best Validation Loss: 0.6956\n",
      "Steps: 110, Timesteps: 119 Best Validation Loss: 0.6935\n",
      "Steps: 111, Timesteps: 120 Best Validation Loss: 0.6957\n",
      "Steps: 112, Timesteps: 121 Best Validation Loss: 0.6915\n",
      "Steps: 113, Timesteps: 122 Best Validation Loss: 0.6916\n",
      "Steps: 114, Timesteps: 123 Best Validation Loss: 0.6911\n",
      "Steps: 115, Timesteps: 124 Best Validation Loss: 0.6912\n",
      "Steps: 116, Timesteps: 125 Best Validation Loss: 0.6953\n",
      "Steps: 117, Timesteps: 126 Best Validation Loss: 0.6907\n",
      "Steps: 118, Timesteps: 127 Best Validation Loss: 0.6882\n",
      "Steps: 119, Timesteps: 128 Best Validation Loss: 0.6910\n",
      "Steps: 120, Timesteps: 129 Best Validation Loss: 0.6885\n",
      "Steps: 121, Timesteps: 130 Best Validation Loss: 0.6941\n",
      "Steps: 122, Timesteps: 131 Best Validation Loss: 0.6914\n",
      "Steps: 123, Timesteps: 132 Best Validation Loss: 0.6953\n",
      "Steps: 124, Timesteps: 133 Best Validation Loss: 0.6919\n",
      "Steps: 125, Timesteps: 134 Best Validation Loss: 0.6921\n",
      "Steps: 126, Timesteps: 135 Best Validation Loss: 0.6903\n",
      "Steps: 127, Timesteps: 136 Best Validation Loss: 0.6952\n",
      "Steps: 128, Timesteps: 137 Best Validation Loss: 0.6949\n",
      "Steps: 129, Timesteps: 138 Best Validation Loss: 0.6947\n",
      "Steps: 130, Timesteps: 139 Best Validation Loss: 0.6993\n",
      "Steps: 131, Timesteps: 140 Best Validation Loss: 0.6903\n",
      "Steps: 132, Timesteps: 141 Best Validation Loss: 0.6932\n",
      "Steps: 133, Timesteps: 142 Best Validation Loss: 0.6901\n",
      "Steps: 134, Timesteps: 143 Best Validation Loss: 0.6884\n",
      "Steps: 135, Timesteps: 144 Best Validation Loss: 0.6929\n",
      "Steps: 136, Timesteps: 145 Best Validation Loss: 0.6898\n",
      "Steps: 137, Timesteps: 146 Best Validation Loss: 0.6947\n",
      "Steps: 138, Timesteps: 147 Best Validation Loss: 0.6920\n",
      "Steps: 139, Timesteps: 148 Best Validation Loss: 0.6905\n",
      "Steps: 140, Timesteps: 149 Best Validation Loss: 0.6921\n",
      "Steps: 141, Timesteps: 150 Best Validation Loss: 0.6924\n",
      "Steps: 142, Timesteps: 151 Best Validation Loss: 0.6921\n",
      "Steps: 143, Timesteps: 152 Best Validation Loss: 0.6980\n",
      "Steps: 144, Timesteps: 153 Best Validation Loss: 0.6901\n",
      "Steps: 145, Timesteps: 154 Best Validation Loss: 0.6924\n",
      "Steps: 146, Timesteps: 155 Best Validation Loss: 0.6936\n",
      "Steps: 147, Timesteps: 156 Best Validation Loss: 0.6879\n",
      "Steps: 148, Timesteps: 157 Best Validation Loss: 0.6930\n",
      "Steps: 149, Timesteps: 158 Best Validation Loss: 0.6970\n",
      "Steps: 150, Timesteps: 159 Best Validation Loss: 0.6930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 151, Timesteps: 160 Best Validation Loss: 0.6904\n",
      "Steps: 152, Timesteps: 161 Best Validation Loss: 0.6983\n",
      "Steps: 153, Timesteps: 162 Best Validation Loss: 0.6934\n",
      "Steps: 154, Timesteps: 163 Best Validation Loss: 0.6961\n",
      "Steps: 155, Timesteps: 164 Best Validation Loss: 0.6912\n",
      "Steps: 156, Timesteps: 165 Best Validation Loss: 0.6924\n",
      "Steps: 157, Timesteps: 166 Best Validation Loss: 0.6934\n",
      "Steps: 158, Timesteps: 167 Best Validation Loss: 0.6947\n",
      "Steps: 159, Timesteps: 168 Best Validation Loss: 0.6928\n",
      "Steps: 160, Timesteps: 169 Best Validation Loss: 0.6927\n",
      "Steps: 161, Timesteps: 170 Best Validation Loss: 0.6908\n",
      "Steps: 162, Timesteps: 171 Best Validation Loss: 0.6894\n",
      "Steps: 163, Timesteps: 172 Best Validation Loss: 0.6906\n",
      "Steps: 164, Timesteps: 173 Best Validation Loss: 0.6919\n",
      "Steps: 165, Timesteps: 174 Best Validation Loss: 0.6926\n",
      "Steps: 166, Timesteps: 175 Best Validation Loss: 0.6910\n",
      "Steps: 167, Timesteps: 176 Best Validation Loss: 0.6937\n",
      "Steps: 168, Timesteps: 177 Best Validation Loss: 0.6886\n",
      "Steps: 169, Timesteps: 178 Best Validation Loss: 0.6965\n",
      "Steps: 170, Timesteps: 179 Best Validation Loss: 0.6929\n",
      "Steps: 171, Timesteps: 180 Best Validation Loss: 0.6888\n",
      "Steps: 172, Timesteps: 181 Best Validation Loss: 0.6927\n",
      "Steps: 173, Timesteps: 182 Best Validation Loss: 0.6941\n",
      "Steps: 174, Timesteps: 183 Best Validation Loss: 0.6904\n",
      "Steps: 175, Timesteps: 184 Best Validation Loss: 0.6936\n",
      "Steps: 176, Timesteps: 185 Best Validation Loss: 0.6936\n",
      "Steps: 177, Timesteps: 186 Best Validation Loss: 0.6906\n",
      "Steps: 178, Timesteps: 187 Best Validation Loss: 0.6894\n",
      "Steps: 179, Timesteps: 188 Best Validation Loss: 0.6943\n",
      "Steps: 180, Timesteps: 189 Best Validation Loss: 0.6912\n",
      "Steps: 181, Timesteps: 190 Best Validation Loss: 0.6950\n",
      "Steps: 182, Timesteps: 191 Best Validation Loss: 0.6950\n",
      "Steps: 183, Timesteps: 192 Best Validation Loss: 0.6984\n",
      "Steps: 184, Timesteps: 193 Best Validation Loss: 0.6959\n",
      "Steps: 185, Timesteps: 194 Best Validation Loss: 0.6973\n",
      "Steps: 186, Timesteps: 195 Best Validation Loss: 0.6944\n",
      "Steps: 187, Timesteps: 196 Best Validation Loss: 0.6914\n",
      "Steps: 188, Timesteps: 197 Best Validation Loss: 0.6952\n",
      "Steps: 189, Timesteps: 198 Best Validation Loss: 0.6958\n",
      "Steps: 190, Timesteps: 199 Best Validation Loss: 0.6940\n"
     ]
    }
   ],
   "source": [
    "#Initiate empty list to store best validation loss for a particular timestep\n",
    "t_loss = []\n",
    "steps = 1\n",
    "\n",
    "\n",
    "#Loop through the learning rate and train network\n",
    "for t in time:\n",
    "    \n",
    "    #Generate inputs on timesteps\n",
    "    X_train_2, y_train_2 = get_inputs(X_train, y_train, batch_size, t)\n",
    "    X_val_2, y_val_2 = get_inputs(X_val, y_val, batch_size, t)\n",
    "    \n",
    "    #Build network\n",
    "    t_network = build_network(best_n_hidden, best_drop_out, input_shape=(t, n_dim), batch_size=batch_size)\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optimizers.Adam(lr=best_lr)\n",
    "    \n",
    "    #Compile\n",
    "    t_network.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    #Train\n",
    "    fitted_timesteps = t_network.fit(X_train_2, y_train_2, batch_size=batch_size, epochs=5, verbose=0, validation_data=(X_val_2, y_val_2), shuffle=True)\n",
    "    \n",
    "    #Display\n",
    "    print('Steps: %d,' % steps, 'Timesteps: %d' % t, 'Best Validation Loss: %.4f' % np.min(fitted_timesteps.history['val_loss']))\n",
    "    \n",
    "    #Save\n",
    "    t_loss.append([np.min(fitted_timesteps.history['val_loss'])])\n",
    "    \n",
    "    #Update\n",
    "    steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Drop out probability: (0.25, 0.25),  Validation Loss: 0.6839\n",
      "Stored 'best_timestep' (int64)\n",
      "Stored 't_loss' (list)\n"
     ]
    }
   ],
   "source": [
    "best_timestep = time[np.argmin(t_loss)]\n",
    "best_t_loss = np.min(t_loss)\n",
    "print('Best Drop out probability: (%.2f, %.2f), ' % best_drop_out, 'Validation Loss: %.4f' % best_t_loss)\n",
    "\n",
    "%store best_timestep\n",
    "%store t_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary\n",
      "-------------\n",
      "Learning Rate: 0.00005\n",
      "Number of layers:  (45, 25, 50)\n",
      "Dropout Probability: (0.25, 0.25)\n",
      "Timesteps:  17\n"
     ]
    }
   ],
   "source": [
    "print('Summary')\n",
    "print('-------------')\n",
    "print('Learning Rate: %.5f' % best_lr)\n",
    "print('Number of layers: ', best_n_hidden)\n",
    "print('Dropout Probability: (%.2f, %.2f)' % best_drop_out)\n",
    "print('Timesteps: ', best_timestep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 8: Refit using best parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_best, y_train_best = get_inputs(X_train, y_train, batch_size, best_timestep)\n",
    "X_val_best, y_val_best = get_inputs(X_val, y_val, batch_size, best_timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1280 samples, validate on 256 samples\n",
      "Epoch 1/500\n",
      "1280/1280 [==============================] - 15s - loss: 0.6953 - acc: 0.4993 - val_loss: 0.6951 - val_acc: 0.4954\n",
      "Epoch 2/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6947 - acc: 0.5033 - val_loss: 0.6945 - val_acc: 0.4964\n",
      "Epoch 3/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6939 - acc: 0.5082 - val_loss: 0.6934 - val_acc: 0.4998\n",
      "Epoch 4/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6929 - acc: 0.5147 - val_loss: 0.6927 - val_acc: 0.5082\n",
      "Epoch 5/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6925 - acc: 0.5128 - val_loss: 0.6922 - val_acc: 0.5115\n",
      "Epoch 6/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6911 - acc: 0.5269 - val_loss: 0.6913 - val_acc: 0.5190\n",
      "Epoch 7/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6902 - acc: 0.5337 - val_loss: 0.6910 - val_acc: 0.5226\n",
      "Epoch 8/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6892 - acc: 0.5353 - val_loss: 0.6901 - val_acc: 0.5296\n",
      "Epoch 9/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6881 - acc: 0.5452 - val_loss: 0.6891 - val_acc: 0.5327\n",
      "Epoch 10/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6876 - acc: 0.5479 - val_loss: 0.6880 - val_acc: 0.5469\n",
      "Epoch 11/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6871 - acc: 0.5500 - val_loss: 0.6877 - val_acc: 0.5489\n",
      "Epoch 12/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6860 - acc: 0.5576 - val_loss: 0.6868 - val_acc: 0.5563\n",
      "Epoch 13/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6851 - acc: 0.5609 - val_loss: 0.6856 - val_acc: 0.5579\n",
      "Epoch 14/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6833 - acc: 0.5706 - val_loss: 0.6848 - val_acc: 0.5688\n",
      "Epoch 15/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6829 - acc: 0.5721 - val_loss: 0.6835 - val_acc: 0.5705\n",
      "Epoch 16/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6823 - acc: 0.5745 - val_loss: 0.6833 - val_acc: 0.5726\n",
      "Epoch 17/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6809 - acc: 0.5805 - val_loss: 0.6839 - val_acc: 0.5654\n",
      "Epoch 18/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6805 - acc: 0.5814 - val_loss: 0.6821 - val_acc: 0.5708\n",
      "Epoch 19/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6797 - acc: 0.5803 - val_loss: 0.6814 - val_acc: 0.5742\n",
      "Epoch 20/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6781 - acc: 0.5885 - val_loss: 0.6805 - val_acc: 0.5802\n",
      "Epoch 21/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6785 - acc: 0.5852 - val_loss: 0.6805 - val_acc: 0.5742\n",
      "Epoch 22/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6773 - acc: 0.5882 - val_loss: 0.6789 - val_acc: 0.5788\n",
      "Epoch 23/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6760 - acc: 0.5895 - val_loss: 0.6782 - val_acc: 0.5801\n",
      "Epoch 24/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6753 - acc: 0.5940 - val_loss: 0.6779 - val_acc: 0.5826\n",
      "Epoch 25/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6753 - acc: 0.5887 - val_loss: 0.6765 - val_acc: 0.5808\n",
      "Epoch 26/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6744 - acc: 0.5911 - val_loss: 0.6765 - val_acc: 0.5809\n",
      "Epoch 27/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6740 - acc: 0.5900 - val_loss: 0.6757 - val_acc: 0.5819\n",
      "Epoch 28/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6732 - acc: 0.5935 - val_loss: 0.6744 - val_acc: 0.5893\n",
      "Epoch 29/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6725 - acc: 0.5933 - val_loss: 0.6763 - val_acc: 0.5848\n",
      "Epoch 30/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6719 - acc: 0.5952 - val_loss: 0.6747 - val_acc: 0.5854\n",
      "Epoch 31/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6707 - acc: 0.5974 - val_loss: 0.6719 - val_acc: 0.5903\n",
      "Epoch 32/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6705 - acc: 0.5970 - val_loss: 0.6732 - val_acc: 0.5880\n",
      "Epoch 33/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6700 - acc: 0.5977 - val_loss: 0.6707 - val_acc: 0.5886\n",
      "Epoch 34/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6693 - acc: 0.5989 - val_loss: 0.6714 - val_acc: 0.5885\n",
      "Epoch 35/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6687 - acc: 0.5990 - val_loss: 0.6711 - val_acc: 0.5887\n",
      "Epoch 36/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6675 - acc: 0.6022 - val_loss: 0.6693 - val_acc: 0.5901\n",
      "Epoch 37/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6676 - acc: 0.5992 - val_loss: 0.6715 - val_acc: 0.5893\n",
      "Epoch 38/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6672 - acc: 0.6022 - val_loss: 0.6716 - val_acc: 0.5896\n",
      "Epoch 39/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6661 - acc: 0.6031 - val_loss: 0.6708 - val_acc: 0.5903\n",
      "Epoch 40/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6664 - acc: 0.6007 - val_loss: 0.6692 - val_acc: 0.5921\n",
      "Epoch 41/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6659 - acc: 0.6009 - val_loss: 0.6701 - val_acc: 0.5903\n",
      "Epoch 42/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6648 - acc: 0.6023 - val_loss: 0.6681 - val_acc: 0.5956\n",
      "Epoch 43/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6645 - acc: 0.6026 - val_loss: 0.6692 - val_acc: 0.5886\n",
      "Epoch 44/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6650 - acc: 0.6027 - val_loss: 0.6673 - val_acc: 0.5957\n",
      "Epoch 45/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6640 - acc: 0.6052 - val_loss: 0.6683 - val_acc: 0.5903\n",
      "Epoch 46/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6643 - acc: 0.6047 - val_loss: 0.6654 - val_acc: 0.5993\n",
      "Epoch 47/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6635 - acc: 0.6058 - val_loss: 0.6665 - val_acc: 0.6008\n",
      "Epoch 48/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6618 - acc: 0.6075 - val_loss: 0.6678 - val_acc: 0.5942\n",
      "Epoch 49/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6609 - acc: 0.6077 - val_loss: 0.6657 - val_acc: 0.5943\n",
      "Epoch 50/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6622 - acc: 0.6076 - val_loss: 0.6690 - val_acc: 0.5917\n",
      "Epoch 51/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6621 - acc: 0.6061 - val_loss: 0.6668 - val_acc: 0.5950\n",
      "Epoch 52/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6608 - acc: 0.6060 - val_loss: 0.6651 - val_acc: 0.6010\n",
      "Epoch 53/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6624 - acc: 0.6039 - val_loss: 0.6655 - val_acc: 0.5987\n",
      "Epoch 54/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6601 - acc: 0.6091 - val_loss: 0.6674 - val_acc: 0.5959\n",
      "Epoch 55/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6601 - acc: 0.6092 - val_loss: 0.6628 - val_acc: 0.6037\n",
      "Epoch 56/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6598 - acc: 0.6085 - val_loss: 0.6660 - val_acc: 0.6006\n",
      "Epoch 57/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6591 - acc: 0.6121 - val_loss: 0.6666 - val_acc: 0.6000\n",
      "Epoch 58/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6581 - acc: 0.6145 - val_loss: 0.6629 - val_acc: 0.6044\n",
      "Epoch 59/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6580 - acc: 0.6147 - val_loss: 0.6652 - val_acc: 0.5990\n",
      "Epoch 60/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6576 - acc: 0.6117 - val_loss: 0.6640 - val_acc: 0.6019\n",
      "Epoch 61/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6575 - acc: 0.6144 - val_loss: 0.6620 - val_acc: 0.6004\n",
      "Epoch 62/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6568 - acc: 0.6143 - val_loss: 0.6679 - val_acc: 0.5985\n",
      "Epoch 63/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6568 - acc: 0.6169 - val_loss: 0.6636 - val_acc: 0.5987\n",
      "Epoch 64/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6565 - acc: 0.6166 - val_loss: 0.6643 - val_acc: 0.5982\n",
      "Epoch 65/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6559 - acc: 0.6180 - val_loss: 0.6585 - val_acc: 0.6103\n",
      "Epoch 66/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6556 - acc: 0.6178 - val_loss: 0.6648 - val_acc: 0.5979\n",
      "Epoch 67/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6547 - acc: 0.6172 - val_loss: 0.6595 - val_acc: 0.6075\n",
      "Epoch 68/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6545 - acc: 0.6184 - val_loss: 0.6627 - val_acc: 0.6008\n",
      "Epoch 69/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6545 - acc: 0.6170 - val_loss: 0.6618 - val_acc: 0.6027\n",
      "Epoch 70/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6532 - acc: 0.6200 - val_loss: 0.6603 - val_acc: 0.6101\n",
      "Epoch 71/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6530 - acc: 0.6213 - val_loss: 0.6613 - val_acc: 0.6003\n",
      "Epoch 72/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6526 - acc: 0.6187 - val_loss: 0.6608 - val_acc: 0.6064\n",
      "Epoch 73/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6514 - acc: 0.6204 - val_loss: 0.6628 - val_acc: 0.6047\n",
      "Epoch 74/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6520 - acc: 0.6214 - val_loss: 0.6588 - val_acc: 0.6065\n",
      "Epoch 75/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6511 - acc: 0.6241 - val_loss: 0.6566 - val_acc: 0.6090\n",
      "Epoch 76/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6500 - acc: 0.6236 - val_loss: 0.6609 - val_acc: 0.6063\n",
      "Epoch 77/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6503 - acc: 0.6247 - val_loss: 0.6578 - val_acc: 0.6105\n",
      "Epoch 78/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6497 - acc: 0.6239 - val_loss: 0.6609 - val_acc: 0.6036\n",
      "Epoch 79/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6496 - acc: 0.6240 - val_loss: 0.6576 - val_acc: 0.6096\n",
      "Epoch 80/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6496 - acc: 0.6257 - val_loss: 0.6571 - val_acc: 0.6120\n",
      "Epoch 81/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6486 - acc: 0.6280 - val_loss: 0.6581 - val_acc: 0.6104\n",
      "Epoch 82/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6469 - acc: 0.6292 - val_loss: 0.6589 - val_acc: 0.6017\n",
      "Epoch 83/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6468 - acc: 0.6308 - val_loss: 0.6575 - val_acc: 0.6089\n",
      "Epoch 84/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6461 - acc: 0.6313 - val_loss: 0.6550 - val_acc: 0.6152\n",
      "Epoch 85/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6452 - acc: 0.6322 - val_loss: 0.6570 - val_acc: 0.6083\n",
      "Epoch 86/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6456 - acc: 0.6314 - val_loss: 0.6555 - val_acc: 0.6155\n",
      "Epoch 87/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6449 - acc: 0.6321 - val_loss: 0.6570 - val_acc: 0.6127\n",
      "Epoch 88/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6437 - acc: 0.6317 - val_loss: 0.6565 - val_acc: 0.6103\n",
      "Epoch 89/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6421 - acc: 0.6348 - val_loss: 0.6548 - val_acc: 0.6113\n",
      "Epoch 90/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6434 - acc: 0.6328 - val_loss: 0.6539 - val_acc: 0.6174\n",
      "Epoch 91/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6411 - acc: 0.6365 - val_loss: 0.6525 - val_acc: 0.6190\n",
      "Epoch 92/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6412 - acc: 0.6364 - val_loss: 0.6553 - val_acc: 0.6128\n",
      "Epoch 93/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6411 - acc: 0.6364 - val_loss: 0.6526 - val_acc: 0.6181\n",
      "Epoch 94/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6413 - acc: 0.6377 - val_loss: 0.6515 - val_acc: 0.6196\n",
      "Epoch 95/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6381 - acc: 0.6395 - val_loss: 0.6506 - val_acc: 0.6199\n",
      "Epoch 96/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6401 - acc: 0.6375 - val_loss: 0.6519 - val_acc: 0.6194\n",
      "Epoch 97/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6370 - acc: 0.6430 - val_loss: 0.6546 - val_acc: 0.6131\n",
      "Epoch 98/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6372 - acc: 0.6414 - val_loss: 0.6525 - val_acc: 0.6218\n",
      "Epoch 99/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6381 - acc: 0.6378 - val_loss: 0.6519 - val_acc: 0.6188\n",
      "Epoch 100/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6358 - acc: 0.6424 - val_loss: 0.6516 - val_acc: 0.6183\n",
      "Epoch 101/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6355 - acc: 0.6417 - val_loss: 0.6499 - val_acc: 0.6240\n",
      "Epoch 102/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6342 - acc: 0.6450 - val_loss: 0.6498 - val_acc: 0.6171\n",
      "Epoch 103/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6347 - acc: 0.6445 - val_loss: 0.6509 - val_acc: 0.6220\n",
      "Epoch 104/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6326 - acc: 0.6466 - val_loss: 0.6505 - val_acc: 0.6227\n",
      "Epoch 105/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6326 - acc: 0.6461 - val_loss: 0.6498 - val_acc: 0.6217\n",
      "Epoch 106/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6310 - acc: 0.6473 - val_loss: 0.6503 - val_acc: 0.6205\n",
      "Epoch 107/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6319 - acc: 0.6467 - val_loss: 0.6479 - val_acc: 0.6243\n",
      "Epoch 108/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6316 - acc: 0.6502 - val_loss: 0.6505 - val_acc: 0.6159\n",
      "Epoch 109/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6300 - acc: 0.6508 - val_loss: 0.6517 - val_acc: 0.6195\n",
      "Epoch 110/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6301 - acc: 0.6479 - val_loss: 0.6493 - val_acc: 0.6195\n",
      "Epoch 111/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6298 - acc: 0.6488 - val_loss: 0.6503 - val_acc: 0.6247\n",
      "Epoch 112/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6289 - acc: 0.6488 - val_loss: 0.6515 - val_acc: 0.6240\n",
      "Epoch 113/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6275 - acc: 0.6509 - val_loss: 0.6494 - val_acc: 0.6219\n",
      "Epoch 114/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6277 - acc: 0.6508 - val_loss: 0.6481 - val_acc: 0.6174\n",
      "Epoch 115/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6264 - acc: 0.6537 - val_loss: 0.6503 - val_acc: 0.6191\n",
      "Epoch 116/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6270 - acc: 0.6544 - val_loss: 0.6468 - val_acc: 0.6212\n",
      "Epoch 117/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6260 - acc: 0.6526 - val_loss: 0.6462 - val_acc: 0.6251\n",
      "Epoch 118/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6253 - acc: 0.6558 - val_loss: 0.6470 - val_acc: 0.6255\n",
      "Epoch 119/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6257 - acc: 0.6547 - val_loss: 0.6451 - val_acc: 0.6272\n",
      "Epoch 120/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6237 - acc: 0.6558 - val_loss: 0.6445 - val_acc: 0.6235\n",
      "Epoch 121/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6239 - acc: 0.6569 - val_loss: 0.6466 - val_acc: 0.6227\n",
      "Epoch 122/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6222 - acc: 0.6575 - val_loss: 0.6457 - val_acc: 0.6263\n",
      "Epoch 123/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6217 - acc: 0.6564 - val_loss: 0.6467 - val_acc: 0.6222\n",
      "Epoch 124/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6217 - acc: 0.6575 - val_loss: 0.6434 - val_acc: 0.6247\n",
      "Epoch 125/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6214 - acc: 0.6614 - val_loss: 0.6449 - val_acc: 0.6284\n",
      "Epoch 126/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6205 - acc: 0.6614 - val_loss: 0.6434 - val_acc: 0.6257\n",
      "Epoch 127/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6197 - acc: 0.6588 - val_loss: 0.6450 - val_acc: 0.6249\n",
      "Epoch 128/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6206 - acc: 0.6576 - val_loss: 0.6454 - val_acc: 0.6280\n",
      "Epoch 129/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280/1280 [==============================] - 0s - loss: 0.6181 - acc: 0.6625 - val_loss: 0.6467 - val_acc: 0.6280\n",
      "Epoch 130/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6190 - acc: 0.6612 - val_loss: 0.6440 - val_acc: 0.6276\n",
      "Epoch 131/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6178 - acc: 0.6622 - val_loss: 0.6450 - val_acc: 0.6249\n",
      "Epoch 132/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6168 - acc: 0.6662 - val_loss: 0.6423 - val_acc: 0.6295\n",
      "Epoch 133/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6171 - acc: 0.6617 - val_loss: 0.6405 - val_acc: 0.6315\n",
      "Epoch 134/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6165 - acc: 0.6640 - val_loss: 0.6427 - val_acc: 0.6302\n",
      "Epoch 135/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6160 - acc: 0.6624 - val_loss: 0.6404 - val_acc: 0.6364\n",
      "Epoch 136/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6153 - acc: 0.6635 - val_loss: 0.6448 - val_acc: 0.6242\n",
      "Epoch 137/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6148 - acc: 0.6650 - val_loss: 0.6429 - val_acc: 0.6239\n",
      "Epoch 138/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6139 - acc: 0.6678 - val_loss: 0.6433 - val_acc: 0.6278\n",
      "Epoch 139/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6135 - acc: 0.6690 - val_loss: 0.6444 - val_acc: 0.6251\n",
      "Epoch 140/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6119 - acc: 0.6678 - val_loss: 0.6451 - val_acc: 0.6291\n",
      "Epoch 141/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6122 - acc: 0.6669 - val_loss: 0.6454 - val_acc: 0.6266\n",
      "Epoch 142/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6115 - acc: 0.6693 - val_loss: 0.6431 - val_acc: 0.6250\n",
      "Epoch 143/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6117 - acc: 0.6677 - val_loss: 0.6426 - val_acc: 0.6275\n",
      "Epoch 144/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6105 - acc: 0.6718 - val_loss: 0.6433 - val_acc: 0.6303\n",
      "Epoch 145/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6101 - acc: 0.6695 - val_loss: 0.6454 - val_acc: 0.6293\n",
      "Epoch 146/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6093 - acc: 0.6712 - val_loss: 0.6428 - val_acc: 0.6302\n",
      "Epoch 147/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6095 - acc: 0.6710 - val_loss: 0.6430 - val_acc: 0.6307\n",
      "Epoch 148/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6086 - acc: 0.6729 - val_loss: 0.6430 - val_acc: 0.6302\n",
      "Epoch 149/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6092 - acc: 0.6688 - val_loss: 0.6424 - val_acc: 0.6317\n",
      "Epoch 150/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6077 - acc: 0.6725 - val_loss: 0.6414 - val_acc: 0.6340\n",
      "Epoch 151/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6077 - acc: 0.6734 - val_loss: 0.6457 - val_acc: 0.6258\n",
      "Epoch 152/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6060 - acc: 0.6753 - val_loss: 0.6420 - val_acc: 0.6296\n",
      "Epoch 153/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6051 - acc: 0.6729 - val_loss: 0.6420 - val_acc: 0.6302\n",
      "Epoch 154/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6034 - acc: 0.6758 - val_loss: 0.6458 - val_acc: 0.6340\n",
      "Epoch 155/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6048 - acc: 0.6739 - val_loss: 0.6434 - val_acc: 0.6330\n",
      "Epoch 156/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6043 - acc: 0.6744 - val_loss: 0.6435 - val_acc: 0.6340\n",
      "Epoch 157/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6056 - acc: 0.6743 - val_loss: 0.6425 - val_acc: 0.6255\n",
      "Epoch 158/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6038 - acc: 0.6750 - val_loss: 0.6416 - val_acc: 0.6329\n",
      "Epoch 159/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6035 - acc: 0.6757 - val_loss: 0.6444 - val_acc: 0.6336\n",
      "Epoch 160/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6022 - acc: 0.6781 - val_loss: 0.6427 - val_acc: 0.6327\n",
      "Epoch 161/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6020 - acc: 0.6778 - val_loss: 0.6436 - val_acc: 0.6336\n",
      "Epoch 162/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6037 - acc: 0.6745 - val_loss: 0.6428 - val_acc: 0.6274\n",
      "Epoch 163/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6031 - acc: 0.6779 - val_loss: 0.6410 - val_acc: 0.6313\n",
      "Epoch 164/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6020 - acc: 0.6784 - val_loss: 0.6462 - val_acc: 0.6314\n",
      "Epoch 165/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6009 - acc: 0.6794 - val_loss: 0.6476 - val_acc: 0.6272\n",
      "Epoch 166/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6003 - acc: 0.6773 - val_loss: 0.6447 - val_acc: 0.6319\n",
      "Epoch 167/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5988 - acc: 0.6827 - val_loss: 0.6428 - val_acc: 0.6309\n",
      "Epoch 168/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.6015 - acc: 0.6769 - val_loss: 0.6411 - val_acc: 0.6360\n",
      "Epoch 169/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5979 - acc: 0.6818 - val_loss: 0.6435 - val_acc: 0.6322\n",
      "Epoch 170/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5987 - acc: 0.6791 - val_loss: 0.6444 - val_acc: 0.6342\n",
      "Epoch 171/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5982 - acc: 0.6829 - val_loss: 0.6386 - val_acc: 0.6382\n",
      "Epoch 172/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5992 - acc: 0.6814 - val_loss: 0.6440 - val_acc: 0.6312\n",
      "Epoch 173/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5971 - acc: 0.6844 - val_loss: 0.6439 - val_acc: 0.6353\n",
      "Epoch 174/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5968 - acc: 0.6829 - val_loss: 0.6429 - val_acc: 0.6295\n",
      "Epoch 175/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5989 - acc: 0.6843 - val_loss: 0.6440 - val_acc: 0.6338\n",
      "Epoch 176/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5978 - acc: 0.6824 - val_loss: 0.6450 - val_acc: 0.6348\n",
      "Epoch 177/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5956 - acc: 0.6853 - val_loss: 0.6431 - val_acc: 0.6359\n",
      "Epoch 178/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5970 - acc: 0.6818 - val_loss: 0.6435 - val_acc: 0.6384\n",
      "Epoch 179/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5950 - acc: 0.6855 - val_loss: 0.6477 - val_acc: 0.6321\n",
      "Epoch 180/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5960 - acc: 0.6832 - val_loss: 0.6405 - val_acc: 0.6376\n",
      "Epoch 181/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5960 - acc: 0.6851 - val_loss: 0.6482 - val_acc: 0.6302\n",
      "Epoch 182/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5949 - acc: 0.6831 - val_loss: 0.6444 - val_acc: 0.6310\n",
      "Epoch 183/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5944 - acc: 0.6839 - val_loss: 0.6458 - val_acc: 0.6318\n",
      "Epoch 184/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5916 - acc: 0.6879 - val_loss: 0.6431 - val_acc: 0.6301\n",
      "Epoch 185/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5914 - acc: 0.6866 - val_loss: 0.6420 - val_acc: 0.6360\n",
      "Epoch 186/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5947 - acc: 0.6849 - val_loss: 0.6456 - val_acc: 0.6295\n",
      "Epoch 187/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5896 - acc: 0.6906 - val_loss: 0.6463 - val_acc: 0.6322\n",
      "Epoch 188/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5906 - acc: 0.6877 - val_loss: 0.6432 - val_acc: 0.6399\n",
      "Epoch 189/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5915 - acc: 0.6889 - val_loss: 0.6442 - val_acc: 0.6401\n",
      "Epoch 190/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5926 - acc: 0.6893 - val_loss: 0.6450 - val_acc: 0.6397\n",
      "Epoch 191/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5892 - acc: 0.6889 - val_loss: 0.6465 - val_acc: 0.6378\n",
      "Epoch 192/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5910 - acc: 0.6888 - val_loss: 0.6451 - val_acc: 0.6334\n",
      "Epoch 193/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5895 - acc: 0.6918 - val_loss: 0.6429 - val_acc: 0.6401\n",
      "Epoch 194/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5884 - acc: 0.6910 - val_loss: 0.6433 - val_acc: 0.6353\n",
      "Epoch 195/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5892 - acc: 0.6891 - val_loss: 0.6438 - val_acc: 0.6397\n",
      "Epoch 196/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5899 - acc: 0.6914 - val_loss: 0.6413 - val_acc: 0.6359\n",
      "Epoch 197/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5879 - acc: 0.6921 - val_loss: 0.6408 - val_acc: 0.6427\n",
      "Epoch 198/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5875 - acc: 0.6917 - val_loss: 0.6422 - val_acc: 0.6415\n",
      "Epoch 199/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5858 - acc: 0.6909 - val_loss: 0.6439 - val_acc: 0.6335\n",
      "Epoch 200/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5871 - acc: 0.6920 - val_loss: 0.6411 - val_acc: 0.6365\n",
      "Epoch 201/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5857 - acc: 0.6937 - val_loss: 0.6455 - val_acc: 0.6379\n",
      "Epoch 202/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5858 - acc: 0.6922 - val_loss: 0.6483 - val_acc: 0.6383\n",
      "Epoch 203/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5858 - acc: 0.6940 - val_loss: 0.6439 - val_acc: 0.6430\n",
      "Epoch 204/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5863 - acc: 0.6931 - val_loss: 0.6446 - val_acc: 0.6363\n",
      "Epoch 205/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5867 - acc: 0.6914 - val_loss: 0.6433 - val_acc: 0.6432\n",
      "Epoch 206/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5844 - acc: 0.6960 - val_loss: 0.6471 - val_acc: 0.6390\n",
      "Epoch 207/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5852 - acc: 0.6948 - val_loss: 0.6493 - val_acc: 0.6338\n",
      "Epoch 208/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5845 - acc: 0.6940 - val_loss: 0.6459 - val_acc: 0.6352\n",
      "Epoch 209/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5856 - acc: 0.6946 - val_loss: 0.6475 - val_acc: 0.6312\n",
      "Epoch 210/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5830 - acc: 0.6966 - val_loss: 0.6470 - val_acc: 0.6351\n",
      "Epoch 211/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5823 - acc: 0.6952 - val_loss: 0.6475 - val_acc: 0.6349\n",
      "Epoch 212/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5807 - acc: 0.6960 - val_loss: 0.6467 - val_acc: 0.6350\n",
      "Epoch 213/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5837 - acc: 0.6957 - val_loss: 0.6460 - val_acc: 0.6392\n",
      "Epoch 214/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5810 - acc: 0.7007 - val_loss: 0.6440 - val_acc: 0.6380\n",
      "Epoch 215/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5808 - acc: 0.7000 - val_loss: 0.6441 - val_acc: 0.6394\n",
      "Epoch 216/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5830 - acc: 0.6953 - val_loss: 0.6473 - val_acc: 0.6420\n",
      "Epoch 217/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5808 - acc: 0.6966 - val_loss: 0.6474 - val_acc: 0.6390\n",
      "Epoch 218/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5819 - acc: 0.6968 - val_loss: 0.6472 - val_acc: 0.6336\n",
      "Epoch 219/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5800 - acc: 0.6988 - val_loss: 0.6462 - val_acc: 0.6403\n",
      "Epoch 220/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5797 - acc: 0.6982 - val_loss: 0.6492 - val_acc: 0.6342\n",
      "Epoch 221/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5795 - acc: 0.6994 - val_loss: 0.6487 - val_acc: 0.6392\n",
      "Epoch 222/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5772 - acc: 0.7019 - val_loss: 0.6488 - val_acc: 0.6334\n",
      "Epoch 223/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5764 - acc: 0.7021 - val_loss: 0.6484 - val_acc: 0.6374\n",
      "Epoch 224/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5801 - acc: 0.6989 - val_loss: 0.6456 - val_acc: 0.6383\n",
      "Epoch 225/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5784 - acc: 0.6983 - val_loss: 0.6450 - val_acc: 0.6413\n",
      "Epoch 226/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5760 - acc: 0.7027 - val_loss: 0.6499 - val_acc: 0.6349\n",
      "Epoch 227/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5766 - acc: 0.7023 - val_loss: 0.6478 - val_acc: 0.6401\n",
      "Epoch 228/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5743 - acc: 0.7001 - val_loss: 0.6504 - val_acc: 0.6306\n",
      "Epoch 229/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5754 - acc: 0.7047 - val_loss: 0.6567 - val_acc: 0.6367\n",
      "Epoch 230/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5749 - acc: 0.7023 - val_loss: 0.6508 - val_acc: 0.6333\n",
      "Epoch 231/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5754 - acc: 0.7019 - val_loss: 0.6457 - val_acc: 0.6390\n",
      "Epoch 232/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5728 - acc: 0.7042 - val_loss: 0.6477 - val_acc: 0.6398\n",
      "Epoch 233/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5748 - acc: 0.7021 - val_loss: 0.6538 - val_acc: 0.6353\n",
      "Epoch 234/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5734 - acc: 0.7041 - val_loss: 0.6501 - val_acc: 0.6380\n",
      "Epoch 235/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5740 - acc: 0.7051 - val_loss: 0.6534 - val_acc: 0.6348\n",
      "Epoch 236/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5730 - acc: 0.7043 - val_loss: 0.6536 - val_acc: 0.6349\n",
      "Epoch 237/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5735 - acc: 0.7046 - val_loss: 0.6516 - val_acc: 0.6319\n",
      "Epoch 238/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5734 - acc: 0.7048 - val_loss: 0.6510 - val_acc: 0.6366\n",
      "Epoch 239/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5694 - acc: 0.7075 - val_loss: 0.6534 - val_acc: 0.6365\n",
      "Epoch 240/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5736 - acc: 0.7055 - val_loss: 0.6518 - val_acc: 0.6388\n",
      "Epoch 241/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5712 - acc: 0.7057 - val_loss: 0.6502 - val_acc: 0.6372\n",
      "Epoch 242/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5706 - acc: 0.7069 - val_loss: 0.6533 - val_acc: 0.6336\n",
      "Epoch 243/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5689 - acc: 0.7081 - val_loss: 0.6527 - val_acc: 0.6378\n",
      "Epoch 244/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5692 - acc: 0.7065 - val_loss: 0.6505 - val_acc: 0.6388\n",
      "Epoch 245/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5676 - acc: 0.7094 - val_loss: 0.6530 - val_acc: 0.6382\n",
      "Epoch 246/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5689 - acc: 0.7071 - val_loss: 0.6523 - val_acc: 0.6379\n",
      "Epoch 247/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5679 - acc: 0.7115 - val_loss: 0.6521 - val_acc: 0.6407\n",
      "Epoch 248/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5684 - acc: 0.7077 - val_loss: 0.6574 - val_acc: 0.6412\n",
      "Epoch 249/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5706 - acc: 0.7076 - val_loss: 0.6548 - val_acc: 0.6340\n",
      "Epoch 250/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5672 - acc: 0.7094 - val_loss: 0.6544 - val_acc: 0.6371\n",
      "Epoch 251/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5678 - acc: 0.7062 - val_loss: 0.6513 - val_acc: 0.6373\n",
      "Epoch 252/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5672 - acc: 0.7116 - val_loss: 0.6535 - val_acc: 0.6350\n",
      "Epoch 253/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5663 - acc: 0.7128 - val_loss: 0.6540 - val_acc: 0.6388\n",
      "Epoch 254/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5661 - acc: 0.7105 - val_loss: 0.6563 - val_acc: 0.6403\n",
      "Epoch 255/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5658 - acc: 0.7119 - val_loss: 0.6520 - val_acc: 0.6396\n",
      "Epoch 256/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5650 - acc: 0.7112 - val_loss: 0.6517 - val_acc: 0.6389\n",
      "Epoch 257/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280/1280 [==============================] - 0s - loss: 0.5668 - acc: 0.7101 - val_loss: 0.6550 - val_acc: 0.6343\n",
      "Epoch 258/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5670 - acc: 0.7121 - val_loss: 0.6616 - val_acc: 0.6350\n",
      "Epoch 259/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5650 - acc: 0.7116 - val_loss: 0.6548 - val_acc: 0.6368\n",
      "Epoch 260/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5658 - acc: 0.7095 - val_loss: 0.6564 - val_acc: 0.6390\n",
      "Epoch 261/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5612 - acc: 0.7135 - val_loss: 0.6533 - val_acc: 0.6435\n",
      "Epoch 262/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5619 - acc: 0.7133 - val_loss: 0.6561 - val_acc: 0.6349\n",
      "Epoch 263/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5626 - acc: 0.7115 - val_loss: 0.6580 - val_acc: 0.6351\n",
      "Epoch 264/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5623 - acc: 0.7152 - val_loss: 0.6571 - val_acc: 0.6337\n",
      "Epoch 265/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5621 - acc: 0.7127 - val_loss: 0.6594 - val_acc: 0.6345\n",
      "Epoch 266/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5611 - acc: 0.7151 - val_loss: 0.6562 - val_acc: 0.6371\n",
      "Epoch 267/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5606 - acc: 0.7177 - val_loss: 0.6560 - val_acc: 0.6372\n",
      "Epoch 268/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5580 - acc: 0.7175 - val_loss: 0.6550 - val_acc: 0.6348\n",
      "Epoch 269/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5602 - acc: 0.7158 - val_loss: 0.6625 - val_acc: 0.6390\n",
      "Epoch 270/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5601 - acc: 0.7154 - val_loss: 0.6600 - val_acc: 0.6369\n",
      "Epoch 271/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5578 - acc: 0.7201 - val_loss: 0.6570 - val_acc: 0.6384\n",
      "Epoch 272/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5585 - acc: 0.7167 - val_loss: 0.6614 - val_acc: 0.6368\n",
      "Epoch 273/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5591 - acc: 0.7151 - val_loss: 0.6575 - val_acc: 0.6392\n",
      "Epoch 274/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5576 - acc: 0.7188 - val_loss: 0.6562 - val_acc: 0.6372\n",
      "Epoch 275/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5595 - acc: 0.7154 - val_loss: 0.6591 - val_acc: 0.6374\n",
      "Epoch 276/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5576 - acc: 0.7193 - val_loss: 0.6628 - val_acc: 0.6364\n",
      "Epoch 277/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5568 - acc: 0.7190 - val_loss: 0.6602 - val_acc: 0.6387\n",
      "Epoch 278/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5567 - acc: 0.7206 - val_loss: 0.6628 - val_acc: 0.6395\n",
      "Epoch 279/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5550 - acc: 0.7219 - val_loss: 0.6659 - val_acc: 0.6378\n",
      "Epoch 280/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5563 - acc: 0.7196 - val_loss: 0.6614 - val_acc: 0.6288\n",
      "Epoch 281/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5564 - acc: 0.7188 - val_loss: 0.6604 - val_acc: 0.6371\n",
      "Epoch 282/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5558 - acc: 0.7161 - val_loss: 0.6657 - val_acc: 0.6349\n",
      "Epoch 283/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5550 - acc: 0.7203 - val_loss: 0.6618 - val_acc: 0.6357\n",
      "Epoch 284/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5574 - acc: 0.7188 - val_loss: 0.6583 - val_acc: 0.6405\n",
      "Epoch 285/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5553 - acc: 0.7208 - val_loss: 0.6586 - val_acc: 0.6397\n",
      "Epoch 286/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5541 - acc: 0.7196 - val_loss: 0.6613 - val_acc: 0.6366\n",
      "Epoch 287/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5534 - acc: 0.7200 - val_loss: 0.6678 - val_acc: 0.6302\n",
      "Epoch 288/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5521 - acc: 0.7220 - val_loss: 0.6610 - val_acc: 0.6388\n",
      "Epoch 289/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5542 - acc: 0.7201 - val_loss: 0.6649 - val_acc: 0.6397\n",
      "Epoch 290/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5526 - acc: 0.7216 - val_loss: 0.6612 - val_acc: 0.6389\n",
      "Epoch 291/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5559 - acc: 0.7211 - val_loss: 0.6639 - val_acc: 0.6390\n",
      "Epoch 292/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5531 - acc: 0.7188 - val_loss: 0.6617 - val_acc: 0.6309\n",
      "Epoch 293/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5519 - acc: 0.7220 - val_loss: 0.6606 - val_acc: 0.6348\n",
      "Epoch 294/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5532 - acc: 0.7231 - val_loss: 0.6640 - val_acc: 0.6355\n",
      "Epoch 295/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5516 - acc: 0.7230 - val_loss: 0.6684 - val_acc: 0.6337\n",
      "Epoch 296/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5509 - acc: 0.7231 - val_loss: 0.6636 - val_acc: 0.6386\n",
      "Epoch 297/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5491 - acc: 0.7232 - val_loss: 0.6655 - val_acc: 0.6357\n",
      "Epoch 298/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5473 - acc: 0.7257 - val_loss: 0.6652 - val_acc: 0.6355\n",
      "Epoch 299/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5477 - acc: 0.7243 - val_loss: 0.6664 - val_acc: 0.6343\n",
      "Epoch 300/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5494 - acc: 0.7263 - val_loss: 0.6677 - val_acc: 0.6364\n",
      "Epoch 301/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5486 - acc: 0.7261 - val_loss: 0.6666 - val_acc: 0.6386\n",
      "Epoch 302/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5470 - acc: 0.7265 - val_loss: 0.6674 - val_acc: 0.6330\n",
      "Epoch 303/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5478 - acc: 0.7268 - val_loss: 0.6640 - val_acc: 0.6376\n",
      "Epoch 304/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5485 - acc: 0.7239 - val_loss: 0.6690 - val_acc: 0.6341\n",
      "Epoch 305/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5474 - acc: 0.7256 - val_loss: 0.6652 - val_acc: 0.6338\n",
      "Epoch 306/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5458 - acc: 0.7280 - val_loss: 0.6687 - val_acc: 0.6345\n",
      "Epoch 307/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5474 - acc: 0.7276 - val_loss: 0.6695 - val_acc: 0.6344\n",
      "Epoch 308/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5454 - acc: 0.7313 - val_loss: 0.6694 - val_acc: 0.6360\n",
      "Epoch 309/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5465 - acc: 0.7267 - val_loss: 0.6661 - val_acc: 0.6365\n",
      "Epoch 310/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5438 - acc: 0.7290 - val_loss: 0.6689 - val_acc: 0.6299\n",
      "Epoch 311/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5453 - acc: 0.7246 - val_loss: 0.6691 - val_acc: 0.6347\n",
      "Epoch 312/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5457 - acc: 0.7288 - val_loss: 0.6702 - val_acc: 0.6318\n",
      "Epoch 313/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5449 - acc: 0.7258 - val_loss: 0.6703 - val_acc: 0.6348\n",
      "Epoch 314/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5442 - acc: 0.7295 - val_loss: 0.6718 - val_acc: 0.6317\n",
      "Epoch 315/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5438 - acc: 0.7303 - val_loss: 0.6656 - val_acc: 0.6392\n",
      "Epoch 316/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5430 - acc: 0.7288 - val_loss: 0.6671 - val_acc: 0.6423\n",
      "Epoch 317/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5442 - acc: 0.7279 - val_loss: 0.6648 - val_acc: 0.6363\n",
      "Epoch 318/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5422 - acc: 0.7304 - val_loss: 0.6676 - val_acc: 0.6361\n",
      "Epoch 319/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5418 - acc: 0.7299 - val_loss: 0.6674 - val_acc: 0.6345\n",
      "Epoch 320/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5422 - acc: 0.7315 - val_loss: 0.6720 - val_acc: 0.6369\n",
      "Epoch 321/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5404 - acc: 0.7339 - val_loss: 0.6723 - val_acc: 0.6382\n",
      "Epoch 322/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5429 - acc: 0.7320 - val_loss: 0.6683 - val_acc: 0.6341\n",
      "Epoch 323/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5429 - acc: 0.7302 - val_loss: 0.6759 - val_acc: 0.6352\n",
      "Epoch 324/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5397 - acc: 0.7306 - val_loss: 0.6738 - val_acc: 0.6293\n",
      "Epoch 325/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5384 - acc: 0.7335 - val_loss: 0.6745 - val_acc: 0.6383\n",
      "Epoch 326/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5382 - acc: 0.7347 - val_loss: 0.6748 - val_acc: 0.6325\n",
      "Epoch 327/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5377 - acc: 0.7353 - val_loss: 0.6705 - val_acc: 0.6372\n",
      "Epoch 328/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5397 - acc: 0.7324 - val_loss: 0.6816 - val_acc: 0.6306\n",
      "Epoch 329/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5412 - acc: 0.7311 - val_loss: 0.6738 - val_acc: 0.6376\n",
      "Epoch 330/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5358 - acc: 0.7357 - val_loss: 0.6756 - val_acc: 0.6303\n",
      "Epoch 331/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5373 - acc: 0.7300 - val_loss: 0.6755 - val_acc: 0.6388\n",
      "Epoch 332/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5409 - acc: 0.7330 - val_loss: 0.6707 - val_acc: 0.6344\n",
      "Epoch 333/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5378 - acc: 0.7336 - val_loss: 0.6772 - val_acc: 0.6372\n",
      "Epoch 334/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5384 - acc: 0.7338 - val_loss: 0.6745 - val_acc: 0.6382\n",
      "Epoch 335/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5363 - acc: 0.7369 - val_loss: 0.6769 - val_acc: 0.6324\n",
      "Epoch 336/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5355 - acc: 0.7380 - val_loss: 0.6826 - val_acc: 0.6352\n",
      "Epoch 337/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5349 - acc: 0.7384 - val_loss: 0.6769 - val_acc: 0.6401\n",
      "Epoch 338/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5342 - acc: 0.7346 - val_loss: 0.6753 - val_acc: 0.6383\n",
      "Epoch 339/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5371 - acc: 0.7343 - val_loss: 0.6725 - val_acc: 0.6355\n",
      "Epoch 340/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5340 - acc: 0.7377 - val_loss: 0.6717 - val_acc: 0.6433\n",
      "Epoch 341/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5316 - acc: 0.7382 - val_loss: 0.6784 - val_acc: 0.6342\n",
      "Epoch 342/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5340 - acc: 0.7369 - val_loss: 0.6814 - val_acc: 0.6364\n",
      "Epoch 343/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5346 - acc: 0.7369 - val_loss: 0.6774 - val_acc: 0.6345\n",
      "Epoch 344/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5337 - acc: 0.7355 - val_loss: 0.6791 - val_acc: 0.6336\n",
      "Epoch 345/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5301 - acc: 0.7414 - val_loss: 0.6811 - val_acc: 0.6328\n",
      "Epoch 346/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5373 - acc: 0.7370 - val_loss: 0.6760 - val_acc: 0.6314\n",
      "Epoch 347/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5357 - acc: 0.7352 - val_loss: 0.6788 - val_acc: 0.6299\n",
      "Epoch 348/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5323 - acc: 0.7386 - val_loss: 0.6738 - val_acc: 0.6401\n",
      "Epoch 349/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5319 - acc: 0.7397 - val_loss: 0.6753 - val_acc: 0.6379\n",
      "Epoch 350/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5298 - acc: 0.7431 - val_loss: 0.6794 - val_acc: 0.6389\n",
      "Epoch 351/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5286 - acc: 0.7407 - val_loss: 0.6757 - val_acc: 0.6335\n",
      "Epoch 352/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5308 - acc: 0.7391 - val_loss: 0.6833 - val_acc: 0.6372\n",
      "Epoch 353/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5325 - acc: 0.7384 - val_loss: 0.6812 - val_acc: 0.6410\n",
      "Epoch 354/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5314 - acc: 0.7392 - val_loss: 0.6783 - val_acc: 0.6387\n",
      "Epoch 355/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5303 - acc: 0.7383 - val_loss: 0.6773 - val_acc: 0.6356\n",
      "Epoch 356/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5319 - acc: 0.7389 - val_loss: 0.6796 - val_acc: 0.6343\n",
      "Epoch 357/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5298 - acc: 0.7404 - val_loss: 0.6856 - val_acc: 0.6295\n",
      "Epoch 358/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5290 - acc: 0.7425 - val_loss: 0.6892 - val_acc: 0.6282\n",
      "Epoch 359/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5285 - acc: 0.7412 - val_loss: 0.6850 - val_acc: 0.6321\n",
      "Epoch 360/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5284 - acc: 0.7400 - val_loss: 0.6833 - val_acc: 0.6298\n",
      "Epoch 361/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5302 - acc: 0.7398 - val_loss: 0.6848 - val_acc: 0.6325\n",
      "Epoch 362/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5253 - acc: 0.7429 - val_loss: 0.6885 - val_acc: 0.6319\n",
      "Epoch 363/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5296 - acc: 0.7413 - val_loss: 0.6801 - val_acc: 0.6325\n",
      "Epoch 364/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5267 - acc: 0.7405 - val_loss: 0.6773 - val_acc: 0.6435\n",
      "Epoch 365/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5276 - acc: 0.7419 - val_loss: 0.6911 - val_acc: 0.6341\n",
      "Epoch 366/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5281 - acc: 0.7417 - val_loss: 0.6834 - val_acc: 0.6320\n",
      "Epoch 367/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5248 - acc: 0.7422 - val_loss: 0.6835 - val_acc: 0.6283\n",
      "Epoch 368/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5249 - acc: 0.7423 - val_loss: 0.6833 - val_acc: 0.6318\n",
      "Epoch 369/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5277 - acc: 0.7410 - val_loss: 0.6888 - val_acc: 0.6381\n",
      "Epoch 370/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5254 - acc: 0.7434 - val_loss: 0.6884 - val_acc: 0.6326\n",
      "Epoch 371/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5226 - acc: 0.7455 - val_loss: 0.6873 - val_acc: 0.6384\n",
      "Epoch 372/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5247 - acc: 0.7447 - val_loss: 0.6915 - val_acc: 0.6302\n",
      "Epoch 373/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5234 - acc: 0.7442 - val_loss: 0.6853 - val_acc: 0.6343\n",
      "Epoch 374/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5229 - acc: 0.7440 - val_loss: 0.6843 - val_acc: 0.6343\n",
      "Epoch 375/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5227 - acc: 0.7449 - val_loss: 0.6896 - val_acc: 0.6301\n",
      "Epoch 376/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5212 - acc: 0.7467 - val_loss: 0.6919 - val_acc: 0.6301\n",
      "Epoch 377/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5221 - acc: 0.7435 - val_loss: 0.6909 - val_acc: 0.6325\n",
      "Epoch 378/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5227 - acc: 0.7450 - val_loss: 0.6853 - val_acc: 0.6348\n",
      "Epoch 379/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5230 - acc: 0.7450 - val_loss: 0.6838 - val_acc: 0.6318\n",
      "Epoch 380/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5211 - acc: 0.7487 - val_loss: 0.6892 - val_acc: 0.6314\n",
      "Epoch 381/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5214 - acc: 0.7456 - val_loss: 0.6865 - val_acc: 0.6382\n",
      "Epoch 382/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5235 - acc: 0.7466 - val_loss: 0.6929 - val_acc: 0.6340\n",
      "Epoch 383/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5212 - acc: 0.7453 - val_loss: 0.6936 - val_acc: 0.6261\n",
      "Epoch 384/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5191 - acc: 0.7485 - val_loss: 0.6964 - val_acc: 0.6299\n",
      "Epoch 385/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280/1280 [==============================] - 0s - loss: 0.5156 - acc: 0.7492 - val_loss: 0.6866 - val_acc: 0.6321\n",
      "Epoch 386/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5186 - acc: 0.7489 - val_loss: 0.6876 - val_acc: 0.6320\n",
      "Epoch 387/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5156 - acc: 0.7527 - val_loss: 0.6901 - val_acc: 0.6296\n",
      "Epoch 388/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5211 - acc: 0.7503 - val_loss: 0.6950 - val_acc: 0.6399\n",
      "Epoch 389/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5187 - acc: 0.7509 - val_loss: 0.6885 - val_acc: 0.6349\n",
      "Epoch 390/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5209 - acc: 0.7464 - val_loss: 0.6884 - val_acc: 0.6363\n",
      "Epoch 391/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5212 - acc: 0.7460 - val_loss: 0.6939 - val_acc: 0.6366\n",
      "Epoch 392/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5174 - acc: 0.7511 - val_loss: 0.6951 - val_acc: 0.6335\n",
      "Epoch 393/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5194 - acc: 0.7492 - val_loss: 0.6997 - val_acc: 0.6336\n",
      "Epoch 394/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5167 - acc: 0.7538 - val_loss: 0.6941 - val_acc: 0.6298\n",
      "Epoch 395/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5177 - acc: 0.7466 - val_loss: 0.6892 - val_acc: 0.6333\n",
      "Epoch 396/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5166 - acc: 0.7500 - val_loss: 0.6923 - val_acc: 0.6332\n",
      "Epoch 397/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5154 - acc: 0.7513 - val_loss: 0.6990 - val_acc: 0.6302\n",
      "Epoch 398/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5158 - acc: 0.7506 - val_loss: 0.6942 - val_acc: 0.6360\n",
      "Epoch 399/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5166 - acc: 0.7493 - val_loss: 0.7028 - val_acc: 0.6265\n",
      "Epoch 400/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5182 - acc: 0.7509 - val_loss: 0.6973 - val_acc: 0.6333\n",
      "Epoch 401/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5138 - acc: 0.7511 - val_loss: 0.6940 - val_acc: 0.6357\n",
      "Epoch 402/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5132 - acc: 0.7529 - val_loss: 0.6944 - val_acc: 0.6343\n",
      "Epoch 403/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5127 - acc: 0.7522 - val_loss: 0.6987 - val_acc: 0.6284\n",
      "Epoch 404/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5122 - acc: 0.7521 - val_loss: 0.6952 - val_acc: 0.6365\n",
      "Epoch 405/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5111 - acc: 0.7548 - val_loss: 0.6956 - val_acc: 0.6330\n",
      "Epoch 406/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5160 - acc: 0.7482 - val_loss: 0.7007 - val_acc: 0.6240\n",
      "Epoch 407/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5122 - acc: 0.7526 - val_loss: 0.6981 - val_acc: 0.6337\n",
      "Epoch 408/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5150 - acc: 0.7509 - val_loss: 0.6956 - val_acc: 0.6361\n",
      "Epoch 409/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5112 - acc: 0.7550 - val_loss: 0.6965 - val_acc: 0.6347\n",
      "Epoch 410/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5114 - acc: 0.7525 - val_loss: 0.7019 - val_acc: 0.6364\n",
      "Epoch 411/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5127 - acc: 0.7538 - val_loss: 0.6966 - val_acc: 0.6381\n",
      "Epoch 412/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5097 - acc: 0.7545 - val_loss: 0.6965 - val_acc: 0.6301\n",
      "Epoch 413/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5098 - acc: 0.7543 - val_loss: 0.7042 - val_acc: 0.6280\n",
      "Epoch 414/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5095 - acc: 0.7563 - val_loss: 0.7052 - val_acc: 0.6312\n",
      "Epoch 415/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5128 - acc: 0.7519 - val_loss: 0.7005 - val_acc: 0.6322\n",
      "Epoch 416/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5103 - acc: 0.7576 - val_loss: 0.6985 - val_acc: 0.6337\n",
      "Epoch 417/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5078 - acc: 0.7565 - val_loss: 0.7013 - val_acc: 0.6290\n",
      "Epoch 418/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5076 - acc: 0.7572 - val_loss: 0.7050 - val_acc: 0.6248\n",
      "Epoch 419/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5106 - acc: 0.7507 - val_loss: 0.6991 - val_acc: 0.6336\n",
      "Epoch 420/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5035 - acc: 0.7596 - val_loss: 0.7039 - val_acc: 0.6297\n",
      "Epoch 421/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5051 - acc: 0.7570 - val_loss: 0.7010 - val_acc: 0.6335\n",
      "Epoch 422/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5073 - acc: 0.7547 - val_loss: 0.6987 - val_acc: 0.6359\n",
      "Epoch 423/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5066 - acc: 0.7579 - val_loss: 0.6994 - val_acc: 0.6314\n",
      "Epoch 424/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5082 - acc: 0.7571 - val_loss: 0.7059 - val_acc: 0.6353\n",
      "Epoch 425/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5042 - acc: 0.7580 - val_loss: 0.7053 - val_acc: 0.6305\n",
      "Epoch 426/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5073 - acc: 0.7548 - val_loss: 0.7077 - val_acc: 0.6273\n",
      "Epoch 427/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5073 - acc: 0.7577 - val_loss: 0.7067 - val_acc: 0.6332\n",
      "Epoch 428/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5044 - acc: 0.7612 - val_loss: 0.7105 - val_acc: 0.6186\n",
      "Epoch 429/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5067 - acc: 0.7570 - val_loss: 0.7047 - val_acc: 0.6350\n",
      "Epoch 430/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5059 - acc: 0.7579 - val_loss: 0.7117 - val_acc: 0.6286\n",
      "Epoch 431/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5063 - acc: 0.7587 - val_loss: 0.7099 - val_acc: 0.6281\n",
      "Epoch 432/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5030 - acc: 0.7571 - val_loss: 0.7000 - val_acc: 0.6328\n",
      "Epoch 433/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5060 - acc: 0.7545 - val_loss: 0.7030 - val_acc: 0.6330\n",
      "Epoch 434/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5036 - acc: 0.7589 - val_loss: 0.7022 - val_acc: 0.6361\n",
      "Epoch 435/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5005 - acc: 0.7624 - val_loss: 0.7070 - val_acc: 0.6298\n",
      "Epoch 436/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5057 - acc: 0.7575 - val_loss: 0.7087 - val_acc: 0.6306\n",
      "Epoch 437/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5024 - acc: 0.7581 - val_loss: 0.7075 - val_acc: 0.6342\n",
      "Epoch 438/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5030 - acc: 0.7553 - val_loss: 0.7082 - val_acc: 0.6360\n",
      "Epoch 439/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4997 - acc: 0.7632 - val_loss: 0.7088 - val_acc: 0.6314\n",
      "Epoch 440/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5023 - acc: 0.7577 - val_loss: 0.7107 - val_acc: 0.6301\n",
      "Epoch 441/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5027 - acc: 0.7576 - val_loss: 0.7146 - val_acc: 0.6347\n",
      "Epoch 442/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5028 - acc: 0.7605 - val_loss: 0.7108 - val_acc: 0.6243\n",
      "Epoch 443/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5050 - acc: 0.7589 - val_loss: 0.7062 - val_acc: 0.6359\n",
      "Epoch 444/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5023 - acc: 0.7591 - val_loss: 0.7167 - val_acc: 0.6299\n",
      "Epoch 445/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5011 - acc: 0.7620 - val_loss: 0.7098 - val_acc: 0.6290\n",
      "Epoch 446/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5001 - acc: 0.7617 - val_loss: 0.7107 - val_acc: 0.6251\n",
      "Epoch 447/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4995 - acc: 0.7636 - val_loss: 0.7108 - val_acc: 0.6324\n",
      "Epoch 448/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4998 - acc: 0.7616 - val_loss: 0.7143 - val_acc: 0.6279\n",
      "Epoch 449/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4980 - acc: 0.7635 - val_loss: 0.7120 - val_acc: 0.6356\n",
      "Epoch 450/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.5008 - acc: 0.7603 - val_loss: 0.7158 - val_acc: 0.6342\n",
      "Epoch 451/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4999 - acc: 0.7632 - val_loss: 0.7106 - val_acc: 0.6334\n",
      "Epoch 452/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4976 - acc: 0.7622 - val_loss: 0.7088 - val_acc: 0.6329\n",
      "Epoch 453/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4984 - acc: 0.7623 - val_loss: 0.7141 - val_acc: 0.6294\n",
      "Epoch 454/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4967 - acc: 0.7635 - val_loss: 0.7162 - val_acc: 0.6258\n",
      "Epoch 455/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4987 - acc: 0.7630 - val_loss: 0.7074 - val_acc: 0.6363\n",
      "Epoch 456/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4973 - acc: 0.7613 - val_loss: 0.7128 - val_acc: 0.6271\n",
      "Epoch 457/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4952 - acc: 0.7668 - val_loss: 0.7149 - val_acc: 0.6315\n",
      "Epoch 458/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4988 - acc: 0.7635 - val_loss: 0.7211 - val_acc: 0.6249\n",
      "Epoch 459/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4946 - acc: 0.7656 - val_loss: 0.7117 - val_acc: 0.6294\n",
      "Epoch 460/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4937 - acc: 0.7661 - val_loss: 0.7176 - val_acc: 0.6295\n",
      "Epoch 461/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4926 - acc: 0.7677 - val_loss: 0.7155 - val_acc: 0.6379\n",
      "Epoch 462/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4905 - acc: 0.7656 - val_loss: 0.7156 - val_acc: 0.6278\n",
      "Epoch 463/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4925 - acc: 0.7671 - val_loss: 0.7128 - val_acc: 0.6322\n",
      "Epoch 464/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4937 - acc: 0.7664 - val_loss: 0.7244 - val_acc: 0.6270\n",
      "Epoch 465/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4913 - acc: 0.7676 - val_loss: 0.7178 - val_acc: 0.6315\n",
      "Epoch 466/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4932 - acc: 0.7669 - val_loss: 0.7250 - val_acc: 0.6310\n",
      "Epoch 467/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4925 - acc: 0.7647 - val_loss: 0.7151 - val_acc: 0.6317\n",
      "Epoch 468/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4921 - acc: 0.7657 - val_loss: 0.7268 - val_acc: 0.6278\n",
      "Epoch 469/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4940 - acc: 0.7681 - val_loss: 0.7193 - val_acc: 0.6298\n",
      "Epoch 470/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4902 - acc: 0.7681 - val_loss: 0.7196 - val_acc: 0.6337\n",
      "Epoch 471/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4893 - acc: 0.7705 - val_loss: 0.7203 - val_acc: 0.6283\n",
      "Epoch 472/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4892 - acc: 0.7695 - val_loss: 0.7220 - val_acc: 0.6299\n",
      "Epoch 473/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4930 - acc: 0.7659 - val_loss: 0.7339 - val_acc: 0.6241\n",
      "Epoch 474/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4907 - acc: 0.7636 - val_loss: 0.7224 - val_acc: 0.6336\n",
      "Epoch 475/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4883 - acc: 0.7683 - val_loss: 0.7206 - val_acc: 0.6306\n",
      "Epoch 476/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4897 - acc: 0.7695 - val_loss: 0.7188 - val_acc: 0.6334\n",
      "Epoch 477/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4911 - acc: 0.7664 - val_loss: 0.7209 - val_acc: 0.6293\n",
      "Epoch 478/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4888 - acc: 0.7666 - val_loss: 0.7246 - val_acc: 0.6310\n",
      "Epoch 479/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4891 - acc: 0.7705 - val_loss: 0.7204 - val_acc: 0.6290\n",
      "Epoch 480/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4880 - acc: 0.7657 - val_loss: 0.7255 - val_acc: 0.6248\n",
      "Epoch 481/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4896 - acc: 0.7682 - val_loss: 0.7233 - val_acc: 0.6306\n",
      "Epoch 482/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4900 - acc: 0.7681 - val_loss: 0.7181 - val_acc: 0.6299\n",
      "Epoch 483/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4852 - acc: 0.7706 - val_loss: 0.7228 - val_acc: 0.6250\n",
      "Epoch 484/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4880 - acc: 0.7671 - val_loss: 0.7272 - val_acc: 0.6317\n",
      "Epoch 485/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4857 - acc: 0.7709 - val_loss: 0.7216 - val_acc: 0.6288\n",
      "Epoch 486/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4862 - acc: 0.7702 - val_loss: 0.7364 - val_acc: 0.6298\n",
      "Epoch 487/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4866 - acc: 0.7710 - val_loss: 0.7307 - val_acc: 0.6340\n",
      "Epoch 488/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4870 - acc: 0.7701 - val_loss: 0.7313 - val_acc: 0.6304\n",
      "Epoch 489/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4827 - acc: 0.7740 - val_loss: 0.7312 - val_acc: 0.6232\n",
      "Epoch 490/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4862 - acc: 0.7686 - val_loss: 0.7300 - val_acc: 0.6275\n",
      "Epoch 491/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4840 - acc: 0.7705 - val_loss: 0.7302 - val_acc: 0.6309\n",
      "Epoch 492/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4827 - acc: 0.7722 - val_loss: 0.7266 - val_acc: 0.6327\n",
      "Epoch 493/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4851 - acc: 0.7715 - val_loss: 0.7344 - val_acc: 0.6232\n",
      "Epoch 494/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4817 - acc: 0.7730 - val_loss: 0.7309 - val_acc: 0.6275\n",
      "Epoch 495/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4804 - acc: 0.7742 - val_loss: 0.7362 - val_acc: 0.6221\n",
      "Epoch 496/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4838 - acc: 0.7719 - val_loss: 0.7345 - val_acc: 0.6266\n",
      "Epoch 497/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4835 - acc: 0.7722 - val_loss: 0.7328 - val_acc: 0.6295\n",
      "Epoch 498/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4829 - acc: 0.7716 - val_loss: 0.7325 - val_acc: 0.6356\n",
      "Epoch 499/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4843 - acc: 0.7710 - val_loss: 0.7292 - val_acc: 0.6302\n",
      "Epoch 500/500\n",
      "1280/1280 [==============================] - 0s - loss: 0.4818 - acc: 0.7733 - val_loss: 0.7340 - val_acc: 0.6228\n"
     ]
    }
   ],
   "source": [
    "best_network = build_network(best_n_hidden, best_drop_out, input_shape=(best_timestep, n_dim), batch_size=batch_size)\n",
    "optimizer = optimizers.Adam(lr=best_lr)\n",
    "best_network.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "best_fit = best_network.fit(X_train_best, y_train_best, batch_size=batch_size, epochs=500, verbose=1, validation_data=(X_val_best, y_val_best), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAisAAAGHCAYAAABxmBIgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faf56782dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(best_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAisAAAGHCAYAAABxmBIgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fadbd1a56a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_accuracy(best_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
