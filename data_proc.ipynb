{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql(start_date, end_date, table, columns='*'):\n",
    "    \n",
    "    #-----------------------------------------------------#\n",
    "    # Function to generate sql query statement for a table and specific list of columns over time\n",
    "    #\n",
    "    # INPUTS:\n",
    "    # start_date = start date of the query (datetime)\n",
    "    # end_date = end date of the query (datetime)\n",
    "    # table = name of table in database (string)\n",
    "    # column = field names to query (list)\n",
    "    #\n",
    "    # OUTPUTS:\n",
    "    # sql = sql query statement (string)\n",
    "    #-----------------------------------------------------#\n",
    "    \n",
    "    #Check format\n",
    "    assert isinstance(start_date, datetime.date), 'Start Date has to be datetime.date object!'\n",
    "    assert isinstance(end_date, datetime.date), 'End Date has to be datetime.date object!'\n",
    "    \n",
    "    #Turn into ISO format\n",
    "    sd = start_date.isoformat()\n",
    "    ed = end_date.isoformat()\n",
    "    \n",
    "    #Collapse columns into string\n",
    "    c = ','.join(columns)\n",
    "        \n",
    "    #Generate SQL statement\n",
    "    sql = 'SELECT ' + c + ' from HSI_DATA.' + table + ' WHERE DATE between ' + '\\'' + sd + '\\'' + ' AND ' + '\\'' + ed + '\\''\n",
    "\n",
    "    return sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(conn, start_date, end_date, table, columns):\n",
    "    \n",
    "    #-----------------------------------------------------#\n",
    "    # Function to get data from mysql server\n",
    "    #\n",
    "    # INPUTS:\n",
    "    # conn = mysqlclient connection (mysqlclient conn object)\n",
    "    # start_date = start date of the query (datetime)\n",
    "    # end_date = end date of the query (datetime)\n",
    "    # table = name of table in database (string)\n",
    "    # column = field names to query (list)\n",
    "    #\n",
    "    # OUTPUTS:\n",
    "    # data = pandas DataFrame\n",
    "    #-----------------------------------------------------#\n",
    "    \n",
    "    #Get data\n",
    "    data = pd.read_sql(generate_sql(start_date, end_date, table, columns), con=conn)\n",
    "    \n",
    "    #Set index if Date is contained in the dataframe\n",
    "    if 'DATE' in list(data.keys()):\n",
    "        data['DATE'] = pd.to_datetime(data['DATE'])\n",
    "        data.set_index(['DATE'], inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fwd_ret(conn, start_date, end_date, step):\n",
    "    \n",
    "    #-----------------------------------------------------#\n",
    "    # Function to generate forward returns of tracker fund for label generation\n",
    "    #\n",
    "    # INPUTS:\n",
    "    # conn = mysqlclient connection (mysqlclient conn object)\n",
    "    # start_date = start date of the query (datetime)\n",
    "    # end_date = end date of the query (datetime)\n",
    "    # step = forward step-day return (int)\n",
    "    #\n",
    "    # OUTPUTS:\n",
    "    # data = forward returns (pandas DataFrame)\n",
    "    #-----------------------------------------------------#\n",
    "    \n",
    "    #Check format\n",
    "    assert isinstance(start_date, datetime.date), 'Start Date has to be datetime.date object!'\n",
    "    assert isinstance(end_date, datetime.date), 'End Date has to be datetime.date object!'\n",
    "    \n",
    "    #Get data\n",
    "    data_now = get_data(conn, start_date, end_date, 'hsi_data', ['DATE', 'CLOSE'])\n",
    "    \n",
    "    #First date from current data\n",
    "    first_date = data_now.index[0]\n",
    "    last_date = data_now.index[-1]\n",
    "    \n",
    "    #Get next dates\n",
    "    fwd_startdate_sql = 'SELECT DATE FROM HSI_DATA.hsi_data WHERE DATE > ' + '\\'' + first_date.isoformat() + '\\'' + ' LIMIT 1'\n",
    "    fwd_enddate_sql = 'SELECT DATE FROM HSI_DATA.hsi_data WHERE DATE > ' + '\\'' + last_date.isoformat() + '\\'' + ' LIMIT 1'\n",
    "        \n",
    "    #Get dates\n",
    "    fwd_startdate = pd.read_sql(fwd_startdate_sql, con=conn).get_values()[0,0]\n",
    "    fwd_enddate = pd.read_sql(fwd_enddate_sql, con=conn).get_values()[0,0]\n",
    "    \n",
    "    #print(fwd_startdate)\n",
    "    #print(fwd_enddate)\n",
    "    \n",
    "    #Fetch data for current\n",
    "    data_fwd = get_data(conn, fwd_startdate, fwd_enddate, 'hsi_data', ['CLOSE'])\n",
    "    data_fwd.set_index(data_now.index, inplace=True)\n",
    "        \n",
    "    #Calculate forward returns\n",
    "    data = np.log(data_fwd/data_now)\n",
    "    data.columns = ['FWD_RET']\n",
    "    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_fwd_ret(fwd_ret, upper_cutoff, lower_cutoff):\n",
    "    \n",
    "    #-----------------------------------------------------#\n",
    "    # Function to generate classification labels from forward returns for prediction\n",
    "    # \n",
    "    # INPUTS:\n",
    "    # fwd_ret: dataframe of forward returns (pandas dataframe)\n",
    "    # upper_cutoff: cutoff % for the classification boundary for \"up\" label\n",
    "    # lower_cutoff: cutoff % for classification boundary for \"down\" label\n",
    "    #\n",
    "    # OUTPUT:\n",
    "    # labels: dataframe of class labels (pandas dataframe)\n",
    "    #-----------------------------------------------------#\n",
    "    \n",
    "    #Classify the returns\n",
    "    labels_array = [0 if ret < lower_cutoff else 2 if ret > upper_cutoff else 0 for ret in fwd_ret.values]\n",
    "    \n",
    "    labels = pd.DataFrame(labels_array).set_index(fwd_ret.index)\n",
    "    labels.columns = ['LABELS']\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_val_test_split(inputs, train_start, val_start, test_start, test_end, normalize='RobustScaler'):\n",
    "    \n",
    "    #-----------------------------------------------------#\n",
    "    # Function to split data into training, validation and test set by date.\n",
    "    # This will generate overlapping samples of a step sie feed into LSTM model\n",
    "    #\n",
    "    # INPUTS:\n",
    "    # inputs: pandas array of data to be split, indexed by time\n",
    "    # train_start: validation set start date\n",
    "    # val_start: test set start date\n",
    "    # test_start: test set start date\n",
    "    # normalize: parameter to control what sklearn normalization to use. It takes whatever sklearn is providing, or None\n",
    "    #\n",
    "    # OUTPUT:\n",
    "    # scaler, (inputs_train, inputs_val, inputs_test): sklearn scaler object, tuples of numpy array\n",
    "    #-----------------------------------------------------#\n",
    "    \n",
    "    #Split the data\n",
    "    inputs_train = inputs[train_start:val_start]\n",
    "    inputs_val = inputs[val_start:test_start]\n",
    "    inputs_test = inputs[test_start:test_end]\n",
    "    \n",
    "    #Define dictionary to store the corresponding mapping between normalize argument and its scaler object\n",
    "    dict_scaler = {'MinMaxScaler': MinMaxScaler(), \n",
    "                   'MaxAbsScaler': MaxAbsScaler(),\n",
    "                   'StandardScaler': StandardScaler(),\n",
    "                   'RobustScaler': RobustScaler(),\n",
    "                   'Normalizer': Normalizer()}\n",
    "\n",
    "    \n",
    "    #Normalize the data\n",
    "    if normalize != None:\n",
    "        \n",
    "        #Define and fit\n",
    "        scaler = dict_scaler[normalize]\n",
    "        scaler.fit(inputs_train)\n",
    "        \n",
    "        #Transform\n",
    "        inputs_train, inputs_val, inputs_test = scaler.transform(inputs_train), scaler.transform(inputs_val), scaler.transform(inputs_test)\n",
    "        \n",
    "        #Output\n",
    "        return scaler, inputs_train, inputs_val, inputs_test\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #Output\n",
    "        return None, inputs_train.as_matrix(), inputs_val.as_matrix(), inputs_test.as_matrix()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(inputs, labels, batch_size, steps):\n",
    "    \n",
    "    #-----------------------------------------------------#\n",
    "    # Function to generate batches inputs to train LSTM model.\n",
    "    # This will generate overlapping samples of a step sie feed into LSTM model\n",
    "    #\n",
    "    # INPUTS:\n",
    "    # inputs: numpy array of data to be batched\n",
    "    # batch_size: size of each batch\n",
    "    # steps: step size to feed into model\n",
    "    #\n",
    "    # OUTPUT:\n",
    "    # batch_data: numpy array of (input_batch, labels_batch) tuple\n",
    "    #-----------------------------------------------------#\n",
    "        \n",
    "    #Calculate number of sequences with each \"steps\" step size able to generate\n",
    "    n_seq = inputs.shape[0] - steps + 1\n",
    "    \n",
    "    #Calculate number of batches\n",
    "    n_samples = n_seq * steps\n",
    "    n_batches = n_samples // (batch_size * steps) + 1 # 1 more batch to capture the residual data\n",
    "    \n",
    "    #Error Check: Assert n_batches has to be > 0\n",
    "    assert(n_batches > 0), 'Not enough data to form 1 batch!'\n",
    "    \n",
    "    #labels\n",
    "    #labels_batch = labels[:n_batches * batch_size, :]\n",
    "    \n",
    "    #Generate batches\n",
    "    for n in range(0, n_batches):\n",
    "        \n",
    "        #Container to store the outputs\n",
    "        inputs_batch = []\n",
    "        labels_batch = []\n",
    "        \n",
    "        for ii in range(n * batch_size, (n + 1) * batch_size):\n",
    "                        \n",
    "            inputs_batch.append(inputs[ii : ii + steps, :])\n",
    "            labels_batch.append(labels[ii : ii + steps])\n",
    "        \n",
    "        #Return batches\n",
    "        yield np.stack(inputs_batch), np.stack(labels_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
